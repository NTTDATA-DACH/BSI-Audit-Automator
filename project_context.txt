==== bsi-audit-automator/.dockerignore ====
# Git / version control
.git
.gitignore

# Docker
Dockerfile
.dockerignore

# Python cache and virtual environment
__pycache__/
*.pyc
.venv
venv/

# Environment variables - should be passed at runtime, not baked into the image
.env

# IDE / Editor specific
.vscode/
.idea/
==== bsi-audit-automator/Dockerfile ====
# Stage 1: Use an official Python runtime as a parent image
# Using a "slim" image to keep the final image size down.
FROM python:3.11-slim-bookworm

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1

# Set the working directory in the container
WORKDIR /app

# Add the non-root user's local bin to the PATH.
# This prevents warnings during pip install.
ENV PATH="/app/.local/bin:${PATH}"

# It's good practice to upgrade pip to the latest version
RUN pip install --upgrade pip

# Create a non-root user and group to run the application
# This avoids the "Running pip as root" warning and is a security best practice.
RUN groupadd -r appgroup && useradd -r -g appgroup -d /app -s /sbin/nologin -c "Docker image user" appuser
RUN chown -R appuser:appgroup /app

# Copy the requirements file into the container
# This is done before copying the rest of the code to leverage Docker's layer caching.
COPY requirements.txt .

# Switch to the non-root user before installing dependencies
USER appuser

# Install any needed packages specified in requirements.txt
# --no-cache-dir reduces image size by not storing the download cache.
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application's source code into the container
COPY . .

# Specify the command to run on container start.
ENTRYPOINT ["python", "-m", "src.main"]

# Set a default command to show the help message if no other command is provided.
CMD ["--help"]
==== bsi-audit-automator/assets/json/BSI_GS_OSCAL_current_2023_benutzerdefinierte.json ====
{
  "catalog": {
    "uuid": "d186e90d-ecbd-4d07-8311-4603deb60225",
    "metadata": {
      "title": "Gesamtkatalog BSI Grundschutz Kompendium 2023",
      "last-modified": "2025-06-27T17:42:20.097143+00:00",
      "version": "1.0.0",
      "oscal-version": "1.1.2"
    },
    "groups": [
      {
        "id": "ISMS",
        "title": "ISMS: Sicherheitsmanagement",
        "class": "main-group",
        "groups": [
          {
            "id": "ISMS.1",
            "title": "Sicherheitsmanagement",
            "class": "baustein",
            "controls": [
              {
                "id": "ISMS.1.A1",
                "title": "Übernahme der Gesamtverantwortung für Informationssicherheit durch die Leitung (B)",
                "class": "Management",
                "props": [
                  {
                    "name": "level",
                    "value": "1",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "phase",
                    "value": "Initiation",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "practice",
                    "value": "GOV",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_c",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_i",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_a",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  }
                ],
                "parts": [
                  {
                    "id": "isms.1.a1-m1",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 1: Partial",
                    "class": "maturity-level-partial",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Institutionsleitung hat die Verantwortung für Informationssicherheit nur mündlich oder informell anerkannt. Die Übernahme der Verantwortung ist nicht dokumentiert und wird nicht aktiv kommuniziert, was zu Unklarheiten bei den Mitarbeitenden führt."
                      },
                      {
                        "name": "guidance",
                        "prose": "In dieser Stufe wird die Verantwortung oft nur reaktiv im Falle eines Sicherheitsvorfalls oder einer externen Prüfung erwähnt. Es gibt keine formalen Dokumente wie eine Ernennungsurkunde für den ISB oder eine offizielle Leitlinie."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer befragt Führungskräfte und stellt fest, ob sie ihre Verantwortung für die Informationssicherheit verstehen und wahrnehmen. Es wird geprüft, ob eine formale Zuweisung der Gesamtverantwortung fehlt."
                      }
                    ]
                  },
                  {
                    "id": "isms.1.a1-m2",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 2: Foundational",
                    "class": "maturity-level-foundational",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Gesamtverantwortung für die Informationssicherheit ist formal der Institutionsleitung zugewiesen, z. B. in einer Geschäftsordnung. Die Leitung nimmt diese Verantwortung jedoch nur passiv wahr und delegiert die Aufgaben vollständig ohne aktive Steuerung oder Kontrolle."
                      },
                      {
                        "name": "guidance",
                        "prose": "Die Zuweisung der Verantwortung ist in einem offiziellen Dokument festgehalten. Die Leitung genehmigt Budgets, greift aber nicht proaktiv in die Gestaltung des Sicherheitsprozesses ein und informiert sich nur unregelmäßig über den Status."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer sichtet Dokumente wie die Geschäftsordnung, um die formale Zuweisung der Verantwortung zu verifizieren. Es wird durch Interviews geprüft, inwieweit die Leitung den Sicherheitsprozess aktiv steuert und kontrolliert."
                      }
                    ]
                  },
                  {
                    "id": "isms.1.a1-m3",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 3: Defined",
                    "class": "maturity-level-defined",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Institutionsleitung MUSS die Gesamtverantwortung für Informationssicherheit in der Institution übernehmen. Dies MUSS für alle Beteiligten deutlich erkennbar sein. Die Institutionsleitung MUSS den Sicherheitsprozess initiieren, steuern und kontrollieren. Die Institutionsleitung MUSS die Zuständigkeiten für Informationssicherheit festlegen und die zuständigen Mitarbeitenden mit den erforderlichen Kompetenzen und Ressourcen ausstatten. Die Institutionsleitung MUSS sich regelmäßig über den Status der Informationssicherheit sowie über mögliche Risiken und Konsequenzen aufgrund fehlender Sicherheitsmaßnahmen informieren lassen."
                      },
                      {
                        "name": "guidance",
                        "prose": "Die Übernahme der Verantwortung wird durch eine von der Leitung unterzeichnete Sicherheitsleitlinie dokumentiert. Regelmäßige Meetings (z. B. quartalsweise) zwischen der Leitung und dem ISB werden etabliert, um den Status und die Risiken zu besprechen. Die Ergebnisse dieser Meetings werden protokolliert."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer prüft die unterzeichnete Sicherheitsleitlinie. Es werden Protokolle der Management-Meetings zur Informationssicherheit eingesehen. Interviews mit Mitarbeitenden bestätigen, dass die Rolle der Leitung im Sicherheitsprozess klar und bekannt ist."
                      }
                    ]
                  },
                  {
                    "id": "isms.1.a1-m4",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 4: Enhanced",
                    "class": "maturity-level-enhanced",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Institutionsleitung lebt die Informationssicherheit aktiv vor ('Tone at the Top'). Sie hinterfragt die erhaltenen Statusberichte kritisch, fordert tiefere Analysen bei Abweichungen und verfolgt die Umsetzung von Maßnahmen zur Risikobehandlung konsequent nach. Die Einhaltung der Sicherheitspolicies wird auch auf Leitungsebene demonstriert."
                      },
                      {
                        "name": "guidance",
                        "prose": "Die Leitung nimmt aktiv an wichtigen Sensibilisierungsmaßnahmen teil. Entscheidungen zur Risikobehandlung werden nicht nur dokumentiert, sondern auch mit einer klaren Begründung für die gewählte Option (z. B. Akzeptanz, Minderung) versehen. Ein dediziertes Risikokomitee unter Vorsitz eines Mitglieds der Institutionsleitung wird eingerichtet."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer analysiert die Protokolle des Risikokomitees und die dokumentierten Risikoentscheidungen auf ihre Nachvollziehbarkeit. Es wird geprüft, ob die Leitung bei der Zuweisung von Ressourcen für Sicherheitsmaßnahmen eine klare Priorisierung basierend auf dem Geschäftsrisiko vornimmt."
                      }
                    ]
                  },
                  {
                    "id": "isms.1.a1-m5",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 5: Comprehensive",
                    "class": "maturity-level-comprehensive",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Informationssicherheitsziele sind vollständig in die übergeordneten Geschäftsziele und die strategische Planung der Institution integriert. Die Leistung im Bereich der Informationssicherheit ist Teil der Zielvereinbarungen und Leistungsbeurteilungen für das obere Management. Die Leitung fördert aktiv eine positive Sicherheitskultur und investiert proaktiv in zukünftige Sicherheitstechnologien."
                      },
                      {
                        "name": "guidance",
                        "prose": "In den jährlichen Geschäftsberichten wird über die Informationssicherheitslage berichtet. Die Zielerreichung wird anhand von Key Performance Indicators (KPIs) gemessen, die in einem Management-Dashboard visualisiert werden. Die Leitung initiiert und finanziert Forschungsprojekte zur Abwehr zukünftiger Bedrohungen."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer überprüft die strategischen Planungsdokumente und die Zielvereinbarungen des Managements auf die Integration von Sicherheitszielen. Es wird die Existenz und Nutzung eines KPI-Dashboards verifiziert. Die Wirksamkeit der Sicherheitskultur wird durch Mitarbeiterbefragungen und die Analyse von Sicherheitsvorfällen bewertet."
                      }
                    ]
                  }
                ]
              },
              {
                "id": "ISMS.1.A2",
                "title": "Festlegung der Sicherheitsziele und -strategie (B)",
                "class": "Management",
                "props": [
                  {
                    "name": "level",
                    "value": "1",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "phase",
                    "value": "Initiation",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "practice",
                    "value": "GOV",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_c",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_i",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_a",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  }
                ],
                "parts": [
                  {
                    "id": "isms.1.a2-m1",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 1: Partial",

[... File truncated. Only first 200 of 204472 lines included. ...]

==== bsi-audit-automator/assets/json/master_report_template.json ====
{
  "bsiAuditReport": {
    "titlePage": {
      "reportTitle": "Auditbericht im Rahmen der Zertifizierung nach ISO 27001 auf der Basis von IT-Grundschutz",
      "auditedInstitution": "",
      "certificationID": ""
    },
    "allgemeines": {
      "chapterNumber": "1.",
      "title": "Allgemeines",
      "versionshistorie": {
        "title": "Versionshistorie",
        "subchapterNumber": "1.1",
        "table": {
          "headers": ["Datum", "Version", "Verfasser", "Bemerkungen"],
          "rows": []
        }
      },
      "auditierteInstitution": {
        "title": "Auditierte Institution",
        "subchapterNumber": "1.2",
        "kontaktinformationenAntragsteller": {
          "title": "Kontaktinformationen des Antragstellers (auditierte Institution):",
          "table": {
              "headers": ["Institution", "Straße / Hausnummer", "PLZ / Ort", "E-Mail"],
              "rows": []
          }
      },
      "ansprechpartnerZertifizierung": {
          "title": "Ansprechpartner für die Zertifizierung beim Antragsteller:",
          "table": {
              "headers": ["Name", "Funktion", "Telefon", "E-Mail", "Optional: Abweichende Anschrift"],
              "rows": []
          }
        }
      },
      "auditteam": {
        "title": "Auditteam",
        "subchapterNumber": "1.3",
        "auditteamleiter": {
            "title": "Auditteamleiter",
            "table": {
                "headers": ["Name", "Institution", "Zertifizierungsnummer", "Gültigkeit Zertifikat", "Straße / Hausnummer", "PLZ / Ort", "Telefon", "E-Mail", "Datum der Freigabe der Unabhängigkeitserklärung"],
                "rows": []
            }
        },
        "auditor": {
            "title": "Auditor",
            "table": {
                "headers": ["Name", "Institution", "Zertifizierungsnummer", "Gültigkeit Zertifikat", "Straße / Hausnummer", "PLZ / Ort", "Telefon", "E-Mail", "Datum der Freigabe der Unabhängigkeitserklärung"],
                "rows": []
            }
        },
        "fachexperte": {
            "title": "Fachexperte",
            "table": {
                "headers": ["Name", "Institution", "Zertifizierungsnummer", "Gültigkeit Zertifikat", "Straße / Hausnummer", "PLZ / Ort", "Telefon", "E-Mail", "Datum der Freigabe der Unabhängigkeitserklärung"],
                "rows": []
            }
        },
        "berater": {
            "title": "Berater",
            "table": {
                "headers": ["Name", "Institution", "Zertifizierungsnummer (falls vorhanden)"],
                "rows": []
            }
        }
      },
      "informationsverbund": {
        "title": "Informationsverbund",
        "subchapterNumber": "1.4",
        "description": "Die nachfolgenden Informationen können dem Zertifizierungsantrag oder dem gültigen Zertifikat entnommen werden.",
        "content": [
          {
            "type": "prose",
            "label": "Kurzbezeichnung:",
            "text": ""
          },
          {
            "type": "prose",
            "label": "Kurzbeschreibung (entspricht Geltungsbereich):",
            "text": ""
          }
        ]
      },
      "audittyp": {
        "title": "Audittyp",
        "subchapterNumber": "1.5",
        "content": "Zertifizierungsaudit"
      },
      "auditprojektierung": {
        "title": "Auditprojektierung",
        "subchapterNumber": "1.6",
        "table": {
            "headers": ["Audit-Phasen", "Datum / Zeitraum", "Aufwand (in PT)"],
            "rows": [
                {"Audit-Phasen": "Voraudit (nur bei Erstzertifizierung)"},
                {"Audit-Phasen": "Übergabe der Referenzdokumente"},
                {"Audit-Phasen": "Sichtung der Referenzdokumente"},
                {"Audit-Phasen": "Vor-Ort-Audit"},
                {"Audit-Phasen": "Prüfung der Nachbesserungen"},
                {"Audit-Phasen": "Erstellung des Auditberichts"},
                {"Audit-Phasen": "Nachforderungen der Zertifizierungsstelle"},
                {"Audit-Phasen": "Bearbeitung der Nachforderungen und ggf. Nachbesserungen durch den Antragssteller"},
                {"Audit-Phasen": "Abschluss der Auditierung"}
            ]
        },
        "content": [
            { "type": "prose", "label": "Anmerkungen zur Auditprojektierung:", "text": ""}
        ]
      },
      "absprachen": {
        "title": "Absprachen",
        "subchapterNumber": "1.7",
        "table": {
            "headers": ["Absprache zu", "Angabe von Datum, Beteiligten, Art usw."],
            "rows": []
        }
      },
      "formaleGrundlagen": {
          "title": "Formale Grundlagen der Auditierung",
          "subchapterNumber": "1.8",
          "table": {
              "headers": ["Bezeichnung", "Version / Edition, Datum"],
              "rows": [
                  {"Bezeichnung": "Zertifizierung nach ISO 27001 auf der Basis von IT-Grundschutz - Zertifizierungsschema"},
                  {"Bezeichnung": "Zertifizierung nach ISO 27001 auf der Basis von IT-Grundschutz – Auditierungsschema"},
                  {"Bezeichnung": "BSI-Standard 200-2 – IT-Grundschutz-Methodik"},
                  {"Bezeichnung": "IT-Grundschutz-Kompendium"},
                  {"Bezeichnung": "Vorlage Auditbericht"},
                  {"Bezeichnung": "Grundlage für die Risikoanalyse"}
              ]
          }
      },
      "inhaltlicheGrundlagen": {
          "title": "Inhaltliche Grundlagen",
          "subchapterNumber": "1.9",
          "content": [
              {"type": "prose", "label": "Verweis der Liste für Referenzdokumente:", "text": "Siehe Anhang 7.1"}
          ]
      },
      "toolbasierteUnterstuetzung": {
          "title": "Toolbasierte Audit-Unterstützung",
          "subchapterNumber": "1.10",
          "content": [
              {"type": "prose", "label": "Tool:", "text": ""},
              {"type": "prose", "label": "Version:", "text": ""},
              {"type": "prose", "label": "Stand des IT-Grundschutz-Kompendiums:", "text": ""}
          ]
      }
    },
    "voraudit": {
      "chapterNumber": "2.",
      "title": "Voraudit",
      "content": [
          {"type": "question", "questionText": "Wurde ein Voraudit durchgeführt?", "answer": null},
          {"type": "prose", "label": "Prüfumfang des Voraudits:", "text": ""},
          {"type": "prose", "label": "Feststellung:", "text": ""}
      ]
    },
    "dokumentenpruefung": {
      "chapterNumber": "3.",
      "title": "Dokumentenprüfung",
      "aktualitaetDerReferenzdokumente": {
        "title": "Aktualität der Referenzdokumente",
        "subchapterNumber": "3.1",
        "content": [
          {"type": "question", "questionText": "Wurden alle Referenzdokumente A.0 gemäß den Vorgaben von A.0.3 Lenkung von Dokumenten überarbeitet?", "answer": null},
          {"type": "question", "questionText": "Wurden alle Dateien zu den Referenzdokumenten A.1, A.2, A.3, A.5 und A.6 für das Audit neu erstellt?", "answer": null},
          {"type": "question", "questionText": "Wurden alle Maßnahmen im A.4 IT-Grundschutz-Check innerhalb der letzten 12 Monate neu bewertet?", "answer": null},
          {"type": "question", "questionText": "Datum der letzten inhaltlichen Änderung im Referenzdokument A.4 IT-Grundschutz-Check.", "answer": ""},
          {"type": "finding", "label": "Feststellung:", "findingText": ""}
        ]
      },
      "sicherheitsleitlinieUndRichtlinienInA0": {
        "title": "Sicherheitsleitlinie und -richtlinien in A.0",
        "subchapterNumber": "3.2",
        "content": [
          {"type": "question", "questionText": "Ist die Leitlinie zur Informationssicherheit (A.0.1) sinnvoll und angemessen für den Antragsteller? Erfüllt die Leitlinie zur Informationssicherheit alle Aspekte gemäß den Anforderungen aus dem Baustein ISMS.1?", "answer": null},
          {"type": "question", "questionText": "Decken sich die Sicherheitsziele aus der Leitlinie mit den Sicherheitsanforderungen der restlichen Referenzdokumente (A.0.2 bis A.0.5)?", "answer": null},
          {"type": "question", "questionText": "Werden die Sicherheitsrichtlinien (A.0.1 bis A.0.5) durch das Management getragen und wurden sie veröffentlicht?", "answer": null},
          {"type": "finding", "label": "Feststellung:", "findingText": ""}
        ]
      },
      "strukturanalyseA1": {
        "title": "Strukturanalyse A.1",
        "subchapterNumber": "3.3",
        "definitionDesInformationsverbundes": {
            "title": "Definition des Informationsverbundes",
            "subchapterNumber": "3.3.1",
            "content": [
                {"type": "question", "questionText": "Ist der Informationsverbund eindeutig abgegrenzt?", "answer": null},
                {"type": "question", "questionText": "Sind alle infrastrukturellen, organisatorischen, personellen und technischen Komponenten im Informationsverbund enthalten, die zur Aufgabenerfüllung notwendig sind?", "answer": null},
                {"type": "question", "questionText": "Sind die Schnittstellen zu allen weiteren Prozessen definiert?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "bereinigterNetzplan": {
            "title": "Bereinigter Netzplan",
            "subchapterNumber": "3.3.2",
            "content": [
                {"type": "question", "questionText": "Optional: Liegt ein aktueller und vollständiger bereinigter Netzplan vor?", "answer": null},
                {"type": "question", "questionText": "Sind alle Komponenten im Netzplan mit den korrekten Bezeichnern versehen?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "listeDerGeschaeftsprozesse": {
            "title": "Liste der Geschäftsprozesse",
            "subchapterNumber": "3.3.3",
            "content": [
                {"type": "question", "questionText": "Enthält die Liste der Geschäftsprozesse alle benötigten Informationen (eindeutige Bezeichnung, Name, Prozessverantwortlicher, kurze Beschreibung, benötigte Anwendungen)?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "listeDerAnwendungen": {
            "title": "Liste der Anwendungen",
            "subchapterNumber": "3.3.4",
            "content": [
                {"type": "question", "questionText": "Enthält die Liste der Anwendungen alle benötigten Informationen?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "listeDerItSysteme": {
            "title": "Liste der IT-Systeme",
            "subchapterNumber": "3.3.5",
            "content": [
                {"type": "question", "questionText": "Enthält die Liste der IT-Systeme alle benötigten Informationen?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "listeDerRaeumeGebaeudeStandorte": {
            "title": "Liste der Räume, Gebäude und Standorte",
            "subchapterNumber": "3.3.6",
            "content": [
                {"type": "question", "questionText": "Enthält die Liste der Räume, Gebäude und Standorte alle benötigten Informationen?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "listeDerKommunikationsverbindungen": {
            "title": "Liste der Kommunikationsverbindungen",
            "subchapterNumber": "3.3.7",
            "content": [
                {"type": "question", "questionText": "Enthält die Liste der Kommunikationsverbindungen alle benötigten Informationen?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "stichprobenDokuStrukturanalyse": {
            "title": "Stichprobendokumentation der Strukturanalyse",
            "subchapterNumber": "3.3.8",
            "content": [
              {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "listeDerDienstleister": {
            "title": "Liste der Dienstleister",
            "subchapterNumber": "3.3.9",
            "content": [
                {"type": "question", "questionText": "Liegt eine aktuelle und vollständige Liste aller externen Dienstleister vor?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "ergebnisDerStrukturanalyse": {
            "title": "Ergebnis der Strukturanalyse",
            "subchapterNumber": "3.3.10",
            "content": [
                {"type": "prose", "label": "Votum Strukturanalyse:", "text": ""}
            ]
        }
      },
      "schutzbedarfsfeststellungA2": {
        "title": "Schutzbedarfsfeststellung A.2",
        "subchapterNumber": "3.4",
        "definitionDerSchutzbedarfskategorien": {
          "title": "Definition der Schutzbedarfskategorien",
          "subchapterNumber": "3.4.1",
          "content": [
            {"type": "question", "questionText": "Ist die Definition der Schutzbedarfskategorien plausibel und für den Informationsverbund angemessen?", "answer": null},
            {"type": "question", "questionText": "Wurden mehr als drei Schutzbedarfskategorien definiert?", "answer": null},
            {"type": "finding", "label": "Feststellung:", "findingText": ""}
          ]
        },
        "schutzbedarfGeschaeftsprozesse": {
            "title": "Schutzbedarf der Geschäftsprozesse",
            "subchapterNumber": "3.4.2",
            "content": [
                {"type": "question", "questionText": "Ist der Schutzbedarf der Geschäftsprozesse vollständig dokumentiert?", "answer": null},
                {"type": "question", "questionText": "Ist der Schutzbedarf der Geschäftsprozesse nachvollziehbar begründet?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "schutzbedarfAnwendungen": {
            "title": "Schutzbedarf der Anwendungen",
            "subchapterNumber": "3.4.3",
            "content": [
                {"type": "question", "questionText": "Ist der Schutzbedarf der Anwendungen vollständig dokumentiert?", "answer": null},
                {"type": "question", "questionText": "Ist der Schutzbedarf der Anwendungen nachvollziehbar begründet?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "schutzbedarfItSysteme": {
            "title": "Schutzbedarf der IT-Systeme",
            "subchapterNumber": "3.4.4",
            "content": [
                {"type": "question", "questionText": "Ist der Schutzbedarf der IT-Systeme vollständig dokumentiert?", "answer": null},
                {"type": "question", "questionText": "Ist der Schutzbedarf der IT-Systeme nachvollziehbar begründet?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "schutzbedarfRaeume": {
            "title": "Schutzbedarf der Räume, Gebäude und Standorte",
            "subchapterNumber": "3.4.5",
            "content": [
                {"type": "question", "questionText": "Ist der Schutzbedarf der Räume, Gebäude und Standorte vollständig dokumentiert?", "answer": null},
                {"type": "question", "questionText": "Ist der Schutzbedarf der Räume, Gebäude und Standorte nachvollziehbar begründet?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "schutzbedarfKommunikationsverbindungen": {
            "title": "Schutzbedarf der Kommunikationsverbindungen",
            "subchapterNumber": "3.4.6",
            "content": [
                {"type": "question", "questionText": "Ist der Schutzbedarf der Außenverbindungen und kritischen Kommunikationsverbindungen vollständig dokumentiert?", "answer": null},
                {"type": "question", "questionText": "Ist der Schutzbedarf der Kommunikationsverbindungen nachvollziehbar begründet?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "stichprobenDokuSchutzbedarf": {
            "title": "Stichprobendokumentation der Schutzbedarfsfeststellung",
            "subchapterNumber": "3.4.7",
            "content": [
              {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "ergebnisDerSchutzbedarfsfeststellung": {
            "title": "Ergebnis der Schutzbedarfsfeststellung",
            "subchapterNumber": "3.4.8",
            "content": [
                {"type": "prose", "label": "Votum Schutzbedarfsfeststellung:", "text": ""}
            ]
        }
      },
      "modellierungDesInformationsverbundesA3": {
        "title": "Modellierung des Informationsverbundes A.3",
        "subchapterNumber": "3.5",
        "modellierungsdetails": {
          "title": "Modellierungsdetails",
          "subchapterNumber": "3.5.1",
          "content": [
            {"type": "question", "questionText": "Ist jeder Baustein des IT-Grundschutz-Kompendiums auf alle relevanten Zielobjekte angewandt?", "answer": null},
            {"type": "question", "questionText": "Ist für jeden Baustein des IT-Grundschutz-Kompendiums, der nicht angewandt wurde, eine plausible Begründung vorhanden?", "answer": null},
            {"type": "question", "questionText": "Wurden alle Zielobjekte angemessen berücksichtigt, für die keine Bausteine des IT-Grundschutz-Kompendiums vorhanden sind?", "answer": null},
            {"type": "question", "questionText": "Gibt es benutzerdefinierte Bausteine?", "answer": null},
            {"type": "finding", "label": "Feststellung:", "findingText": ""}
          ]
        },
        "ergebnisDerModellierung": {
          "title": "Ergebnis der Modellierung",
          "subchapterNumber": "3.5.2",
          "content": [
            {"type": "prose", "label": "Votum Modellierung:", "text": ""}
          ]
        }
      },
      "itGrundschutzCheckA4": {
        "title": "IT-Grundschutz-Check A.4",
        "subchapterNumber": "3.6",
        "detailsZumItGrundschutzCheck": {
          "title": "Details zum IT-Grundschutz-Check",
          "subchapterNumber": "3.6.1",
          "content": [
            {"type": "question", "questionText": "Wurde zu jeder Anforderung der Umsetzungsstatus erhoben?", "answer": null},
            {"type": "question", "questionText": "Wurden alle Anforderungen mit Umsetzungsstatus „entbehrlich“ plausibel begründet?", "answer": null},
            {"type": "question", "questionText": "Sind alle MUSS-Teilanforderungen erfüllt?", "answer": null},
            {"type": "question", "questionText": "Wurden die nicht oder nur teilweise umgesetzten Anforderungen im Referenzdokument A.6 dokumentiert?", "answer": null},
            {"type": "question", "questionText": "Sind alle Anforderungen innerhalb der letzten 12 Monate überprüft worden?", "answer": null},
            {"type": "finding", "label": "Feststellung:", "findingText": ""}
          ]
        },
        "benutzerdefinierteBausteine": {
            "title": "Benutzerdefinierte Bausteine",
            "subchapterNumber": "3.6.2",
            "content": [
                {"type": "question", "questionText": "Wurden benutzerdefinierte Bausteine erstellt und modelliert?", "answer": null},
                {"type": "question", "questionText": "Sind alle Anforderungen der Institution in den benutzerdefinierten Bausteinen enthalten?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "ergebnisItGrundschutzCheck": {
            "title": "Ergebnis IT-Grundschutz-Check",
            "subchapterNumber": "3.6.3",
            "content": [
                {"type": "prose", "label": "Votum IT-Grundschutz-Check:", "text": ""}
            ]
        }
      },
      "risikoanalyseA5": {
          "title": "Risikoanalyse A.5",
          "subchapterNumber": "3.7",
          "risikoanalyse": {
              "title": "Ergebnis Risikoanalyse",
              "subchapterNumber": "3.7.1",
              "content": [
                  {"type": "question", "questionText": "Wurde für alle Zielobjekte, deren Schutzbedarf über „normal“ liegt, eine Risikoanalyse durchgeführt?", "answer": null},
                  {"type": "question", "questionText": "Ist die Risikoanalyse aussagekräftig und nachvollziehbar?", "answer": null},
                  {"type": "question", "questionText": "Ist das vom Management getragene Restrisiko für den Informationsverbund angemessen?", "answer": null},
                  {"type": "question", "questionText": "Wurde der Management-Report zur Risikoanalyse von der Institutionsleitung unterschrieben?", "answer": null},
                  {"type": "finding", "label": "Feststellung:", "findingText": ""}
              ]
          }
      },
      "realisierungsplanA6": {
          "title": "Realisierungsplan A.6",
          "subchapterNumber": "3.8",
          "realisierungsplan": {
              "title": "Ergebnis Realisierungsplan",
              "subchapterNumber": "3.8.1",
              "content": [
                  {"type": "question", "questionText": "Liegt ein Realisierungsplan A.6 vor?", "answer": null},
                  {"type": "question", "questionText": "Werden die bestehenden Risiken nachvollziehbar dokumentiert?", "answer": null},
                  {"type": "question", "questionText": "Besteht ein Umsetzungsplan für die Reduzierung des Restrisikos?", "answer": null},
                  {"type": "finding", "label": "Feststellung:", "findingText": ""}
              ]
          }
      },
      "ergebnisDerDokumentenpruefung": {
        "title": "Ergebnis der Dokumentenprüfung",
        "subchapterNumber": "3.9",
        "content": [
          {
            "type": "question",
            "questionText": "Ist eine Fortführung des Audits mit der Vor-Ort-Prüfung möglich?",
            "answer": null
          },
          {
            "type": "prose",
            "label": "Votum Dokumentenprüfung:",
            "text": ""
          }
        ]
      }
    },
    "erstellungEinesPruefplans": {
      "chapterNumber": "4.",
      "title": "Erstellung eines Prüfplans",
      "auditplanung": {
        "title": "Auditplanung",
        "subchapterNumber": "4.1",
        "auswahlBausteineErstRezertifizierung": {
          "title": "Auswahl Bausteine Erst-/Rezertifizierungsverfahren",
          "subchapterNumber": "4.1.1",
          "table": {
            "headers": ["Schicht", "Baustein", "Zielobjekt-Name", "Zielobjekt-Kürzel", "Begründung zur Auswahl"],
            "rows": []
          }
        },
        "auswahlBausteine1Ueberwachungsaudit": {
            "title": "Auswahl Bausteine 1. Überwachungsaudit",
            "subchapterNumber": "4.1.2",
            "table": {
              "headers": ["Schicht", "Baustein", "Zielobjekt-Name", "Zielobjekt-Kürzel", "Begründung zur Auswahl"],
              "rows": []
            }
        },
        "auswahlBausteine2Ueberwachungsaudit": {
            "title": "Auswahl Bausteine 2. Überwachungsaudit",
            "subchapterNumber": "4.1.3",
            "table": {
              "headers": ["Schicht", "Baustein", "Zielobjekt-Name", "Zielobjekt-Kürzel", "Begründung zur Auswahl"],
              "rows": []
            }
        },
        "auswahlStandorte": {
            "title": "Auswahl Standorte",
            "subchapterNumber": "4.1.4",
            "table": {
                "headers": ["Standort", "Erst- bzw. Rezertifizierung", "1. Überwachungsaudit", "2. Überwachungsaudit", "Begründung für die Auswahl"],
                "rows": []
            }
        },
        "auswahlMassnahmenAusRisikoanalyse": {
            "title": "Auswahl Maßnahmen aus der Risikoanalyse",
            "subchapterNumber": "4.1.5",
            "table": {
              "headers": ["Maßnahme", "Risikoanalyse", "Zielobjekt", "Begründung zur Auswahl"],
              "rows": []
            }
        }
      }
    },
    "vorOrtAudit": {
      "chapterNumber": "5.",
      "title": "Vor-Ort-Audit",
      "wirksamkeitSicherheitsmanagementsystem": {
          "title": "Wirksamkeit des Sicherheitsmanagementsystems",
          "subchapterNumber": "5.1",
          "content": [
              {"type": "question", "questionText": "Ist das ISMS effektiv und effizient im Einsatz? (Interview, Gesamteindruck)", "answer": null},
              {"type": "question", "questionText": "Ist der Sicherheitsprozess konform zum BSI-Standard 200-2, Kapitel 4 organisiert?", "answer": null},
              {"type": "question", "questionText": "Werden die in den Sicherheitsleitlinien vorgegebenen Ziele erreicht?", "answer": null},
              {"type": "question", "questionText": "Werden alle wichtigen Prozesse des Informationsverbundes dokumentiert?", "answer": null},
              {"type": "question", "questionText": "Ist der Sicherheitsprozess konform zum BSI-Standard 200-2, Kapitel 5 dokumentiert?", "answer": null},
              {"type": "question", "questionText": "Sind die Informationen konform zum BSI-Standard 200-2, Kapitel 5.1 klassifiziert?", "answer": null},
              {"type": "question", "questionText": "Wird der Verbesserungsprozess gelebt und das ISMS kontinuierlich verbessert?", "answer": null},
              {"type": "question", "questionText": "Erfolgt die Überprüfung des Informationssicherheitsprozesses anhand von Kennzahlen?", "answer": null},
              {"type": "question", "questionText": "Erfolgt die Bewertung des ISMS mit Hilfe eines Reifegradmodells?", "answer": null},
              {"type": "question", "questionText": "Wird die Umsetzung von Maßnahmen des Realisierungsplans überprüft?", "answer": null},
              {"type": "question", "questionText": "Erfolgt die Übernahme der Gesamtverantwortung durch eine Managementbewertung?", "answer": null},
              {"type": "prose", "label": "Datum der letzten Managementbewertung:", "text": ""},
              {"type": "finding", "label": "Feststellung:", "findingText": ""}
          ]
      },
      "aenderungenAmInformationsverbund": {
          "title": "Änderungen am Informationsverbund",
          "subchapterNumber": "5.2",
          "content": [
              {"type": "question", "questionText": "Liegen Änderungen am Informationsverbund vor?", "answer": null},
              {"type": "question", "questionText": "Sind alle Änderungen am Informationsverbund der Zertifizierungsstelle des BSI mitgeteilt wurden?", "answer": null},
              {"type": "question", "questionText": "Erfordern die Änderungen am Informationsverbund eine Rezertifizierung?", "answer": null},
              {"type": "question", "questionText": "Sind die Änderungen in der Dokumentation des Sicherheitskonzeptes kontinuierlich eingeflossen?", "answer": null},
              {"type": "question", "questionText": "Sind die dokumentierten Änderungen gemäß IT-Grundschutz-Methodik (BSI-Standard 200-2) und des IT-Grundschutz-Kompendiums umgesetzt?", "answer": null},
              {"type": "question", "questionText": "Wird durch den Wegfall / die Hinzunahme von Komponenten die Sicherheit des Informationsverbundes beeinträchtigt?", "answer": null},
              {"type": "finding", "label": "Feststellung:", "findingText": ""}
          ]
      },
      "behebungAbweichungen": {
          "title": "Behebung der Abweichungen und Empfehlungen",
          "subchapterNumber": "5.3",
          "content": [
              {"type": "question", "questionText": "Gab es im vorhergehenden Auditbericht Abweichungen und Empfehlungen?", "answer": null},
              {"type": "question", "questionText": "Sind alle Abweichungen fristgerecht behoben worden? Die Dokumentation hierzu befindet sich in Kapitel 7.2 'Abweichungen und Empfehlungen'.", "answer": null},
              {"type": "question", "questionText": "Wurden alle Empfehlungen angemessen berücksichtigt? Die Dokumentation hierzu befindet sich in Kapitel 7.2 'Abweichungen und Empfehlungen'.", "answer": null},
              {"type": "finding", "label": "Feststellung:", "findingText": ""}
          ]
      },
      "einhaltungAuflagen": {
          "title": "Einhaltung der Auflagen",
          "subchapterNumber": "5.4",
          "content": [
              {"type": "question", "questionText": "Gab es im vorhergehenden Audit Auflagen?", "answer": null}
          ],
          "table": {
              "headers": ["Nummer", "Beschreibung der Auflage", "Auflage erteilt am"],
              "rows": []
          },
          "bearbeitungAuflagen": {
              "content": [
                  {"type": "question", "questionText": "Wurden die Auflagen fristgerecht bearbeitet?", "answer": null},
                  {"type": "question", "questionText": "Wurden die Auflagen vollständig bearbeitet?", "answer": null},
                  {"type": "finding", "label": "Feststellung:", "findingText": ""}
              ]
          }
      },
      "verifikationDesITGrundschutzChecks": {
        "title": "Verifikation des IT-Grundschutz-Checks",
        "subchapterNumber": "5.5",
        "zusammenfassung": {
            "title": "Zusammenfassung des IT-Grundschutz-Checks",
            "subchapterNumber": "5.5.1",
            "content": [
                {"type": "question", "questionText": "Stimmt der im IT-Grundschutz-Check festgestellte Umsetzungsstatus zu den Anforderungen mit dem tatsächlich vorhandenen Informationssicherheitszustand des jeweiligen Zielobjekts überein?", "answer": null},
                {"type": "question", "questionText": "Ist die Begründung der entbehrlichen Anforderungen zulässig und nachvollziehbar? Der Auditor begründet diesen Sachverhalt explizit in der Bausteinprüfung (siehe Kapitel 5.5.2 'Einzelergebnisse des IT-Grundschutz-Checks')", "answer": null},
                {"type": "question", "questionText": "Sind alle Anforderungen mit dem Umsetzungsstatus 'teilweise' oder 'nein' im Referenzdokument A.6 enthalten?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "einzelergebnisse": {
          "title": "Einzelergebnisse des IT-Grundschutz-Checks",
          "subchapterNumber": "5.5.2",
          "bausteinPruefungen": [
            {
              "subchapterNumber": "5.5.2.1",
              "title": "Prüfung für Baustein: [Baustein-Name]",
              "baustein": "",
              "bezogenAufZielobjekt": "",
              "auditiertAm": "",
              "auditor": "",
              "befragtWurde": "",
              "anforderungen": [
                {
                  "nummer": "",
                  "anforderung": "",
                  "bewertung": "",
                  "dokuAntragsteller": "",
                  "pruefmethode": { "D": false, "I": false, "C": false, "S": false, "A": false, "B": false },
                  "auditfeststellung": "",
                  "abweichungen": ""
                }
              ]
            }
          ]
        },
        "ergebnis": {
            "title": "Ergebnis Verifikation des IT-Grundschutz-Checks",
            "subchapterNumber": "5.5.3",
            "content": [
                {"type": "prose", "label": "Votum Verifikation des IT-Grundschutz-Checks:", "text": ""}
            ]
        }
      },
      "risikoanalyseA5": {
        "title": "Risikoanalyse A.5",
        "subchapterNumber": "5.6",
        "zusammenfassungDerRisikoanalyse": {
            "title": "Zusammenfassung der Risikoanalyse",
            "subchapterNumber": "5.6.1",
            "content": [
                {"type": "question", "questionText": "Gibt es im Informationsverbund Zielobjekte mit einem Schutzbedarf 'hoch' oder 'sehr hoch'?", "answer": null},
                {"type": "question", "questionText": "Stimmt der im IT-Grundschutz-Check festgestellte Umsetzungsstatus zusätzlicher Maßnahmen für Zielobjekte mit hohem oder sehr hohem Schutzbedarf mit dem tatsächlich vorhandenen Informationssicherheitszustand des jeweiligen Zielobjekts überein?", "answer": null},
                {"type": "question", "questionText": "Sind alle zusätzlichen Maßnahmen aus der Risikoanalyse als umgesetzt gekennzeichnet?", "answer": null},
                {"type": "question", "questionText": "Sind alle zusätzlichen Maßnahmen mit dem Umsetzungsstatus 'teilweise' oder 'nein' im Referenzdokument A.6 enthalten?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "einzelergebnisseDerRisikoanalyse": {
            "title": "Einzelergebnisse der Risikoanalyse",
            "subchapterNumber": "5.6.2",
            "massnahmenPruefungen": [
              {
                "massnahme": "",
                "zielobjekt": "",
                "bewertung": "",
                "dokuAntragsteller": "",
                "pruefmethode": { "D": false, "I": false, "C": false, "S": false, "A": false, "B": false },
                "auditfeststellung": ""
              }
            ]
        },
        "ergebnisDerRisikoanalyse": {
            "title": "Ergebnis Risikoanalyse",
            "subchapterNumber": "5.6.3",
            "content": [
                {"type": "prose", "label": "Votum Risikoanalyse:", "text": ""}
            ]
        }
      }
    },
    "gesamtvotum": {
        "chapterNumber": "6.",
        "title": "Gesamtvotum",
        "gesamteinschaetzung": "",
        "votum": {
          "entspricht": null,
          "datum": "",
          "unterschrift": "Unterschrift des Auditteamleiters"
        }        
    },
    "anhang": {
      "chapterNumber": "7.",
      "title": "Anhang",
      "referenzdokumente": {
        "title": "Referenzdokumente",
        "subchapterNumber": "7.1",
        "table": {
          "headers": ["Nummer", "Kurzbezeichnung", "Dateiname / Verweis", "Version, Datum", "Relevante Änderungen"],
          "rows": []
        }
      },
      "abweichungenUndEmpfehlungen": {
        "title": "Abweichungen und Empfehlungen",
        "subchapterNumber": "7.2",
        "schwerwiegendeAbweichungen": {
          "title": "Schwerwiegende Abweichungen",
          "table": {
            "headers": ["Nummer", "Beschreibung der Abweichung", "Quelle (Kapitel)", "Behebungsfrist", "Status"],
            "rows": []
          }
        },
        "geringfuegigeAbweichungen": {
          "title": "Geringfügige Abweichungen",
          "table": {
            "headers": ["Nummer", "Beschreibung der Abweichung", "Quelle (Kapitel)", "Behebungsfrist", "Status"],
            "rows": []
          }
        },
        "empfehlungen": {
          "title": "Empfehlungen",
          "table": {
            "headers": ["Nummer", "Beschreibung der Empfehlung", "Quelle (Kapitel)", "Behebungsfrist", "Status"],
            "rows": []
          }
        }
      }
    }
  }
}
==== bsi-audit-automator/assets/json/prompt_config.json ====
{
  "system_message": "You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.\n\nRules:\n1. Imperative: Based *only* on the attached customer documentation files or provided context, answer the following questions!\n2. Imperative: If the attached documents are insufficient to answer a question, state that clearly in your answer (e.g., \"Cannot be determined from the provided documents.\").\n3. While you try to answer the questions, check if the documents contain deviations from BSI Grundschutz.\n4. Generate a finding for each deviation.\n5. Categorize the finding as 'AG' (Minor Deviation), 'AS' (Major Deviation, very seldom, use carefully!), 'E' (Recommendation).\n6. Provide evidence and a reference to the specific document that caused the finding in the finding's description.\n\nYour response MUST be a single JSON object that strictly adheres to the provided JSON schema.\n6. You answer in professional german, you are precise and on the point with ecamples and reasons for your decision.",
  "stages": {
    "ETL": {
      "classify_documents": {
        "prompt": "You are an expert BSI (German Federal Office for Information Security) audit assistant. Your task is to analyze a list of document filenames and classify each one into a single, most appropriate category from a predefined list. The filenames often contain clues like \"A.1\", \"A.4\", \"Netzplan\", \"Sicherheitsleitlinie\", \"Auditbericht\", etc.\n\n**Predefined Categories:**\n- \"Sicherheitsleitlinie\": The main, high-level security policy document.\n- \"Organisations-Richtlinie\": Other policies, guidelines, or organizational rules.\n- \"Informationsverbund\": Documents describing the scope and boundary of the information network.\n- \"Netzplan\": Network diagrams or topology plans.\n- \"Strukturanalyse\": The core structural analysis document (often A.1).\n- \"Schutzbedarfsfeststellung\": Protection needs assessment document (often A.2).\n- \"Modellierung\": The IT Grundschutz modeling document (often A.3).\n- \"Grundschutz-Check\": The implementation check of controls (often A.4).\n- \"Risikoanalyse\": Risk analysis documents (often A.5).\n- \"Realisierungsplan\": The risk treatment or implementation plan (often A.6).\n- \"Dienstleister-Liste\": Lists of external service providers.\n- \"Vorheriger-Auditbericht\": A previous, complete audit report.\n- \"Sonstiges\": Any other document that does not fit the above categories.\n\nAnalyze the following list of filenames and return a structured JSON response mapping each filename to its category.\n\n**Filenames to Classify:**\n---\n{filenames_json}\n---",
        "schema_path": "assets/schemas/etl_classify_documents_schema.json"
      }
    },
    "Chapter-3-Ground-Truth": {
      "extract_zielobjekte": {
        "prompt": "You are an expert data extraction system. From the attached Strukturanalyse document (A.1), extract a complete list of all Zielobjekte (target objects). For each Zielobjekt, provide its unique ID (Kürzel) and its full descriptive name (Name).",
        "schema_path": "assets/schemas/stage_3_gt_zielobjekte_schema.json"
      },
      "extract_baustein_mappings": {
        "prompt": "You are an expert data extraction system. From the attached Modellierung document (A.3), analyze which Zielobjekt (target object) each Baustein (building block) is applied to. Return a list of mappings, where each mapping contains the Baustein ID and the Kürzel (unique ID) of the Zielobjekt it's mapped to. Ignore Bausteine from layers ISMS, ORP, CON, OPS, and DER.",
        "schema_path": "assets/schemas/stage_3_gt_baustein_mappings_schema.json"
      }
    },
    "Chapter-1": {
      "informationsverbund": {
        "prompt": "Your task is to analyze the provided context, which describes the scope of an information security network (Informationsverbund), and extract two key pieces of information:\n1.  **kurzbezeichnung**: A short, official name or title for the Informationsverbund.\n2.  **kurzbeschreibung**: A concise paragraph summarizing the scope, including key business processes, applications, and physical locations. This should also serve as the main description for the 'Geltungsbereich'.\n\nIf the provided context is insufficient or empty, state that the scope could not be fully determined from the documents and must be clarified manually.",
        "schema_path": "assets/schemas/stage_1_4_informationsverbund_schema.json"
      }
    },
    "Chapter-3": {
      "generic_question": {
        "prompt": "The questions to answer are:\n{questions}"
      },
      "targeted_question": {
        "prompt": "You are a BSI auditor. Based *only* on the following structured JSON data and, if provided, the attached context document(s), answer the question: {question}\n\n**JSON Data:**\n---\n{json_data}\n---"
      },
      "generic_summary": {
        "prompt": "You are a BSI security auditor providing a final verdict on the {summary_topic}.\nBased on the summary of findings from the previous sections provided below, provide a summary verdict (\"Votum\").\n\n**Summary of Previous Findings:**\n---\n{previous_findings}\n---"
      },
      "detailsZumItGrundschutzCheck_extraction": {
        "prompt": "You are an expert system for refining BSI Grundschutz data. Your input is a JSON of entities extracted by the Google Cloud Document AI Form Parser. Your task is to analyze these entities and assemble them into a final, structured list of requirements according to the provided schema. Use the provided `system_map.json` to infer the correct `Zielobjekt` for each requirement based on its ID prefix and the page number context. Group related entities (ID, title, status, explanation, date) into single requirement objects.\n\n**System Map (Ground Truth):**\n---\n{ground_truth_map_json}\n---\n\n**Document AI Extracted Entities:**\n---\n{document_ai_json}\n---",
        "prompt_old": "You are an expert data extraction system. Your task is to extract requirements and headings from the provided document. Use the authoritative list of Zielobjekte provided below to help you identify headings.\n\n**Authoritative List of Zielobjekte (Ground Truth):**\n---\n{zielobjekte_list_json}\n---\n\n**Extraction Rules:**\n1.  **Extract `chapter_headings`:** These are lines identifying a new Zielobjekt, like 'S-001 Windows Server'. Extract the Kürzel and Name. You MUST find these in the provided list.\n2.  **Extract `anforderungen`:** These are the security requirements. For each, extract the ID, title, status, and explanation.\n3.  **CRITICAL RULE - Direct Association:** If an `Anforderung` appears on the same page as, and *after*, a `chapter_heading`, you MUST add the `zielobjekt_kuerzel` and `zielobjekt_name` from that heading directly to that `Anforderung` object.\n4.  **Page Numbers:** You MUST provide the page number for every item you extract.\n5.  **Do NOT extract Baustein IDs** (kuerzel NOT LIKE 'ISMS.1','SYS.1.1', 'SYS.1.3', 'SYS.1.5', 'SYS.1.8', 'SYS.4.3' etc) or Anforderung IDs (kuerzel NOT LIKE 'ISMS.1.A1') as `chapter_headings`. Headings are ONLY for Zielobjekte from the provided list.\n**Greedy Rule**: In doubt, take the chapter_heading.\n**Hint**:chapter_headings are usually in a sligtly larger or differnt font than the rest of the text.\n\n\nReturn the result as a single JSON object.",
        "schema_path": "assets/schemas/stage_3_6_1_extract_check_data_schema.json"
      },
      "aktualitaetDerReferenzdokumente": {
        "schema_path": "assets/schemas/stage_3_1_aktualitaet_schema.json",
        "prompt": "Ignore the missing A.4 GRundschutz-Check.",
        "source_categories": ["Sicherheitsleitlinie", "Organisations-Richtlinie", "Netzplan", "Strukturanalyse", "Dienstleister-Liste", "Realisierungsplan", "Risikoanalyse", "Schutzbedarfsfeststellung", "Modellierung"]
      },
      "sicherheitsleitlinieUndRichtlinienInA0": {
        "schema_path": "assets/schemas/stage_3_2_sicherheitsleitlinie_schema.json",
        "source_categories": ["Sicherheitsleitlinie", "Organisations-Richtlinie"]
      },
      "definitionDesInformationsverbundes": {
        "schema_path": "assets/schemas/stage_3_3_1_informationsverbund_schema.json",
        "source_categories": ["Informationsverbund", "Strukturanalyse"]
      },
      "bereinigterNetzplan": {
        "schema_path": "assets/schemas/stage_3_3_2_netzplan_schema.json",
        "source_categories": ["Netzplan", "Strukturanalyse"]
      },
      "listeDerGeschaeftsprozesse": {
        "schema_path": "assets/schemas/stage_3_3_3_geschaeftsprozesse_schema.json",
        "source_categories": ["Strukturanalyse"]
      },
      "listeDerAnwendungen": {
        "schema_path": "assets/schemas/generic_1_question_schema.json",
        "source_categories": ["Strukturanalyse"]
      },
      "listeDerItSysteme": {
        "schema_path": "assets/schemas/generic_1_question_schema.json",
        "source_categories": ["Strukturanalyse"]
      },
      "listeDerRaeumeGebaeudeStandorte": {
        "schema_path": "assets/schemas/generic_1_question_schema.json",
        "source_categories": ["Strukturanalyse"]
      },
      "listeDerKommunikationsverbindungen": {
        "schema_path": "assets/schemas/generic_1_question_schema.json",
        "source_categories": ["Strukturanalyse"]
      },
      "listeDerDienstleister": {
        "schema_path": "assets/schemas/generic_1_question_schema.json",
        "source_categories": ["Strukturanalyse", "Dienstleister-Liste"]
      },
      "stichprobenDokuStrukturanalyse": {
        "schema_path": "assets/schemas/generic_0_question_schema.json",
        "source_categories": ["Strukturanalyse"]
       },
      "stichprobenDokuSchutzbedarf": {
         "schema_path": "assets/schemas/generic_0_question_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },      
      "ergebnisDerStrukturanalyse": {
        "type": "summary",
        "schema_path": "assets/schemas/stage_3_summary_schema.json"
      },
      "definitionDerSchutzbedarfskategorien": {
        "schema_path": "assets/schemas/stage_3_4_1_schutzbedarfskategorien_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },
      "schutzbedarfGeschaeftsprozesse": {
        "schema_path": "assets/schemas/generic_2_question_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },
      "schutzbedarfAnwendungen": {
        "schema_path": "assets/schemas/generic_2_question_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },
      "schutzbedarfItSysteme": {
        "schema_path": "assets/schemas/generic_2_question_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },
      "schutzbedarfRaeume": {
        "schema_path": "assets/schemas/generic_2_question_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },
      "schutzbedarfKommunikationsverbindungen": {
        "schema_path": "assets/schemas/generic_2_question_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },
      "ergebnisDerSchutzbedarfsfeststellung": {
        "type": "summary",
        "schema_path": "assets/schemas/stage_3_summary_schema.json"
      },
      "modellierungsdetails": {
        "schema_path": "assets/schemas/stage_3_5_1_modellierungsdetails_schema.json",
        "prompt": "The questions to answer are:\n{questions}\n\nAdditionally, review the attached Modellierung document against the authoritative list of Zielobjekte provided below. Does the Modellierung correctly account for all Zielobjekte from this list that are relevant?\n\n**Authoritative List of Zielobjekte (Ground Truth):**\n---\n{zielobjekte_json}\n---",
        "source_categories": ["Modellierung"]
      },
      "ergebnisDerModellierung": {
        "type": "summary",
        "schema_path": "assets/schemas/stage_3_summary_schema.json"
      },
      "detailsZumItGrundschutzCheck": {
        "type": "custom_logic"
      },
      "benutzerdefinierteBausteine": {
        "schema_path": "assets/schemas/generic_2_question_schema.json",
        "source_categories": ["Modellierung"]
      },
      "ergebnisItGrundschutzCheck": {
        "type": "summary",
        "schema_path": "assets/schemas/stage_3_summary_schema.json"
      },
      "risikoanalyse": {
        "schema_path": "assets/schemas/stage_3_7_risikoanalyse_schema.json",
        "source_categories": ["Risikoanalyse"]
      },
      "questions": {
        "entbehrlich": "Sind die Begründungen für 'entbehrlich' plausibel? BSI-Regel: 1. Wenn eine alternative Schutzmaßnahme beschrieben ist\n2. Eine Anforderung mit Level 5 ist immer entbehrlich, **außer** wenn sie durch in der beigefügten Risikoanalyse explizit gefordert wird. Eine Formulierung, dass diese Anforderung nicht von der Risikoanalyse gefordert wird, ist also für Anforderungen im Level 5 immer akzeptabel! Auch ist es akzeptabel, wenn für Level 5 keine Begründung für Entbehrlich vorhanden ist!\n\nFüge eine Liste der nicht ausreichenden begründeten Entbehrlichen Anforderungen mit begründung der Feststellung hinzu!",
        "muss_anforderungen": "Sind alle diese MUSS-Anforderungen (Level 1) mit Status 'Ja' umgesetzt? Füge eine Liste der nicht umgesetzten MUSS Anforderungen der Feststellung hinzu!",
        "nicht_umgesetzt": "Sind diese nicht oder teilweise umgesetzten Anforderungen im angehängten Realisierungsplan (A.6) dokumentiert? Füge eine kurze Auswahl der im A.6 enthaltenen nicht umgesetzten Anforderungen der Feststellung hinzu!"
      },
      "realisierungsplan": {
        "schema_path": "assets/schemas/generic_3_question_schema.json",
        "source_categories": ["Realisierungsplan"]
      },
      "ergebnisDerDokumentenpruefung": {
        "type": "summary",
        "schema_path": "assets/schemas/stage_3_9_ergebnis_schema.json"
      }
    },
    "Chapter-4": {
      "auswahlBausteineErstRezertifizierung": {
        "key": "4.1.1",
        "prompt": "You are a BSI Lead Auditor planning an initial certification audit (Erstzertifizierung). Your plan must strictly adhere to the rules outlined in the official BSI \"Auditierungsschema\".\n\n**CRITICAL INSTRUCTION:** Your plan MUST be based on the customer's actual system structure provided below in the 'System Structure Map'. When selecting a Zielobjekt, you MUST use the corresponding `name` and `kuerzel` from the `zielobjekte` list. The selected `kuerzel` MUST be one that the chosen Baustein is actually mapped to in the `baustein_to_zielobjekt_mapping`.\n\n**System Structure Map (Ground Truth):**\n---\n{ground_truth_map_json}\n---\n\n**Official Rules for Baustein Selection (from Auditierungsschema, Chapter 4.3):**\n1.  **Minimum Count:** You MUST audit at least 6 Bausteine.\n2.  **Mandatory Baustein:** The Baustein 'ISMS.1 Sicherheitsmanagement' MUST be included in your selection. Its Zielobjekt is ALWAYS 'Gesamter Informationsverbund' (Kürzel: 'Informationsverbund').\n3.  **Risk-Oriented Selection:** You must select the other Bausteine based on risk, ensuring that you cover a variety of layers (Schichten) like ORP, CON, OPS, NET, INF, and SYS.\n4.  **Justification:** You MUST provide a concise, professional justification ('Begründung zur Auswahl') for the selection of each Baustein. The justification must be a full sentence explaining the risk-based reason for the selection. Example: 'Überprüfung der zentralen Firewall-Regeln, da diese den gesamten Informationsverbund nach außen absichern.'\n\nBased on the ground-truth map and the official rules, generate a realistic, accurate, and compliant audit plan with the required columns 'Schicht', 'Baustein', 'Zielobjekt-Name', 'Zielobjekt-Kürzel', and 'Begründung zur Auswahl'.",
        "schema_path": "assets/schemas/stage_4_1_1_auswahl_bausteine_erst_schema.json"
      },
      "auswahlBausteine1Ueberwachungsaudit": {
        "key": "4.1.2",
        "prompt": "You are a BSI Lead Auditor planning the **first surveillance audit (1. Überwachungsaudit)**. Your plan must strictly adhere to the rules outlined in the official BSI \"Auditierungsschema\".\n\n**CRITICAL INSTRUCTION:** Your plan MUST be based on the customer's actual system structure provided below in the 'System Structure Map'. When selecting a Zielobjekt, you MUST use the corresponding `name` and `kuerzel` from the `zielobjekte` list. The selected `kuerzel` MUST be one that the chosen Baustein is actually mapped to in the `baustein_to_zielobjekt_mapping`.\n\n**System Structure Map (Ground Truth):**\n---\n{ground_truth_map_json}\n---\n\n**Official Rules for Baustein Selection for a Surveillance Audit:**\n1.  **Mandatory Baustein:** The Baustein 'ISMS.1 Sicherheitsmanagement' MUST be included. Its Zielobjekt is ALWAYS 'Gesamter Informationsverbund' (Kürzel: 'Informationsverbund').\n2.  **Minimum Count:** You MUST select at least **two** other Bausteine in addition to ISMS.1.\n3.  **Risk-Oriented Selection:** You must select the other Bausteine based on significant changes, previous deviations, or risk. Ensure you cover a variety of layers (Schichten) like ORP, CON, OPS, NET, INF, and SYS.\n4.  **Justification:** You must provide a concise, professional justification (Begründung zur Auswahl) for the selection of each Baustein.\n\nBased on the ground-truth map and the official rules, generate a realistic, accurate, and compliant audit plan with the required columns 'Schicht', 'Baustein', 'Zielobjekt-Name', 'Zielobjekt-Kürzel', and 'Begründung zur Auswahl'.",
        "schema_path": "assets/schemas/stage_4_1_2_auswahl_bausteine_ueberwachung_schema.json"
      },
        "auswahlBausteine2Ueberwachungsaudit": {
          "key": "4.1.3",
          "prompt": "You are a BSI Lead Auditor planning the **second surveillance audit (2. Überwachungsaudit)**. Your plan must strictly adhere to the rules outlined in the official BSI \"Auditierungsschema\".\n\n**CRITICAL INSTRUCTION:** Your plan MUST be based on the customer's actual system structure provided below in the 'System Structure Map'. When selecting a Zielobjekt, you MUST use the corresponding `name` and `kuerzel` from the `zielobjekte` list. The selected `kuerzel` MUST be one that the chosen Baustein is actually mapped to in the `baustein_to_zielobjekt_mapping`.\n\n**System Structure Map (Ground Truth):**\n---\n{ground_truth_map_json}\n---\n\n**Official Rules for Baustein Selection for a Surveillance Audit:**\n1.  **Mandatory Baustein:** The Baustein 'ISMS.1 Sicherheitsmanagement' MUST be included. Its Zielobjekt is ALWAYS 'Gesamter Informationsverbund' (Kürzel: 'Informationsverbund').\n2.  **Minimum Count:** You MUST select at least **two** other Bausteine in addition to ISMS.1.\n3.  **No Repeats (where possible):** The selected Bausteine (other than ISMS.1) should ideally be different from those chosen in the first surveillance audit to ensure broad coverage over the certification lifecycle.\n4.  **Risk-Oriented Selection:** You must select the other Bausteine based on significant changes, previous deviations, or risk. Ensure you cover a variety of layers (Schichten) like ORP, CON, OPS, NET, INF, and SYS.\n5.  **Justification:** You must provide a concise, professional justification (Begründung zur Auswahl) for the selection of each Baustein.\n\nBased on the ground-truth map and the official rules, generate a realistic, accurate, and compliant audit plan with the required columns 'Schicht', 'Baustein', 'Zielobjekt-Name', 'Zielobjekt-Kürzel', and 'Begründung zur Auswahl'.",
          "schema_path": "assets/schemas/stage_4_1_3_auswahl_bausteine_ueberwachung_schema.json"
      },
      "auswahlMassnahmenAusRisikoanalyse": {
        "key": "4.1.5",
        "prompt": "You are a BSI Lead Auditor planning an audit. Your task is to select a small, representative sample of 2-3 additional security measures that would typically result from a risk analysis for assets with \"high\" or \"very high\" protection needs. These are measures BEYOND the standard IT-Grundschutz baseline requirements.\n\nFor each selected measure, provide a plausible and risk-oriented justification (key: 'Begründung zur Auswahl') for its inclusion in the on-site audit plan.\n\nExamples of good justifications:\n- \"To verify the effectiveness of the advanced DDoS mitigation for the public-facing web application, which is a critical business process.\"\n- \"To check the implementation of database encryption, as this measure was identified to protect highly sensitive customer PII.\"\n- \"To confirm the sandboxing of the email attachment analysis system, a key control against advanced persistent threats.\"",
        "schema_path": "assets/schemas/stage_4_1_5_auswahl_massnahmen_risiko_schema.json"
      }
    },
    "Scan-Report": {
      "extract_chapter_1": {
        "prompt": "You are an expert data extraction system. From the attached previous audit report, extract the complete content of the tables in subchapters 1.1 (Versionshistorie), 1.2 (Kontaktinformationen des Antragstellers AND Ansprechpartner), and 1.3 (Auditteam). The auditteam has multiple sub-tables; capture all of them.",
        "schema_path": "assets/schemas/scan_report_ch1_schema.json"
      },
      "extract_chapter_4": {
        "prompt": "You are an expert data extraction system. From the attached previous audit report, extract the 'Auswahl Bausteine' tables from subchapters 4.1.1 and 4.1.2, and the 'Auswahl Standorte' table from 4.1.4. Capture all rows from all three tables.",
        "schema_path": "assets/schemas/scan_report_ch4_schema.json"
      },
      "extract_chapter_7": {
        "prompt": "From the attached audit report, find chapter 7.2 'Abweichungen und Empfehlungen'. Scan this chapter and its sub-chapters for any tables containing findings. These tables might be titled 'Abweichungen', 'Schwerwiegende Abweichungen', 'Geringfügige Abweichungen', or 'Empfehlungen'. Extract every single finding you can identify into a single, flat list. For each finding, determine its category ('AS' for Schwerwiegend, 'AG' for Geringfügig, or 'E' for Empfehlung) and extract all its corresponding table columns like 'Nummer', 'Beschreibung', 'Quelle', 'Behebungsfrist', and 'Status'.",
        "schema_path": "assets/schemas/scan_report_ch7_schema.json"
      }
    }
  }
}
==== bsi-audit-automator/assets/schemas/bsi_gk_2023_oscal_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://example.com/oscal/bsi-grundschutz-catalog.final.schema.json",
  "title": "Final Valid OSCAL-Compliant BSI Grundschutz Catalog Schema",
  "description": "A corrected, valid OSCAL schema. This version correctly defines the 'parts' property within a group, making it fully compliant with OSCAL 1.1.2. It now includes 'prose_qs' for generated questions.",
  "type": "object",
  "required": ["catalog"],
  "properties": {
    "catalog": {
      "title": "OSCAL Catalog",
      "type": "object",
      "required": ["uuid", "metadata", "groups"],
      "properties": {
        "uuid": { "$ref": "#/definitions/common/uuid" },
        "metadata": { "$ref": "#/definitions/oscal/metadata" },
        "groups": { "type": "array", "items": { "$ref": "#/definitions/oscal/group" } }
      },
      "additionalProperties": false
    }
  },
  "definitions": {
    "oscal": {
      "metadata": {
        "title": "OSCAL Metadata",
        "type": "object",
        "required": ["title", "last-modified", "version", "oscal-version"],
        "properties": {
          "title": { "type": "string" },
          "last-modified": { "$ref": "#/definitions/common/dateTime" },
          "version": { "type": "string" },
          "oscal-version": { "type": "string", "const": "1.1.2" }
        },
        "additionalProperties": false
      },
      "group": {
        "title": "OSCAL Control Group",
        "description": "Represents a BSI layer (e.g., 'OPS') or a nested 'Baustein'. Uses 'id' for identification.",
        "type": "object",
        "required": ["id", "title"],
        "properties": {
          "id": { "$ref": "#/definitions/common/token" },
          "class": { "$ref": "#/definitions/common/token" },
          "title": { "type": "string" },
          "parts": {
            "description": "Contextual parts for a Baustein, like introduction or risks.",
            "type": "array",
            "items": { "$ref": "#/definitions/oscal/part" }
          },
          "groups": {
            "type": "array",
            "items": { "$ref": "#/definitions/oscal/group" }
          },
          "controls": {
            "type": "array",
            "items": { "$ref": "#/definitions/oscal/control" }
          }
        },
        "additionalProperties": false
      },
      "control": {
        "title": "OSCAL Control",
        "description": "A single BSI requirement.",
        "type": "object",
        "required": ["id", "title"],
        "properties": {
          "id": { "$ref": "#/definitions/common/token" },
          "class": { "$ref": "#/definitions/common/token" },
          "title": { "type": "string" },
          "props": { "type": "array", "items": { "$ref": "#/definitions/oscal/property" } },
          "parts": { "type": "array", "items": { "$ref": "#/definitions/oscal/part" } }
        },
        "additionalProperties": false
      },
      "part": {
        "title": "OSCAL Part",
        "description": "A textual component of a control or group.",
        "type": "object",
        "required": ["name"],
        "properties": {
          "id": { "$ref": "#/definitions/common/token" },
          "name": { "$ref": "#/definitions/common/token" },
          "title": { "type": "string" },
          "class": { "$ref": "#/definitions/common/token" },
          "prose": { "type": "string" },
          "prose_qs": { "type": "string" },
          "parts": { "type": "array", "items": { "$ref": "#/definitions/oscal/part" } }
        },
        "additionalProperties": false
      },
      "property": { "type": "object", "required": ["name", "value"], "properties": { "name": { "$ref": "#/definitions/common/token" }, "value": { "type": "string" }, "ns": { "type": "string", "format": "uri" } }, "additionalProperties": false }
    },
    "common": {
      "uuid": { "type": "string", "format": "uuid", "pattern": "^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$" },
      "token": { "type": "string", "pattern": "^\\S(.*\\S)?$" },
      "dateTime": { "type": "string", "format": "date-time" }
    }
  }
}
==== bsi-audit-automator/assets/schemas/etl_classify_documents_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Document Classification Schema",
    "description": "A schema for the classified list of source documents.",
    "type": "object",
    "properties": {
      "document_map": {
        "type": "array",
        "description": "An array mapping each source document to its BSI category.",
        "items": {
          "type": "object",
          "properties": {
            "filename": {
              "type": "string",
              "description": "The full original filename from GCS."
            },
            "category": {
              "type": "string",
              "description": "The BSI document category.",
              "enum": [
                "Sicherheitsleitlinie",
                "Organisations-Richtlinie",
                "Informationsverbund",
                "Netzplan",
                "Strukturanalyse",
                "Schutzbedarfsfeststellung",
                "Modellierung",
                "Grundschutz-Check",
                "Risikoanalyse",
                "Realisierungsplan",
                "Dienstleister-Liste",
                "Vorheriger-Auditbericht",
                "Sonstiges"
              ]
            }
          },
          "required": ["filename", "category"]
        }
      }
    },
    "required": ["document_map"]
  }
==== bsi-audit-automator/assets/schemas/generic_0_question_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic Schema for a finding only",
    "description": "A schema for a response that only contains a structured audit finding.",
    "type": "object",
    "properties": {
      "finding": {
        "type": "object",
        "properties": {
          "category": { "type": "string", "enum": ["AG", "AS", "E", "OK"] },
          "description": { "type": "string" }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["finding"]
  }
==== bsi-audit-automator/assets/schemas/generic_1_question_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic Schema for a Single Boolean Question",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "Answer to the question.",
        "items": { "type": "boolean" },
        "minItems": 1,
        "maxItems": 1
      },
      "finding": {
        "type": "object",
        "properties": {
          "category": { "type": "string", "enum": ["AG", "AS", "E", "OK"] },
          "description": { "type": "string" }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/generic_2_question_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic Schema for Two Boolean Questions",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "Answers to the questions.",
        "items": { "type": "boolean" },
        "minItems": 2,
        "maxItems": 2
      },
      "finding": {
        "type": "object",
        "properties": {
          "category": { "type": "string", "enum": ["AG", "AS", "E", "OK"] },
          "description": { "type": "string" }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/generic_3_question_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic 3-Question Response",
    "description": "A generic schema for answering 3 questions and providing a single audit finding.",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "An array of 3 answers, one for each question asked. The order must be preserved.",
        "items": { "type": "string" },
        "minItems": 3,
        "maxItems": 3
      },
      "finding": {
        "type": "object",
        "description": "A single, structured finding for this section.",
        "properties": {
          "category": {
            "type": "string",
            "description": "Category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK'.",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string",
            "description": "A clear, concise description of the finding, including evidence and document reference."
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/generic_4_question_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic 4-Question Response",
    "description": "A generic schema for answering 4 questions and providing a single audit finding.",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "An array of 4 answers, one for each question asked. The order must be preserved.",
        "items": { "type": "string" },
        "minItems": 4,
        "maxItems": 4
      },
      "finding": {
        "type": "object",
        "description": "A single, structured finding for this section.",
        "properties": {
          "category": {
            "type": "string",
            "description": "Category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK'.",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string",
            "description": "A clear, concise description of the finding, including evidence and document reference."
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/generic_5_question_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic 5-Question Response",
    "description": "A generic schema for answering 5 questions and providing a single audit finding.",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "An array of 5 answers, one for each question asked. The order must be preserved.",
        "items": { "type": "string" },
        "minItems": 5,
        "maxItems": 5
      },
      "finding": {
        "type": "object",
        "description": "A single, structured finding for this section.",
        "properties": {
          "category": {
            "type": "string",
            "description": "Category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK'.",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string",
            "description": "A clear, concise description of the finding, including evidence and document reference."
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/generic_summary_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic Summary Votum Response",
    "description": "A schema for a summary section that provides a final verdict ('Votum') and a structured finding.",
    "type": "object",
    "properties": {
      "votum": {
        "type": "string",
        "description": "The summary verdict or assessment for the entire section."
      },
      "finding": {
        "type": "object",
        "description": "A single, overarching finding for the entire summarized section. Set to 'OK' if the overall status is compliant.",
        "properties": {
          "category": {
            "type": "string",
            "description": "The overall category for the summary: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK'.",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string",
            "description": "A clear, concise description of the overall finding for the section."
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["votum", "finding"]
  }
==== bsi-audit-automator/assets/schemas/scan_report_ch1_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Scanned Previous Report Data (Chapter 1)",
    "description": "Extracts key informational tables from Chapter 1 of a previous audit report.",
    "type": "object",
    "properties": {
        "versionshistorie": {
            "type": "object",
            "properties": {
                "table": {
                    "type": "object",
                    "properties": {
                        "rows": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "Datum": { "type": "string" },
                                    "Version": { "type": "string" },
                                    "Verfasser": { "type": "string" },
                                    "Bemerkungen": { "type": "string" }
                                },
                                "required": ["Datum", "Version", "Verfasser", "Bemerkungen"]
                            }
                        }
                    },
                    "required": ["rows"]
                }
            },
            "required": ["table"]
        },
        "auditierteInstitution": {
            "type": "object",
            "properties": {
                "kontaktinformationenAntragsteller": {
                    "type": "object",
                    "properties": {
                        "table": {
                            "type": "object",
                            "properties": {
                                "rows": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "Institution": { "type": "string" },
                                            "Straße / Hausnummer": { "type": "string" },
                                            "PLZ / Ort": { "type": "string" },
                                            "E-Mail": { "type": "string" }
                                        },
                                        "required": ["Institution", "Straße / Hausnummer", "PLZ / Ort", "E-Mail"]
                                    }
                                }
                            },
                            "required": ["rows"]
                        }
                    },
                    "required": ["table"]
                },
                "ansprechpartnerZertifizierung": {
                    "type": "object",
                    "properties": {
                        "table": {
                            "type": "object",
                            "properties": {
                                "rows": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "Name": { "type": "string" },
                                            "Funktion": { "type": "string" },
                                            "Telefon": { "type": "string" },
                                            "E-Mail": { "type": "string" },
                                            "Optional: Abweichende Anschrift": { "type": "string" }
                                        }
                                    }
                                }
                            },
                            "required": ["rows"]
                        }
                    },
                    "required": ["table"]
                }
            },
            "required": ["kontaktinformationenAntragsteller", "ansprechpartnerZertifizierung"]
        },
        "auditteam": {
            "type": "object",
            "description": "Contains all sub-tables related to the audit team. Properties are optional.",
            "properties": {
                "auditteamleiter": {
                    "type": "object",
                    "properties": {
                        "table": {
                            "type": "object",
                             "properties": {
                                "rows": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "Name": { "type": "string" },
                                            "Institution": { "type": "string" }
                                        }
                                    }
                                }
                            },
                            "required": ["rows"]
                        }
                    }
                },
                "fachexperte": {
                    "type": "object",
                    "properties": {
                        "table": {
                            "type": "object",
                             "properties": {
                                "rows": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "Name": { "type": "string" },
                                            "Institution": { "type": "string" }
                                        }
                                    }
                                }
                            },
                            "required": ["rows"]
                        }
                    }
                },
                "auditor": {
                    "type": "object",
                    "properties": {
                        "table": {
                            "type": "object",
                             "properties": {
                                "rows": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "Name": { "type": "string" },
                                            "Institution": { "type": "string" }
                                        }
                                    }
                                }
                            },
                            "required": ["rows"]
                        }
                    }
                }
            }
        }
    },
    "required": ["versionshistorie", "auditierteInstitution", "auditteam"]
}
==== bsi-audit-automator/assets/schemas/scan_report_ch4_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Scanned Previous Report Data (Chapter 4)",
    "description": "Extracts previous audit scope tables from Chapter 4.",
    "type": "object",
    "properties": {
    "auswahlBausteineErstRezertifizierung": {
    "type": "object",
    "properties": {
    "table": {
    "type": "object",
    "properties": {
    "rows": {
    "type": "array",
    "items": {
    "type": "object",
    "properties": {
    "Schicht": { "type": "string" },
    "Baustein": { "type": "string" },
    "Zielobjekt-Name": { "type": "string" },
    "Begründung zur Auswahl": { "type": "string" }
    },
    "required": ["Schicht", "Baustein", "Zielobjekt", "Begründung zur Auswahl"]
    }
    }
    },
    "required": ["rows"]
    }
    }
    },
    "auswahlBausteine1Ueberwachungsaudit": {
    "type": "object",
    "properties": {
    "table": {
    "type": "object",
    "properties": {
    "rows": {
    "type": "array",
    "items": {
    "type": "object",
    "properties": {
    "Schicht": { "type": "string" },
    "Baustein": { "type": "string" },
    "Zielobjekt-Name": { "type": "string" },
    "Begründung zur Auswahl": { "type": "string" }
    },
    "required": ["Schicht", "Baustein", "Zielobjekt", "Begründung zur Auswahl"]
    }
    }
    },
    "required": ["rows"]
    }
    }
    },
    "auswahlStandorte": {
    "type": "object",
    "properties": {
    "table": {
    "type": "object",
    "properties": {
    "rows": {
    "type": "array",
    "items": {
    "type": "object",
    "properties": {
    "Standort": { "type": "string" },
    "Erst- bzw. Rezertifizierung": { "type": "string" },
    "1. Überwachungsaudit": { "type": "string" },
    "2. Überwachungsaudit": { "type": "string" },
    "Begründung für die Auswahl": { "type": "string" }
    }
    }
    }
    }
    }
    }
    }
    }
    }
==== bsi-audit-automator/assets/schemas/scan_report_ch7_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Scanned Previous Report Findings (Chapter 7.2)",
  "description": "Extracts a single, flat list of all findings (AS, AG, E) from tables in or around Chapter 7.2 of a previous audit report.",
  "type": "object",
  "properties": {
    "all_findings": {
      "type": "array",
      "description": "A single, flat list of all findings extracted from the report.",
      "items": {
        "type": "object",
        "properties": {
          "category": {
            "type": "string",
            "description": "The determined category of the finding.",
            "enum": ["AS", "AG", "E"]
          },
          "nummer": { "type": "string", "description": "The finding's number or ID." },
          "beschreibung": { "type": "string", "description": "The full description of the finding." },
          "quelle": { "type": "string", "description": "The source chapter or reference." },
          "behebungsfrist": { "type": "string", "description": "The deadline for correction." },
          "status": { "type": "string", "description": "The current status of the finding." }
        },
        "required": ["category", "nummer", "beschreibung"]
      }
    }
  },
  "required": ["all_findings"]
}
==== bsi-audit-automator/assets/schemas/stage_1_2_geltungsbereich_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for BSI Grundschutz Stage 1.2 - Scope of Certification",
    "description": "Defines the structure for the scope description and an associated finding.",
    "type": "object",
    "properties": {
      "text": {
        "type": "string",
        "description": "The formal text describing the scope of the audit (Geltungsbereich)."
      },
      "finding": {
        "type": "object",
        "description": "A structured finding regarding the quality and completeness of the scope definition.",
        "properties": {
          "category": {
            "type": "string",
            "description": "The category of the finding: 'AG', 'AS', 'E', or 'OK'.",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string",
            "description": "A detailed description of the finding related to the scope definition."
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["text", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_1_4_informationsverbund_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Informationsverbund Description",
    "description": "Schema for the AI's analysis of the Informationsverbund scope (Chapter 1.4).",
    "type": "object",
    "properties": {
      "kurzbezeichnung": {
        "description": "The short, official name of the Informationsverbund.",
        "type": "string"
      },
      "kurzbeschreibung": {
        "description": "A concise summary of the audit scope (Geltungsbereich), including business processes, locations, and applications.",
        "type": "string"
      },
      "finding": {
        "description": "A structured finding regarding the quality and clarity of the scope definition.",
        "type": "object",
        "properties": {
          "category": {
            "description": "The category of the finding: 'OK' if no issue, 'AG' (Minor Deviation), 'AS' (Major Deviation), or 'E' (Recommendation).",
            "type": "string",
            "enum": ["OK", "AG", "AS", "E"]
          },
          "description": {
            "description": "A detailed, evidence-based description of the finding. If category is 'OK', this should be a brief confirmation.",
            "type": "string"
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["kurzbezeichnung", "kurzbeschreibung", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_3_1_aktualitaet_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Subchapter 3.1 Aktualitaet der Referenzdokumente",
  "type": "object",
  "properties": {
    "answers": {
      "type": "array",
      "description": "The answers to the four questions regarding the actuality of the documents.",
      "items": {
        "anyOf": [
          { "type": "boolean" },
          { "type": "string", "format": "date" }
        ]
      },
      "minItems": 4,
      "maxItems": 4
    },
    "finding": {
      "type": "object",
      "description": "A structured finding based on the analysis.",
      "properties": {
        "category": {
          "type": "string",
          "description": "The category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK' (No Deviation).",
          "enum": ["AG", "AS", "E", "OK"]
        },
        "description": {
          "type": "string",
          "description": "A detailed description of the finding."
        }
      },
      "required": ["category", "description"]
    }
  },
  "required": ["answers", "finding"]
}
==== bsi-audit-automator/assets/schemas/stage_3_2_sicherheitsleitlinie_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Subchapter 3.2 Sicherheitsleitlinie und Richtlinien in A.0",
  "type": "object",
  "properties": {
    "answers": {
      "type": "array",
      "description": "The answers to the three questions regarding the security policy.",
      "items": {
        "type": "boolean"
      },
      "minItems": 3,
      "maxItems": 3
    },
    "finding": {
      "type": "object",
      "description": "A structured finding based on the analysis.",
      "properties": {
        "category": {
          "type": "string",
          "description": "The category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK' (No Deviation).",
          "enum": ["AG", "AS", "E", "OK"]
        },
        "description": {
          "type": "string",
          "description": "A detailed description of the finding."
        }
      },
      "required": ["category", "description"]
    }
  },
  "required": ["answers", "finding"]
}
==== bsi-audit-automator/assets/schemas/stage_3_3_1_informationsverbund_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Subchapter 3.3.1 Definition des Informationsverbundes",
  "type": "object",
  "properties": {
    "answers": {
      "type": "array",
      "description": "The answers to the three questions regarding the definition of the informational asset network.",
      "items": {
        "type": "boolean"
      },
      "minItems": 3,
      "maxItems": 3
    },
    "finding": {
      "type": "object",
      "description": "A structured finding based on the analysis.",
      "properties": {
        "category": {
          "type": "string",
          "description": "The category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK' (No Deviation).",
          "enum": ["AG", "AS", "E", "OK"]
        },
        "description": {
          "type": "string",
          "description": "A detailed description of the finding."
        }
      },
      "required": ["category", "description"]
    }
  },
  "required": ["answers", "finding"]
}
==== bsi-audit-automator/assets/schemas/stage_3_3_2_netzplan_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Chapter 3.3.2 Netzplan",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "Answers to the questions about the network plan.",
        "items": {
          "type": "boolean"
        },
        "minItems": 2,
        "maxItems": 2
      },
      "finding": {
        "type": "object",
        "properties": {
          "category": {
            "type": "string",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string"
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_3_3_3_geschaeftsprozesse_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Chapter 3.3.3 Liste der Geschäftsprozesse",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "Answer to the question about business processes.",
        "items": {
          "type": "boolean"
        },
        "minItems": 1,
        "maxItems": 1
      },
      "finding": {
        "type": "object",
        "properties": {
          "category": {
            "type": "string",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string"
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_3_4_1_schutzbedarfskategorien_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Chapter 3.4.1 Definition der Schutzbedarfskategorien",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "Answers to the questions about protection requirement categories.",
        "items": {
          "type": "boolean"
        },
        "minItems": 2,
        "maxItems": 2
      },
      "finding": {
        "type": "object",
        "properties": {
          "category": {
            "type": "string",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string"
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_3_5_1_modellierungsdetails_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Chapter 3.5.1 Modellierungsdetails",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "Answers to the questions about modeling details.",
        "items": {
          "type": "boolean"
        },
        "minItems": 4,
        "maxItems": 4
      },
      "finding": {
        "type": "object",
        "properties": {
          "category": {
            "type": "string",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string"
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_3_6_1_extract_check_data_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Extracted Grundschutz-Check Data",
  "description": "A schema to hold the structured list of requirements and chapter headings extracted from a Grundschutz-Check document chunk.",
  "type": "object",
  "properties": {
    "anforderungen": {
      "type": "array",
      "description": "A list of all security requirements found in the document chunk.",
      "items": {
        "type": "object",
        "properties": {
          "id": {
            "type": "string",
            "description": "The full ID of the requirement, e.g., 'ISMS.1.A1'."
          },
          "titel": {
            "type": "string",
            "description": "The title of the requirement."
          },
          "umsetzungsstatus": {
            "type": "string",
            "description": "The implementation status, normalized (e.g., 'Ja', 'Nein', 'teilweise', 'entbehrlich')."
          },
          "umsetzungserlaeuterung": {
            "type": "string",
            "description": "The full text of the implementation explanation."
          },
          "datumLetztePruefung": {
            "type": "string",
            "description": "The date of the last check, extracted as seen in the document. The fallback for a missing date is '1970-01-01'."
          },
          "pagenumber": {
            "type": "integer",
            "description": "The page number where this requirement was found."
          },
          "zielobjekt_kuerzel": {
            "type": "string",
            "description": "The Kürzel of the Zielobjekt, if directly associated on the same page by the AI."
          },
          "zielobjekt_name": {
            "type": "string",
            "description": "The Name of the Zielobjekt, if directly associated on the same page by the AI."
          }
        },
        "required": [
          "id",
          "umsetzungsstatus",
          "umsetzungserlaeuterung",
          "datumLetztePruefung",
          "pagenumber"
        ]
      }
    }

  },
  "required": ["anforderungen"]
}
==== bsi-audit-automator/assets/schemas/stage_3_6_1_grundschutz_check_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Chapter 3.6.1 IT-Grundschutz-Check",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "Answers to the questions about the IT-Grundschutz-Check.",
        "items": {
          "type": "boolean"
        },
        "minItems": 5,
        "maxItems": 5
      },
      "finding": {
        "type": "object",
        "properties": {
          "category": {
            "type": "string",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string"
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_3_7_risikoanalyse_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Risikoanalyse Response",
    "description": "A schema for answering the 4 questions of the risk analysis section and providing a single audit finding.",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "An array of 4 answers, one for each question asked. The order must be preserved.",
        "items": { "type": "string" },
        "minItems": 4,
        "maxItems": 4
      },
      "finding": {
        "type": "object",
        "description": "A single, structured finding for this section.",
        "properties": {
          "category": {
            "type": "string",
            "description": "Category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK'.",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string",
            "description": "A clear, concise description of the finding, including evidence and document reference."
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_3_9_ergebnis_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Subchapter 3.9 Ergebnis der Dokumentenpruefung",
  "type": "object",
  "properties": {
    "answers": {
      "type": "array",
      "description": "The single boolean answer to the question of whether the audit can proceed.",
      "items": {
        "type": "boolean"
      },
      "minItems": 1,
      "maxItems": 1
    },
    "votum": {
      "type": "string",
      "description": "The summary verdict or 'Votum' for the document review phase."
    }
  },
  "required": ["answers", "votum"]
}
==== bsi-audit-automator/assets/schemas/stage_3_gt_baustein_mappings_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Baustein-to-Zielobjekt Mappings",
    "description": "Validates the list of mappings extracted from the Modellierung document (A.3).",
    "type": "object",
    "properties": {
        "mappings": {
            "type": "array",
            "description": "A list of mappings.",
            "items": {
                "type": "object",
                "properties": {
                    "baustein_id": {
                        "type": "string",
                        "description": "The ID of the Baustein, e.g., 'SYS.1.1'."
                    },
                    "zielobjekt_kuerzel": {
                        "type": "string",
                        "description": "The unique ID (Kürzel) of the Zielobjekt it is mapped to."
                    }
                },
                "required": ["baustein_id", "zielobjekt_kuerzel"]
            }
        }
    },
    "required": ["mappings"]
}
==== bsi-audit-automator/assets/schemas/stage_3_gt_zielobjekte_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Zielobjekte Extraction",
    "description": "Validates the list of Zielobjekte extracted from the Strukturanalyse document (A.1).",
    "type": "object",
    "properties": {
        "zielobjekte": {
            "type": "array",
            "description": "A list of all target objects (Zielobjekte).",
            "items": {
                "type": "object",
                "properties": {
                    "kuerzel": {
                        "type": "string",
                        "description": "The unique, customer-defined ID of the Zielobjekt, e.g., 'A-001'."
                    },
                    "name": {
                        "type": "string",
                        "description": "The full, descriptive name of the Zielobjekt, e.g., 'Main Web Server'."
                    }
                },
                "required": ["kuerzel", "name"]
            }
        }
    },
    "required": ["zielobjekte"]
}
==== bsi-audit-automator/assets/schemas/stage_3_summary_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for a generic summary/votum subchapter",
    "type": "object",
    "properties": {
      "votum": {
        "type": "string",
        "description": "A summary verdict based on the provided findings."
      }
    },
    "required": ["votum"]
  }
==== bsi-audit-automator/assets/schemas/stage_4_1_1_auswahl_bausteine_erst_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Subchapter 4.1.1 Auswahl Bausteine Überwachung",
  "type": "object",
  "properties": {
    "rows": {
      "type": "array",
      "description": "A list of Bausteine selected for the audit.",
      "items": {
        "type": "object",
        "properties": {
          "Schicht": { "type": "string", "description": "e.g., 'ISMS', 'ORP', 'INF'" },
          "Baustein": { "type": "string", "description": "The full Baustein ID and name, e.g., 'ISMS.1 Sicherheitsmanagement'" },
          "Zielobjekt-Name": { "type": "string", "description": "The descriptive name of the target object, e.g., 'Gesamter Informationsverbund'" },
          "Zielobjekt-Kürzel": { "type": "string", "description": "The unique ID (Kürzel) of the target object." },
          "Begründung zur Auswahl": { "type": "string", "description": "Justification for selecting this Baustein for the audit." }
        },
        "required": [
          "Schicht",
          "Baustein",
          "Zielobjekt-Name",
          "Zielobjekt-Kürzel",
          "Begründung zur Auswahl"
        ]
      }
    }
  },
  "required": ["rows"]
}
==== bsi-audit-automator/assets/schemas/stage_4_1_2_auswahl_bausteine_ueberwachung_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Subchapter 4.1.2 Auswahl Bausteine Überwachung",
    "type": "object",
    "properties": {
      "rows": {
        "type": "array",
        "description": "A list of Bausteine selected for the audit.",
        "items": {
          "type": "object",
          "properties": {
            "Schicht": { "type": "string", "description": "e.g., 'ISMS', 'ORP', 'INF'" },            
            "Baustein": { "type": "string", "description": "The full Baustein ID and name, e.g., 'ISMS.1 Sicherheitsmanagement'" },
            "Zielobjekt-Name": { "type": "string", "description": "The descriptive name of the target object, e.g., 'Gesamter Informationsverbund'" },            "Zielobjekt-Kürzel": { "type": "string", "description": "The unique ID (Kürzel) of the target object." },
            "Begründung zur Auswahl": { "type": "string", "description": "Justification for selecting this Baustein for the audit." }
          },
           "required": [
            "Schicht",
            "Baustein",
            "Zielobjekt-Name",
            "Zielobjekt-Kürzel",
            "Begründung zur Auswahl"
          ]
        }
      }
    },
    "required": ["rows"]
  }
==== bsi-audit-automator/assets/schemas/stage_4_1_3_auswahl_bausteine_ueberwachung_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Subchapter 4.1.3 Auswahl Bausteine Überwachung",
  "type": "object",
  "properties": {
    "rows": {
      "type": "array",
      "description": "A list of Bausteine selected for the audit.",
      "items": {
        "type": "object",
        "properties": {
          "Schicht": { "type": "string", "description": "e.g., 'ISMS', 'ORP', 'INF'" },            
          "Baustein": { "type": "string", "description": "The full Baustein ID and name, e.g., 'ISMS.1 Sicherheitsmanagement'" },
          "Zielobjekt-Name": { "type": "string", "description": "The descriptive name of the target object, e.g., 'Gesamter Informationsverbund'" },            "Zielobjekt-Kürzel": { "type": "string", "description": "The unique ID (Kürzel) of the target object." },
          "Begründung zur Auswahl": { "type": "string", "description": "Justification for selecting this Baustein for the audit." }
        },
         "required": [
          "Schicht",
          "Baustein",
          "Zielobjekt-Name",
          "Zielobjekt-Kürzel",
          "Begründung zur Auswahl"
        ]
      }
    }
  },
  "required": ["rows"]
}
==== bsi-audit-automator/assets/schemas/stage_4_1_5_auswahl_massnahmen_risiko_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Subchapter 4.1.5 Auswahl Maßnahmen aus der Risikoanalyse",
    "type": "object",
    "properties": {
      "rows": {
        "type": "array",
        "description": "A list of security measures selected from the risk analysis for on-site verification.",
        "items": {
          "type": "object",
          "properties": {
            "Maßnahme": { "type": "string", "description": "A concise name for the security measure (e.g., 'Advanced DDoS Protection', 'Database Field-Level Encryption')." },
            "Risikoanalyse": { "type": "string", "description": "Reference to the risk analysis document or section (e.g., 'RA-2023-Web-App', 'Risikoanalyse PII-Datenbank')." },
            "Zielobjekt": { "type": "string", "description": "The target system or asset the measure applies to (e.g., 'Web Application Cluster', 'CRM Customer Database')." },
            "Begründung zur Auswahl": { "type": "string", "description": "The risk-based justification for selecting this measure for the audit." }
          },
          "required": ["Maßnahme", "Risikoanalyse", "Zielobjekt", "Begründung zur Auswahl"]
        }
      }
    },
    "required": ["rows"]
  }
==== bsi-audit-automator/check_import.py ====
from google.cloud import documentai
from google.api_core.client_options import ClientOptions

location = "us"  # or your region, e.g., "eu"
opts = ClientOptions(api_endpoint=f"{location}-documentai.googleapis.com")
client = documentai.DocumentProcessorServiceClient(client_options=opts)
print("Client initialized successfully!")

==== bsi-audit-automator/delete_old_prompts.sh ====
#!/bin/bash
# Deletes the now-obsolete individual prompt text files.

echo "Deleting old prompt files from ./assets/prompts/ ..."

rm -f ./assets/prompts/etl_classify_documents.txt
rm -f ./assets/prompts/generic_question_prompt.txt
rm -f ./assets/prompts/generic_summary_prompt.txt
rm -f ./assets/prompts/stage_1_2_geltungsbereich.txt
rm -f ./assets/prompts/stage_1_4_informationsverbund.txt
rm -f ./assets/prompts/stage_3_1_aktualitaet.txt
rm -f ./assets/prompts/stage_3_2_sicherheitsleitlinie.txt
rm -f ./assets/prompts/stage_3_3_1_informationsverbund.txt
rm -f ./assets/prompts/stage_3_3_2_netzplan.txt
rm -f ./assets/prompts/stage_3_3_3_geschaeftsprozesse.txt
rm -f ./assets/prompts/stage_3_4_1_schutzbedarfskategorien.txt
rm -f ./assets/prompts/stage_3_5_1_modellierungsdetails.txt
rm -f ./assets/prompts/stage_3_5_2_ergebnis_modellierung.txt
rm -f ./assets/prompts/stage_3_6_1_extract_check_data.txt
rm -f ./assets/prompts/stage_3_6_1_grundschutz_check.txt
rm -f ./assets/prompts/stage_3_9_ergebnis.txt
rm -f ./assets/prompts/stage_4_1_1_auswahl_bausteine_erst.txt
rm -f ./assets/prompts/stage_4_1_2_auswahl_bausteine_ueberwachung.txt
rm -f ./assets/prompts/stage_4_1_5_auswahl_massnahmen_risiko.txt
rm -f ./assets/prompts/stage_7_2_abweichungen.txt

# Also remove the now-empty directory
rmdir ./assets/prompts 2>/dev/null || true

echo "Deletion complete."
==== bsi-audit-automator/envs.sh ====
#!/bin/bash
#
# DYNAMIC Environment variable setup for local BSI Audit Automator development.
#
# This script dynamically fetches configuration from your Terraform state,
# ensuring your local environment matches the cloud deployment.
#
# It also defines a helper function `bsi-auditor` to simplify running the app.
#
# PREREQUISITES:
#   - You must have run 'terraform apply' in the ./terraform directory.
#   - You must have the 'terraform' CLI installed and in your PATH.
#
# USAGE:
#   Run this command from the project root (the 'bsi-audit-automator' directory):
#      source ./envs.sh
#
#   Then, you can run the application like this:
#      bsi-auditor --run-etl
#      bsi-auditor --run-stage Chapter-1
#
set -e # Exit on error

TERRAFORM_DIR="../terraform"

if [ ! -d "$TERRAFORM_DIR" ]; then
    echo "❌ Error: Terraform directory not found at '$TERRAFORM_DIR'. Please run this script from the project root."
    return 1
fi
if ! command -v terraform &> /dev/null; then
    echo "❌ Error: 'terraform' command not found. Please install Terraform."
    return 1
fi

echo "🔹 Fetching infrastructure details from Terraform..."

# --- Dynamic Values from Terraform ---
export GCP_PROJECT_ID="$(terraform -chdir=${TERRAFORM_DIR} output -raw project_id)"
export REGION="$(terraform -chdir=${TERRAFORM_DIR} output -raw region)"
export BUCKET_NAME="$(terraform -chdir=${TERRAFORM_DIR} output -raw gcs_bucket_name)"
export DOC_AI_PROCESSOR_NAME="$(terraform -chdir=${TERRAFORM_DIR} output -raw documentai_processor_name)"
# NEW: Fetch the public domain if it exists, otherwise set to empty string.

# --- Static Values for Local Development ---
# These prefixes now reflect the simpler GCS layout.
export SOURCE_PREFIX="source_documents/"
export OUTPUT_PREFIX="output/"

# Manually set the audit type and test mode for your local run
export AUDIT_TYPE="2. Überwachungsaudit"
export TEST="true"
export MAX_CONCURRENT_AI_REQUESTS=5 # New: Tunable concurrency limit

# --- NEW: Helper function for correct execution ---
# This alias ensures we always run the application as a module,
# which correctly resolves the relative imports in src/main.py.
bsi-auditor() {
    python -m src.main "$@"
}


set +e
echo "✅ Environment variables configured successfully'."
echo "   - GCP_PROJECT_ID: ${GCP_PROJECT_ID}"
echo "   - BUCKET_NAME:    ${BUCKET_NAME}"
echo "   - DOC_AI_PROC:    ${DOC_AI_PROCESSOR_NAME}"
echo "   - AUDIT_TYPE:     ${AUDIT_TYPE}"
echo "   - TEST mode:      ${TEST}"
echo ""
echo "👉 A new command 'bsi-auditor' is now available in your shell."
echo "   Run the app with: bsi-auditor --run-stage Chapter-1"
==== bsi-audit-automator/requirements.txt ====
# GCP and Vertex AI
google-cloud-storage
google-cloud-aiplatform
google-cloud-documentai

# For local development, to load .env files
python-dotenv
jsonschema # For validating AI model outputs
PyMuPDF # For PDF processing (provides 'fitz' module)
==== bsi-audit-automator/src/audit/controller.py ====
# src/audit/controller.py
import logging
import json
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict
from google.cloud.exceptions import NotFound

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient
from src.clients.document_ai_client import DocumentAiClient
from src.clients.rag_client import RagClient
from src.audit.stages.stage_previous_report_scan import PreviousReportScanner
from src.audit.stages.stage_1_general import Chapter1Runner
from src.audit.stages.stage_3_dokumentenpruefung import Chapter3Runner
from src.audit.stages.stage_4_pruefplan import Chapter4Runner
from src.audit.stages.stage_5_vor_ort_audit import Chapter5Runner
from src.audit.stages.stage_7_anhang import Chapter7Runner
from src.audit.stages.stage_gs_check_extraction import GrundschutzCheckExtractionRunner

class AuditController:
    """Orchestrates the entire staged audit process with lazy initialization of runners."""

    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self.rag_client = rag_client
        self.all_findings: List[Dict[str, Any]] = []
        self.finding_counters = defaultdict(int)

        self.stage_runner_classes = {
            "Scan-Report": PreviousReportScanner,
            "Grundschutz-Check-Extraction": GrundschutzCheckExtractionRunner,
            "Chapter-1": Chapter1Runner,
            "Chapter-3": Chapter3Runner,
            "Chapter-4": Chapter4Runner,
            "Chapter-5": Chapter5Runner,
            "Chapter-7": Chapter7Runner,
        }
        # This defines the exact order of dependencies for each runner's constructor.
        self.runner_dependencies = {
            "Scan-Report": (self.config, self.ai_client, self.rag_client),
            "Grundschutz-Check-Extraction": (self.config, self.gcs_client, None, self.ai_client, self.rag_client), # Placeholder for doc_ai_client
            "Chapter-1": (self.config, self.ai_client, self.rag_client),
            "Chapter-3": (self.config, self.gcs_client, self.ai_client, self.rag_client),
            "Chapter-4": (self.config, self.gcs_client, self.ai_client),
            "Chapter-5": (self.config, self.gcs_client, self.ai_client),
            "Chapter-7": (self.config, self.gcs_client),
        }
        logging.info("Audit Controller initialized with lazy stage loading and findings collector.")

    def _parse_finding_id(self, finding_id: str) -> Tuple[Optional[str], int]:
        """Parses a finding ID like 'AG-12' into its category 'AG' and number 12."""
        if not finding_id or '-' not in finding_id:
            return None, 0
        parts = finding_id.split('-')
        category = parts[0]
        try:
            num = int(parts[-1])
            return category, num
        except (ValueError, IndexError):
            return None, 0

    def _process_previous_findings(self, previous_findings: List[Dict[str, Any]]):
        """Processes findings from a previous report scan, preserving their IDs and updating counters."""
        logging.info(f"Processing {len(previous_findings)} findings from previous audit report.")
        for finding in previous_findings:
            finding_id = finding.get("nummer")
            if not finding_id:
                continue

            category, num = self._parse_finding_id(finding_id)
            if category and num > 0:
                # Update the counter to the highest number seen for this category
                self.finding_counters[category] = max(self.finding_counters[category], num)

            # Add the finding to the central list with its ID and details preserved
            self.all_findings.append({
                "id": finding_id,
                "category": finding.get("category"),
                "description": finding.get("beschreibung", "No description provided."),
                "source_chapter": f"Previous Audit ({finding.get('quelle', 'N/A')})",
                "status": finding.get("status"),
                "behebungsfrist": finding.get("behebungsfrist")
            })

    def _process_new_finding(self, finding: Dict[str, Any], stage_name: str):
        """Processes a newly generated finding, adding it to the central list to await ID assignment."""
        source_ref = stage_name.replace('Chapter-', '')
        # Add to central list without an ID, which will be assigned at the end.
        self.all_findings.append({
            "category": finding.get("category"),
            "description": finding.get("description"),
            "source_chapter": source_ref
        })
        logging.info(f"Collected new finding from {stage_name}: {finding.get('category')}")

    def _extract_findings_recursive(self, data: Any) -> List[Dict[str, Any]]:
        """
        Recursively traverses a data structure to find all structured `finding` objects.
        Returns a flat list of all findings discovered. This method does NOT handle
        the `all_findings` key from Scan-Report, as that is handled separately.
        """
        found = []
        if isinstance(data, dict):
            if 'finding' in data and isinstance(data['finding'], dict):
                finding_obj = data['finding']
                if finding_obj and finding_obj.get('category') != 'OK':
                    found.append(finding_obj)
            
            for value in data.values():
                found.extend(self._extract_findings_recursive(value))
        
        elif isinstance(data, list):
            for item in data:
                found.extend(self._extract_findings_recursive(item))
        
        return found

    def _extract_and_store_findings(self, stage_name: str, result_data: Dict[str, Any]) -> None:
        """
        Parses stage results, finds all structured `finding` objects recursively,
        and adds them to the central collection.
        """
        if not result_data:
            return

        # Special handling for Scan-Report which has a flat list of previous findings
        if stage_name == "Scan-Report" and 'all_findings' in result_data:
            self._process_previous_findings(result_data['all_findings'])
            # We don't do a recursive search for this stage type to avoid double counting
            return

        # For the extraction stage, there are no findings to process.
        if stage_name == "Grundschutz-Check-Extraction":
            return

        # Standard recursive search for newly generated findings
        newly_discovered_findings = self._extract_findings_recursive(result_data)
        for finding in newly_discovered_findings:
            self._process_new_finding(finding, stage_name)

    def _save_all_findings(self) -> None:
        """
        Saves the centrally collected list of all findings. It preserves existing IDs
        from previous reports and assigns new, sequential IDs for new findings.
        """
        if not self.all_findings:
            logging.info("No findings were collected during the audit. Skipping save.")
            return

        findings_with_ids = []
        for finding in self.all_findings:
            if 'id' in finding and finding['id']:
                # This is a finding from a previous report, ID is already set.
                findings_with_ids.append(finding)
            else:
                # This is a new finding, assign a new ID.
                category = finding['category']
                self.finding_counters[category] += 1
                finding_id = f"{category}-{self.finding_counters[category]}"
                
                # Add the new ID to the finding object
                finding_with_id = {"id": finding_id, **finding}
                findings_with_ids.append(finding_with_id)

        findings_path = f"{self.config.output_prefix}results/all_findings.json"
        self.gcs_client.upload_from_string(
            content=json.dumps(findings_with_ids, indent=2, ensure_ascii=False),
            destination_blob_name=findings_path
        )
        logging.info(f"Successfully saved {len(findings_with_ids)} findings with sequential IDs to {findings_path}")

    async def run_all_stages(self, force_overwrite: bool = False) -> None:
        """
        Runs all defined audit stages in a dependency-aware order.
        """
        # Step 0: Run the critical pre-processing step first.
        logging.info("Step 0: Running pre-processing stage 'Grundschutz-Check-Extraction'...")
        await self.run_single_stage("Grundschutz-Check-Extraction", force_overwrite=force_overwrite)
        logging.info("Completed pre-processing.")

        # Step 1: Run initial independent stages in parallel. Chapter-3 now depends on Step 0.
        initial_parallel_stages = ["Scan-Report", "Chapter-1", "Chapter-3", "Chapter-7"]
        logging.info(f"Step 1: Starting parallel execution for initial stages: {initial_parallel_stages}")
        await asyncio.gather(
            *(self.run_single_stage(stage_name, force_overwrite=force_overwrite) for stage_name in initial_parallel_stages)
        )
        logging.info("Completed initial parallel stages.")

        # Step 2: Run Chapter 4, which depends on Chapter 3's ground-truth map.
        logging.info("Step 2: Running stage Chapter-4...")
        await self.run_single_stage("Chapter-4", force_overwrite=force_overwrite)
        logging.info("Completed stage Chapter-4.")

        # Step 3: Run Chapter 5, which depends on Chapter 4's plan and Chapter 3's data.
        logging.info("Step 3: Running stage Chapter-5...")
        await self.run_single_stage("Chapter-5", force_overwrite=force_overwrite)
        logging.info("Completed stage Chapter-5.")
        
        self._save_all_findings()
        logging.info("All audit stages completed.")

    async def run_single_stage(self, stage_name: str, force_overwrite: bool = False) -> Dict[str, Any]:
        """
        Runs a single, specified audit stage and collects findings from its result.
        """
        if stage_name not in self.stage_runner_classes:
            logging.error(f"Unknown stage '{stage_name}'. Available: {list(self.stage_runner_classes.keys())}")
            raise ValueError(f"Unknown stage: {stage_name}")

        stage_output_path = f"{self.config.output_prefix}results/{stage_name}.json"
        
        if not force_overwrite:
            try:
                # The extraction stage does not produce a reportable JSON, its output is the intermediate file.
                # So we check for the intermediate file's existence to determine if we can skip.
                if stage_name == "Grundschutz-Check-Extraction":
                    if self.gcs_client.blob_exists(GrundschutzCheckExtractionRunner.INTERMEDIATE_CHECK_RESULTS_PATH) and \
                       self.gcs_client.blob_exists(GrundschutzCheckExtractionRunner.GROUND_TRUTH_MAP_PATH):
                        logging.info(f"Stage '{stage_name}' already completed (intermediate files exist). Skipping.")
                        return {"status": "skipped", "reason": "intermediate files found"}
                else:
                    existing_data = self.gcs_client.read_json(stage_output_path)
                    logging.info(f"Stage '{stage_name}' already completed. Skipping generation.")
                    self._extract_and_store_findings(stage_name, existing_data)
                    return existing_data
            except NotFound:
                logging.info(f"No results for stage '{stage_name}' found. Generating...")
            except Exception as e:
                logging.warning(f"Could not read existing state for stage '{stage_name}': {e}. Proceeding.")
        else:
            logging.info(f"Force overwrite enabled for stage '{stage_name}'. Running generation.")

        runner_class = self.stage_runner_classes[stage_name]
        
        # --- DYNAMIC DEPENDENCY INJECTION ---
        # Instantiate DocumentAiClient only if needed for the specific stage
        if stage_name == "Grundschutz-Check-Extraction":
            doc_ai_client = DocumentAiClient(self.config, self.gcs_client)
            dependencies = (self.config, self.gcs_client, doc_ai_client, self.ai_client, self.rag_client)
        else:
            dependencies = self.runner_dependencies[stage_name]
        stage_runner = runner_class(*dependencies)
        logging.info(f"Initialized runner for stage: {stage_name}")

        try:
            # Pass the force_overwrite flag down to the runner.
            result_data = await stage_runner.run(force_overwrite=force_overwrite)

            # The extraction stage does not produce a reportable result, so we don't save a stage JSON.
            if stage_name != "Grundschutz-Check-Extraction":
                self.gcs_client.upload_from_string(
                    content=json.dumps(result_data, indent=2, ensure_ascii=False),
                    destination_blob_name=stage_output_path
                )
                logging.info(f"Successfully saved results for stage '{stage_name}'.")
            
            self._extract_and_store_findings(stage_name, result_data)
            return result_data
        except Exception as e:
            logging.error(f"Stage '{stage_name}' failed: {e}", exc_info=True)
            raise
==== bsi-audit-automator/src/audit/report_generator.py ====
# src/audit/report_generator.py
import logging
import json
import asyncio
from google.cloud.exceptions import NotFound
from typing import Dict, Any, List
from jsonschema import validate, ValidationError

from datetime import datetime

from src.config import AppConfig
from src.clients.gcs_client import GcsClient

class ReportGenerator:
    """Assembles the final audit report from individual stage stubs."""
    LOCAL_MASTER_TEMPLATE_PATH = "assets/json/master_report_template.json"
    STAGES_TO_AGGREGATE = ["Scan-Report", "Chapter-1", "Chapter-3", "Chapter-4", "Chapter-5", "Chapter-7"]

    def __init__(self, config: AppConfig, gcs_client: GcsClient):
        self.config = config
        self.gcs_client = gcs_client
        self.report_schema = self._load_report_schema()
        logging.info("Report Generator initialized.")
    
    def _load_report_schema(self) -> Dict[str, Any]:
        """Loads the master template to use as a validation schema."""
        try:
            with open(self.LOCAL_MASTER_TEMPLATE_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logging.error(f"FATAL: Could not load the master report schema from {self.LOCAL_MASTER_TEMPLATE_PATH}. Error: {e}")
            raise

    def _set_value_by_path(self, report: Dict, path: str, value: Any):
        """
        Safely sets a value in a nested dictionary using a dot-separated path.
        This is more robust than sequential `get` calls.
        """
        keys = path.split('.')
        current_level = report
        for i, key in enumerate(keys[:-1]):
            if not isinstance(current_level, dict):
                logging.warning(f"Path part '{key}' is not a dict in path '{path}'. Cannot set value.")
                return
            if key not in current_level:
                # Create missing dictionary keys if they don't exist
                current_level[key] = {}
            current_level = current_level[key]
        
        if isinstance(current_level, dict):
            current_level[keys[-1]] = value
        else:
            logging.warning(f"Target for path '{path}' is not a dictionary. Cannot set final key '{keys[-1]}'.")

    def _ensure_list_path_exists(self, report: Dict, path: str, min_length: int = 1, default_item: Dict = None) -> List:
        """
        Ensures a list at a given path exists and has a minimum length, padding it if necessary.
        Returns the list object for modification.
        """
        if default_item is None:
            default_item = {"type": "prose", "text": ""}

        keys = path.split('.')
        current_level = report
        for key in keys:
            if not isinstance(current_level, dict):
                logging.warning(f"Path part is not a dict in path '{path}' at key '{key}'. Cannot ensure list path.")
                return []
            if key not in current_level:
                current_level[key] = [] if key == keys[-1] else {}
            current_level = current_level[key]

        if not isinstance(current_level, list):
            logging.warning(f"Target for path '{path}' is not a list. Cannot pad.")
            return []
        
        while len(current_level) < min_length:
            current_level.append(default_item.copy())
            
        return current_level

    def _load_local_report_template(self) -> dict:
        """
        Loads the pristine report template from the local assets folder and
        injects initial configuration like the audit type.
        """
        logging.info(f"Loading pristine report template from local asset: {self.LOCAL_MASTER_TEMPLATE_PATH}")
        try:
            with open(self.LOCAL_MASTER_TEMPLATE_PATH, 'r', encoding='utf-8') as f:
                report = json.load(f)
            
            # Inject dynamic configuration into the fresh template
            self._set_value_by_path(report, 'bsiAuditReport.allgemeines.audittyp.content', self.config.audit_type)
            
            logging.info("Successfully loaded and configured local report template.")
            return report
        except Exception as e:
            logging.error(f"FATAL: Could not load the master report template from {self.LOCAL_MASTER_TEMPLATE_PATH}. Error: {e}")
            raise

    def _populate_chapter_1(self, report: dict, stage_data: dict) -> None:
        """Populates the 'Allgemeines' (Chapter 1) of the report defensively."""
        # Populate 1.4 Informationsverbund
        informationsverbund_data = stage_data.get('informationsverbund', {})
        if informationsverbund_data:
            path_prefix = 'bsiAuditReport.allgemeines.informationsverbund.content'
            content_list = self._ensure_list_path_exists(report, path_prefix, min_length=2)
            if content_list:
                content_list[0]['text'] = informationsverbund_data.get('kurzbezeichnung', '')
                content_list[1]['text'] = informationsverbund_data.get('kurzbeschreibung', '')

        # Populate 1.5 Audittyp
        audittyp_content = stage_data.get('audittyp', {}).get('content', self.config.audit_type)
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.audittyp.content', audittyp_content)

    async def assemble_report(self) -> None:
        """
        Main method to assemble the final report.
        """
        report = self._load_local_report_template()

        stage_read_tasks = [self.gcs_client.read_json_async(f"{self.config.output_prefix}results/{s}.json") for s in self.STAGES_TO_AGGREGATE]
        stage_results = await asyncio.gather(*stage_read_tasks, return_exceptions=True)
        
        stage_data_map = {}
        for i, result in enumerate(stage_results):
            stage_name = self.STAGES_TO_AGGREGATE[i]
            if isinstance(result, Exception):
                logging.warning(f"Result for stage '{stage_name}' not found or failed to load. Skipping. Error: {result}")
            # Add a check to ensure stage_data is a dictionary before processing.
            elif not result or not isinstance(result, dict):
                logging.warning(f"Result for stage '{stage_name}' is empty or not a dictionary. Skipping.")
            else:
                stage_data_map[stage_name] = result

        # The order of population matters. Populate from Scan-Report first as a baseline.
        if "Scan-Report" in stage_data_map:
            self._populate_from_scan_report(report, stage_data_map["Scan-Report"])

        # Then, let the other stages overwrite with fresher, generated data.
        for stage_name, stage_data in stage_data_map.items():
            if stage_name != "Scan-Report": # Avoid running it twice
                self._populate_report(report, stage_name, stage_data)
        
        # Populate the final aggregated findings last.
        self._populate_chapter_7_findings(report)

        try:
            validate(instance=report, schema=self.report_schema)
            logging.info("Final report successfully validated against the master schema.")
        except ValidationError as e:
            logging.error(f"CRITICAL: Final report failed schema validation. Report will not be saved. Error: {e.message}")
            return

        today = datetime.now()
        date_str = today.strftime("%y%m%d")
        final_report_path = f"{self.config.output_prefix}report-{date_str}.json"
        await self.gcs_client.upload_from_string_async(
            content=json.dumps(report, indent=2, ensure_ascii=False),
            destination_blob_name=final_report_path
        )
        logging.info(f"Final report assembled and saved to: gs://{self.config.bucket_name}/{final_report_path}")
        

    def _populate_chapter_3(self, report: dict, stage_data: dict) -> None:
        """Populates Chapter 3 (Dokumentenprüfung) content into the report."""
        base_path = "bsiAuditReport.dokumentenpruefung"
        key_to_path_map = {
            "aktualitaetDerReferenzdokumente": f"{base_path}.aktualitaetDerReferenzdokumente",
            "sicherheitsleitlinieUndRichtlinienInA0": f"{base_path}.sicherheitsleitlinieUndRichtlinienInA0",
            "definitionDesInformationsverbundes": f"{base_path}.strukturanalyseA1.definitionDesInformationsverbundes",
            "bereinigterNetzplan": f"{base_path}.strukturanalyseA1.bereinigterNetzplan",
            "listeDerGeschaeftsprozesse": f"{base_path}.strukturanalyseA1.listeDerGeschaeftsprozesse",
            "listeDerAnwendungen": f"{base_path}.strukturanalyseA1.listeDerAnwendungen",
            "listeDerItSysteme": f"{base_path}.strukturanalyseA1.listeDerItSysteme",
            "listeDerRaeumeGebaeudeStandorte": f"{base_path}.strukturanalyseA1.listeDerRaeumeGebaeudeStandorte",
            "listeDerKommunikationsverbindungen": f"{base_path}.strukturanalyseA1.listeDerKommunikationsverbindungen",
            "stichprobenDokuStrukturanalyse": f"{base_path}.strukturanalyseA1.stichprobenDokuStrukturanalyse",
            "listeDerDienstleister": f"{base_path}.strukturanalyseA1.listeDerDienstleister",
            "ergebnisDerStrukturanalyse": f"{base_path}.strukturanalyseA1.ergebnisDerStrukturanalyse",
            "definitionDerSchutzbedarfskategorien": f"{base_path}.schutzbedarfsfeststellungA2.definitionDerSchutzbedarfskategorien",
            "schutzbedarfGeschaeftsprozesse": f"{base_path}.schutzbedarfsfeststellungA2.schutzbedarfGeschaeftsprozesse",
            "schutzbedarfAnwendungen": f"{base_path}.schutzbedarfsfeststellungA2.schutzbedarfAnwendungen",
            "schutzbedarfItSysteme": f"{base_path}.schutzbedarfsfeststellungA2.schutzbedarfItSysteme",
            "schutzbedarfRaeume": f"{base_path}.schutzbedarfsfeststellungA2.schutzbedarfRaeume",
            "schutzbedarfKommunikationsverbindungen": f"{base_path}.schutzbedarfsfeststellungA2.schutzbedarfKommunikationsverbindungen",
            "stichprobenDokuSchutzbedarf": f"{base_path}.schutzbedarfsfeststellungA2.stichprobenDokuSchutzbedarf",
            "ergebnisDerSchutzbedarfsfeststellung": f"{base_path}.schutzbedarfsfeststellungA2.ergebnisDerSchutzbedarfsfeststellung",
            "modellierungsdetails": f"{base_path}.modellierungDesInformationsverbundesA3.modellierungsdetails",
            "ergebnisDerModellierung": f"{base_path}.modellierungDesInformationsverbundesA3.ergebnisDerModellierung",
            "detailsZumItGrundschutzCheck": f"{base_path}.itGrundschutzCheckA4.detailsZumItGrundschutzCheck",
            "benutzerdefinierteBausteine": f"{base_path}.itGrundschutzCheckA4.benutzerdefinierteBausteine",
            "ergebnisItGrundschutzCheck": f"{base_path}.itGrundschutzCheckA4.ergebnisItGrundschutzCheck",
            "risikoanalyse": f"{base_path}.risikoanalyseA5.risikoanalyse",
            "realisierungsplan": f"{base_path}.realisierungsplanA6.realisierungsplan",
            "ergebnisDerDokumentenpruefung": f"{base_path}.ergebnisDerDokumentenpruefung",
        }

        for subchapter_key, result in stage_data.items():
            if not isinstance(result, dict): continue
            
            target_path = key_to_path_map.get(subchapter_key)
            if not target_path: continue

            if 'finding' in result and isinstance(result.get('finding'), dict):
                finding = result['finding']
                finding_text = f"[{finding.get('category')}] {finding.get('description')}"
                finding_list = self._ensure_list_path_exists(report, f"{target_path}.content")
                if finding_list:
                    for item in finding_list:
                        if item.get("type") == "finding":
                            item["findingText"] = finding_text; break
            
            if "answers" in result:
                answers = result.get("answers", [])
                content_list = self._ensure_list_path_exists(report, f"{target_path}.content", len(answers))
                if content_list:
                    answer_idx = 0
                    for item in content_list:
                        if item.get("type") == "question":
                            if answer_idx < len(answers):
                                item["answer"] = answers[answer_idx]; answer_idx += 1

            if "votum" in result:
                content_list = self._ensure_list_path_exists(report, f"{target_path}.content")
                if content_list:
                    for item in content_list:
                        if item.get("type") == "prose": item["text"] = result.get("votum", ""); break
            
            if "table" in result and isinstance(result.get("table"), dict):
                self._set_value_by_path(report, f"{target_path}.table.rows", result['table'].get('rows', []))

    def _populate_chapter_7_findings(self, report: dict) -> None:
        """
        Populates the findings tables in Chapter 7.2 from the central findings file,
        ensuring the findings are sorted numerically by their ID.
        """
        logging.info("Populating Chapter 7.2 with collected findings...")
        findings_path = f"{self.config.output_prefix}results/all_findings.json"
        try:
            all_findings = self.gcs_client.read_json(findings_path)
        except NotFound:
            logging.warning("Central findings file not found. Chapter 7.2 will be empty.")
            return

        # Use clean local lists for collection
        ag_table_rows, as_table_rows, e_table_rows = [], [], []

        for finding in all_findings:
            category = finding.get('category')
            row_data = {
                "Nummer": finding.get('id', 'N/A'),
                "Quelle (Kapitel)": finding.get('source_chapter', 'N/A')
            }
            if category == 'AG':
                row_data["Beschreibung der Abweichung"] = finding.get('description', 'N/A')                
                if finding.get('status') is not None:
                    row_data["Status"] = finding.get('status', 'Unbekannt')
                    row_data["Behebungsfrist"] = finding.get('behebungsfrist', 'N/A')
                else:
                    row_data["Status"] = "Offen"
                    row_data["Behebungsfrist"] = "30 Tage nach Audit"
                ag_table_rows.append(row_data)
            elif category == 'AS':
                row_data["Beschreibung der Abweichung"] = finding.get('description', 'N/A')                
                if finding.get('status') is not None:
                    row_data["Status"] = finding.get('status', 'Unbekannt')
                    row_data["Behebungsfrist"] = finding.get('behebungsfrist', 'N/A')
                else:
                    row_data["Status"] = "Offen"
                    row_data["Behebungsfrist"] = "Bis zum Abschluss des Audit"
                as_table_rows.append(row_data)
            elif category == 'E':
                row_data["Beschreibung der Empfehlung"] = finding.get('description', 'N/A')                
                if finding.get('status') is not None:
                    row_data["Status"] = finding.get('status', 'Unbekannt')
                    row_data["Behebungsfrist"] = finding.get('behebungsfrist', 'N/A')
                else:
                    row_data["Status"] = "Zur Umsetzung empfohlen"
                    row_data["Behebungsfrist"] = "N/A"
                e_table_rows.append(row_data)

        # --- FIX (Task J): Sort findings numerically by ID ---
        def sort_key(finding_dict: Dict[str, Any]) -> int:
            """Extracts the integer part of a finding ID for sorting."""
            try:
                # 'AG-12' -> '12' -> 12
                return int(finding_dict.get("Nummer", "0").split('-')[-1])
            except (ValueError, IndexError):
                return 0  # Fallback for malformed IDs

        ag_table_rows.sort(key=sort_key)
        as_table_rows.sort(key=sort_key)
        e_table_rows.sort(key=sort_key)
        logging.info("Sorted all findings tables numerically by ID.")
        # --- End of FIX ---

        # Now, set the sorted lists into the report dictionary
        self._set_value_by_path(report, 'bsiAuditReport.anhang.abweichungenUndEmpfehlungen.geringfuegigeAbweichungen.table.rows', ag_table_rows)
        self._set_value_by_path(report, 'bsiAuditReport.anhang.abweichungenUndEmpfehlungen.schwerwiegendeAbweichungen.table.rows', as_table_rows)
        self._set_value_by_path(report, 'bsiAuditReport.anhang.abweichungenUndEmpfehlungen.empfehlungen.table.rows', e_table_rows)

        logging.info(f"Populated Chapter 7.2 with {len(all_findings)} total findings.")

    def _populate_chapter_4(self, report: dict, stage_data: dict) -> None:
        """Populates Chapter 4 (Prüfplan) content into the report."""
        base_path = "bsiAuditReport.erstellungEinesPruefplans.auditplanung"
        # This map now consistently points to the 'rows' property inside a 'table' object.
        key_to_path_map = {
            "auswahlBausteineErstRezertifizierung": f"{base_path}.auswahlBausteineErstRezertifizierung.table.rows",
            "auswahlBausteine1Ueberwachungsaudit": f"{base_path}.auswahlBausteine1Ueberwachungsaudit.table.rows",
            "auswahlBausteine2Ueberwachungsaudit": f"{base_path}.auswahlBausteine2Ueberwachungsaudit.table.rows",
            "auswahlStandorte": f"{base_path}.auswahlStandorte.table.rows",
            "auswahlMassnahmenAusRisikoanalyse": f"{base_path}.auswahlMassnahmenAusRisikoanalyse.table.rows"
        }

        for key, data in stage_data.items():
            target_path = key_to_path_map.get(key)
            if not target_path: continue
            
            # Non-destructive update: Only write to the report if the stage data
            # for this section is non-empty. This preserves the baseline from Scan-Report.
            rows_data = data.get('rows', [])
            if rows_data:
                self._set_value_by_path(report, target_path, rows_data)


    def _populate_chapter_5(self, report: dict, stage_data: dict) -> None:
        """Populates Chapter 5 (Vor-Ort-Audit) content into the report."""
        if "verifikationDesITGrundschutzChecks" in stage_data:
            data = stage_data["verifikationDesITGrundschutzChecks"]
            path = "bsiAuditReport.vorOrtAudit.verifikationDesITGrundschutzChecks.einzelergebnisse.bausteinPruefungen"
            self._set_value_by_path(report, path, data.get("einzelergebnisse", {}).get("bausteinPruefungen", []))

        if "risikoanalyseA5" in stage_data:
            data = stage_data["risikoanalyseA5"]
            path = "bsiAuditReport.vorOrtAudit.risikoanalyseA5.einzelergebnisseDerRisikoanalyse.massnahmenPruefungen"
            self._set_value_by_path(report, path, data.get("einzelergebnisseDerRisikoanalyse", {}).get("massnahmenPruefungen", []))

    def _populate_chapter_7(self, report: dict, stage_data: dict) -> None:
        """Populates Chapter 7 (Anhang) content into the report."""
        ref_docs_data = stage_data.get('referenzdokumente', {})
        if isinstance(ref_docs_data.get('table'), dict):
            path = "bsiAuditReport.anhang.referenzdokumente.table.rows"
            self._set_value_by_path(report, path, ref_docs_data['table'].get('rows', []))

    def _populate_from_scan_report(self, report: dict, stage_data: dict) -> None:
        """Populates the report with baseline data from a scanned previous report."""
        logging.info("Populating baseline data from Scan-Report stage...")

        # 1. Populate Chapter 1 tables
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.versionshistorie.table.rows', stage_data.get('versionshistorie', {}).get('table', {}).get('rows', []))
        
        audit_institution_data = stage_data.get('auditierteInstitution', {})
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.auditierteInstitution.kontaktinformationenAntragsteller.table.rows', audit_institution_data.get('kontaktinformationenAntragsteller', {}).get('table', {}).get('rows', []))
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.auditierteInstitution.ansprechpartnerZertifizierung.table.rows', audit_institution_data.get('ansprechpartnerZertifizierung', {}).get('table', {}).get('rows', []))
        
        auditteam_data = stage_data.get('auditteam', {})
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.auditteam.auditteamleiter.table.rows', auditteam_data.get('auditteamleiter', {}).get('table', {}).get('rows', []))
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.auditteam.auditor.table.rows', auditteam_data.get('auditor', {}).get('table', {}).get('rows', []))
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.auditteam.fachexperte.table.rows', auditteam_data.get('fachexperte', {}).get('table', {}).get('rows', []))
        
        # 2. Populate Chapter 4 tables as a fallback/baseline
        self._set_value_by_path(report, 'bsiAuditReport.erstellungEinesPruefplans.auditplanung.auswahlBausteineErstRezertifizierung.table.rows', stage_data.get('auswahlBausteineErstRezertifizierung', {}).get('table', {}).get('rows', []))
        self._set_value_by_path(report, 'bsiAuditReport.erstellungEinesPruefplans.auditplanung.auswahlBausteine1Ueberwachungsaudit.table.rows', stage_data.get('auswahlBausteine1Ueberwachungsaudit', {}).get('table', {}).get('rows', []))
        self._set_value_by_path(report, 'bsiAuditReport.erstellungEinesPruefplans.auditplanung.auswahlStandorte.table.rows', stage_data.get('auswahlStandorte', {}).get('table', {}).get('rows', []))


    def _populate_report(self, report: dict, stage_name: str, stage_data: dict) -> None:
        """Router function to call the correct population logic for a given stage."""
        logging.info(f"Populating report with data from stage: {stage_name}")
        population_map = {
            "Chapter-1": self._populate_chapter_1,
            "Chapter-3": self._populate_chapter_3,
            "Chapter-4": self._populate_chapter_4,
            "Chapter-5": self._populate_chapter_5,
            "Chapter-7": self._populate_chapter_7,
        }
        
        populate_func = population_map.get(stage_name)
        if populate_func:
            populate_func(report, stage_data)
        else:
            logging.warning(f"No population logic defined for stage: {stage_name}")
==== bsi-audit-automator/src/audit/stages/control_catalog.py ====
# src/audit/stages/control_catalog.py
import logging
import json
from typing import List, Dict, Any, Optional

class ControlCatalog:
    """A utility to load and query the BSI Grundschutz OSCAL catalog."""
    
    def __init__(self, catalog_path: str = "assets/json/BSI_GS_OSCAL_current_2023_benutzerdefinierte.json"):
        self.catalog_path = catalog_path
        self._baustein_map = {}
        self._control_map = {}  # New: Map for direct control lookup
        try:
            self._load_and_parse_catalog()
            logging.info(f"Successfully loaded and parsed BSI Control Catalog from {catalog_path}.")
        except Exception as e:
            logging.error(f"Failed to initialize ControlCatalog: {e}", exc_info=True)
            raise

    def _load_and_parse_catalog(self):
        """Loads the JSON catalog and builds an efficient lookup map."""
        with open(self.catalog_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        catalog = data.get("catalog", {})
        # Layers like 'ISMS', 'ORP', 'INF', etc.
        for layer_group in catalog.get("groups", []):
            # Bausteine within each layer
            for baustein_group in layer_group.get("groups", []):
                baustein_id = baustein_group.get("id")
                if baustein_id:
                    controls = baustein_group.get("controls", [])
                    self._baustein_map[baustein_id] = controls
                    for control in controls:
                        self._control_map[control.get("id")] = control
    
    def get_controls_for_baustein_id(self, baustein_id: str) -> List[Dict[str, Any]]:
        """
        Retrieves all controls for a given Baustein ID.

        Args:
            baustein_id: The ID of the Baustein (e.g., 'ISMS.1').

        Returns:
            A list of control objects, or an empty list if not found.
        """
        controls = self._baustein_map.get(baustein_id, [])
        if not controls:
            logging.warning(f"No controls found for Baustein ID: {baustein_id}")
        return controls

    def get_control_level(self, control_id: str) -> Optional[str]:
        """
        Efficiently retrieves the 'level' property for a given control ID.

        Args:
            control_id: The ID of the control (e.g., 'ISMS.1.A1').

        Returns:
            The level as a string (e.g., '1', '5') or None if not found.
        """
        control = self._control_map.get(control_id)
        if control:
            for prop in control.get("props", []):
                if prop.get("name") == "level":
                    return prop.get("value")
        return None

    def get_level_1_control_ids(self) -> List[str]:
        """
        Scans the entire catalog and returns a list of all control IDs that
        are marked as Level 1 (MUSS-Anforderungen).

        Returns:
            A list of Level 1 control ID strings.
        """
        level_1_ids = []
        for baustein_id, controls in self._baustein_map.items():
            for control in controls:
                for prop in control.get("props", []):
                    if prop.get("name") == "level" and prop.get("value") == "1":
                        level_1_ids.append(control.get("id"))
                        break # Move to the next control once level is found
        logging.info(f"Found {len(level_1_ids)} Level 1 (MUSS) controls in the catalog.")
        return level_1_ids
==== bsi-audit-automator/src/audit/stages/stage_1_general.py ====
# src/audit/stages/stage_1_general.py
import logging
import json
import asyncio
from typing import Dict, Any

from src.config import AppConfig
from src.clients.ai_client import AiClient
from src.clients.rag_client import RagClient

class Chapter1Runner:
    """Handles generating content for Chapter 1, with most sections being manual placeholders."""
    STAGE_NAME = "Chapter-1"
    PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"

    def __init__(self, config: AppConfig, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.ai_client = ai_client
        self.rag_client = rag_client
        self.prompt_config = self._load_asset_json(self.PROMPT_CONFIG_PATH)
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f: return json.load(f)

    async def _process_informationsverbund(self) -> Dict[str, Any]:
        """Handles 1.4 Informationsverbund using a filtered document query."""
        logging.info("Processing 1.4 Informationsverbund...")
        
        stage_config = self.prompt_config["stages"]["Chapter-1"]["informationsverbund"]
        prompt_template = stage_config["prompt"]
        schema = self._load_asset_json(stage_config["schema_path"])
        
        source_categories = ['Informationsverbund', 'Strukturanalyse']
        gcs_uris = self.rag_client.get_gcs_uris_for_categories(source_categories)
        
        if not gcs_uris:
            logging.warning(f"No documents found for categories {source_categories}. Generating deterministic response.")
            return {
                "kurzbezeichnung": "Nicht ermittelt",
                "kurzbeschreibung": "Der Geltungsbereich des Informationsverbunds konnte aus den bereitgestellten Dokumenten nicht eindeutig ermittelt werden. Dies muss manuell geklärt und dokumentiert werden.",
                "finding": {
                    "category": "AS",
                    "description": "Die Abgrenzung des Geltungsbereichs ist unklar, da keine Dokumente der Kategorien 'Informationsverbund' oder 'Strukturanalyse' gefunden wurden. Dies ist eine schwerwiegende Abweichung."
                }
            }
            
        return await self.ai_client.generate_json_response(
            prompt=prompt_template,
            json_schema=schema,
            gcs_uris=gcs_uris,
            request_context_log="Chapter-1: informationsverbund"
        )

    async def run(self, force_overwrite: bool = False) -> dict:
        """Executes the generation logic for Chapter 1."""
        logging.info(f"Executing stage: {self.STAGE_NAME}")
        
        informationsverbund_result = await self._process_informationsverbund()

        final_result = {
            "informationsverbund": informationsverbund_result,
            "audittyp": {
                "content": self.config.audit_type
            }
        }

        logging.info(f"Successfully generated data for stage {self.STAGE_NAME}")
        return final_result
==== bsi-audit-automator/src/audit/stages/stage_3_dokumentenpruefung.py ====
# file: src/audit/stages/stage_3_dokumentenpruefung.py
import logging
import json
import asyncio
from typing import Dict, Any, List, Tuple
from datetime import datetime, timedelta
from google.cloud.exceptions import NotFound
from collections import defaultdict

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient
from src.clients.rag_client import RagClient
from src.audit.stages.control_catalog import ControlCatalog

class Chapter3Runner:
    """
    Handles generating content for Chapter 3 "Dokumentenprüfung" by dynamically
    parsing the master report template and using the central prompt configuration.
    It now relies on the pre-computed `extracted_grundschutz_check_merged.json`
    file generated by the `Grundschutz-Check-Extraction` stage.
    """
    STAGE_NAME = "Chapter-3"
    TEMPLATE_PATH = "assets/json/master_report_template.json"
    PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"
    INTERMEDIATE_CHECK_RESULTS_PATH = "output/results/intermediate/extracted_grundschutz_check_merged.json"
    GROUND_TRUTH_MAP_PATH = "output/results/intermediate/system_structure_map.json"
    SUMMARY_DEPENDENCIES = {
        "ergebnisDerStrukturanalyse": [
            "definitionDesInformationsverbundes", "bereinigterNetzplan", "listeDerGeschaeftsprozesse",
            "listeDerAnwendungen", "listeDerItSysteme", "listeDerRaeumeGebaeudeStandorte",
            "listeDerKommunikationsverbindungen", "stichprobenDokuStrukturanalyse", "listeDerDienstleister"
        ],
        "ergebnisDerSchutzbedarfsfeststellung": [
            "definitionDerSchutzbedarfskategorien", "schutzbedarfGeschaeftsprozesse", "schutzbedarfAnwendungen",
            "schutzbedarfItSysteme", "schutzbedarfRaeume", "schutzbedarfKommunikationsverbindungen",
            "stichprobenDokuSchutzbedarf"
        ],
        "ergebnisDerModellierung": ["modellierungsdetails"],
        "ergebnisItGrundschutzCheck": ["detailsZumItGrundschutzCheck", "benutzerdefinierteBausteine"],
        # 'ergebnisDerDokumentenpruefung' is the final summary and will use all findings by default.
    }
    
    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self.rag_client = rag_client
        self.control_catalog = ControlCatalog()
        self.prompt_config = self._load_asset_json(self.PROMPT_CONFIG_PATH)
        self.execution_plan = self._build_execution_plan_from_template()
        self._doc_map = self.rag_client._document_category_map
        self._ground_truth_map = None # Lazy loaded
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME} with dynamic execution plan.")

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f: return json.load(f)

    async def _get_ground_truth_map(self) -> Dict[str, Any]:
        """Lazy loads the ground truth map and caches it."""
        if self._ground_truth_map is None:
            try:
                self._ground_truth_map = await self.gcs_client.read_json_async(self.GROUND_TRUTH_MAP_PATH)
            except NotFound:
                logging.error(f"FATAL: Ground truth map not found at '{self.GROUND_TRUTH_MAP_PATH}'. Please run the extraction stage.")
                raise
        return self._ground_truth_map

    async def _process_details_zum_it_grundschutz_check(self) -> Dict[str, Any]:
        """
        Uses the pre-computed/refined data to answer the five questions with a mix of
        deterministic and targeted AI-driven logic.
        """
        logging.info("Processing 3.6.1 'Details zum IT-Grundschutz-Check' using pre-computed data...")
        try:
            check_data = self.gcs_client.read_json(self.INTERMEDIATE_CHECK_RESULTS_PATH)
            anforderungen = check_data.get("anforderungen", [])
        except NotFound:
            logging.error(f"FATAL: The required intermediate file '{self.INTERMEDIATE_CHECK_RESULTS_PATH}' was not found. Please run the 'Grundschutz-Check-Extraction' stage first.")
            raise

        answers = [None] * 5
        findings = []
        ground_truth_map = await self._get_ground_truth_map()

        # Task E: Coverage Check
        all_mapped_kuerzel = {k for k_list in ground_truth_map.get("baustein_to_zielobjekt_mapping", {}).values() for k in k_list}
        all_checked_kuerzel = {a.get("zielobjekt_kuerzel") for a in anforderungen}
        missing_in_check = all_mapped_kuerzel - all_checked_kuerzel
        if missing_in_check:
            desc = f"Die Zielobjekte {sorted(list(missing_in_check))} sind in der Modellierung vorhanden, aber es wurden für sie keine Anforderungen im Grundschutz-Check gefunden oder verarbeitet."
            findings.append({"category": "AG", "description": desc})
            logging.warning(f"Coverage Check (Task E) failed: {desc}")

        # Q1: Status erhoben? (Deterministic)
        answers[0] = all(a.get("umsetzungsstatus") for a in anforderungen)
        if not answers[0]:
            findings.append({"category": "AG", "description": "Nicht für alle Anforderungen wurde ein Umsetzungsstatus erhoben."})

        # Q5: Prüfung < 12 Monate? (Deterministic)
        one_year_ago = datetime.now() - timedelta(days=365)
        outdated = [a for a in anforderungen if datetime.strptime(a.get("datumLetztePruefung", "1970-01-01"), "%Y-%m-%d") < one_year_ago]
        answers[4] = not bool(outdated)
        if outdated:
            findings.append({"category": "AG", "description": f"Die Prüfung von {len(outdated)} Anforderungen liegt mehr als 12 Monate zurück."})
            
        # Correctly load the configuration for targeted questions
        ch3_config = self.prompt_config["stages"]["Chapter-3"]
        targeted_prompt_template = ch3_config["targeted_question"]["prompt"]
        questions_config = ch3_config["questions"]

        # Q2: "entbehrlich" plausibel? (Targeted AI - Task D)
        entbehrlich_items = [a for a in anforderungen if a.get("umsetzungsstatus") == "entbehrlich"]
        risikoanalyse_uris = self.rag_client.get_gcs_uris_for_categories(["Risikoanalyse"])
        if entbehrlich_items:
            for item in entbehrlich_items: # Enrich with control level
                item['level'] = self.control_catalog.get_control_level(item.get('id'))
            
            question = questions_config["entbehrlich"]
            prompt = targeted_prompt_template.format(
                question=question,
                json_data=json.dumps(entbehrlich_items, indent=2, ensure_ascii=False),
            )
            res = await self.ai_client.generate_json_response(
                prompt, self._load_asset_json("assets/schemas/generic_1_question_schema.json"), 
                gcs_uris=risikoanalyse_uris, request_context_log="3.6.1-Q2"
            )
            answers[1], findings = (res['answers'][0], findings + [res['finding']] if res['finding']['category'] != 'OK' else findings)
        else:
            answers[1] = True

        # Q3: MUSS-Anforderungen erfüllt? (Targeted AI)
        level_1_ids = self.control_catalog.get_level_1_control_ids()
        muss_anforderungen = [a for a in anforderungen if a.get("id") in level_1_ids]
        if muss_anforderungen:
            prompt = targeted_prompt_template.format(
                question=questions_config["muss_anforderungen"],
                json_data=json.dumps(muss_anforderungen, indent=2, ensure_ascii=False)
            )
            res = await self.ai_client.generate_json_response(prompt, self._load_asset_json("assets/schemas/generic_1_question_schema.json"), request_context_log="3.6.1-Q3")
            answers[2], findings = (res['answers'][0], findings + [res['finding']] if res['finding']['category'] != 'OK' else findings)
        else:
            answers[2] = True

        # Q4: Nicht/teilweise umgesetzte in A.6? (Targeted AI)
        unmet_items = [a for a in anforderungen if a.get("umsetzungsstatus") in ["Nein", "teilweise"]]
        realisierungsplan_uris = self.rag_client.get_gcs_uris_for_categories(["Realisierungsplan"])
        if unmet_items and realisierungsplan_uris:
            prompt = targeted_prompt_template.format(
                question=questions_config["nicht_umgesetzt"],
                json_data=json.dumps(unmet_items, indent=2, ensure_ascii=False)
            )
            res = await self.ai_client.generate_json_response(
                prompt, self._load_asset_json("assets/schemas/generic_1_question_schema.json"), 
                gcs_uris=realisierungsplan_uris, request_context_log="3.6.1-Q4"
            )
            answers[3], findings = (res['answers'][0], findings + [res['finding']] if res['finding']['category'] != 'OK' else findings)
        else:
            answers[3] = not unmet_items
            if unmet_items and not realisierungsplan_uris:
                findings.append({"category": "AG", "description": "Es gibt nicht umgesetzte Anforderungen, aber der Realisierungsplan (A.6) wurde nicht gefunden, um die Dokumentation zu überprüfen."})

        # Consolidate findings
        final_finding = {"category": "OK", "description": "Alle Prüfungen für den IT-Grundschutz-Check waren erfolgreich."}
        if findings:
            final_finding["category"] = "AS" if any(f['category'] == 'AS' for f in findings) else "AG"
            final_finding["description"] = "Zusammenfassung: " + " | ".join([f['description'] for f in findings])

        return {"detailsZumItGrundschutzCheck": {"answers": answers, "finding": final_finding}}

    def _check_document_coverage(self) -> Dict[str, Any]:
        """Checks if all critical BSI document types are present."""
        REQUIRED_CATEGORIES = {
            "Sicherheitsleitlinie", "Strukturanalyse", "Schutzbedarfsfeststellung",
            "Modellierung", "Grundschutz-Check", "Risikoanalyse", "Realisierungsplan"
        }
        present_categories = set(self._doc_map.keys())
        missing_categories = REQUIRED_CATEGORIES - present_categories

        if not missing_categories:
            return {"category": "OK", "description": "Alle kritischen Dokumententypen sind vorhanden."}
        else:
            desc = f"Kritische Dokumente fehlen: {', '.join(sorted(list(missing_categories)))}. Dies ist eine schwerwiegende Abweichung."
            logging.warning(f"Document coverage check failed. Missing: {missing_categories}")
            return {"category": "AS", "description": desc}

    def _build_execution_plan_from_template(self) -> List[Dict[str, Any]]:
        """Parses master_report_template.json to build a dynamic list of tasks."""
        plan = []
        template = self._load_asset_json(self.TEMPLATE_PATH)
        ch3_template = template.get("bsiAuditReport", {}).get("dokumentenpruefung", {})
        
        for subchapter_name, subchapter_data in ch3_template.items():
             if not isinstance(subchapter_data, dict): continue
             task = self._create_task_from_section(subchapter_name, subchapter_data)
             if task: plan.append(task)
             for section_key, section_data in subchapter_data.items():
                if isinstance(section_data, dict):
                    task = self._create_task_from_section(section_key, section_data)
                    if task: plan.append(task)
        return plan

    def _create_task_from_section(self, key: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """Creates a single task dictionary for the execution plan."""
        task_config = self.prompt_config["stages"]["Chapter-3"].get(key)
        if not task_config: return None

        task = {"key": key, "type": task_config.get("type", "ai_driven")}
        if task["type"] == "custom_logic": return task

        task["schema_path"] = task_config["schema_path"]
        task["source_categories"] = task_config.get("source_categories")

        if task["type"] == "ai_driven" or key == 'modellierungsdetails':
            generic_prompt = self.prompt_config["stages"]["Chapter-3"]["generic_question"]["prompt"]
            # For modellierungsdetails, the prompt is custom in the config
            task['prompt'] = task_config.get('prompt', generic_prompt)
            questions = [item["questionText"] for item in data.get("content", []) if item.get("type") == "question"]
            task["questions_formatted"] = "\n".join(f"{i+1}. {q}" for i, q in enumerate(questions))
        elif task["type"] == "summary":
            task["prompt"] = self.prompt_config["stages"]["Chapter-3"]["generic_summary"]["prompt"]
            task["summary_topic"] = data.get("title", key)
        return task

    async def _process_ai_subchapter(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """Generates content for a single AI-driven subchapter."""
        key, schema_path = task["key"], task["schema_path"]
        
        prompt_format_args = {"questions": task.get("questions_formatted", "")}

        # Task C: Inject ground truth context for modellierungsdetails
        if key == 'modellierungsdetails':
            ground_truth_map = await self._get_ground_truth_map()
            zielobjekte_list = ground_truth_map.get('zielobjekte', [])
            prompt_format_args['zielobjekte_json'] = json.dumps(zielobjekte_list, indent=2, ensure_ascii=False)
        
        prompt = task["prompt"].format(**prompt_format_args)
        uris = self.rag_client.get_gcs_uris_for_categories(task.get("source_categories"))
        
        if not uris and task.get("source_categories") is not None:
             return {key: {"error": f"No source documents for categories: {task.get('source_categories')}"}}
        try:
            data = await self.ai_client.generate_json_response(prompt, self._load_asset_json(schema_path), uris, f"Chapter-3: {key}")
            if key == "aktualitaetDerReferenzdokumente":
                coverage_finding = self._check_document_coverage()
                if coverage_finding['category'] != 'OK': data['finding'] = coverage_finding
            return {key: data}
        except Exception as e:
            logging.error(f"Failed to generate for {key}: {e}", exc_info=True)
            return {key: {"error": str(e)}}

    async def _process_summary_subchapter(self, task: Dict[str, Any], previous_findings: str) -> Dict[str, Any]:
        """Generates a summary/verdict for a subchapter."""
        key = task["key"]
        prompt = task["prompt"].format(summary_topic=task["summary_topic"], previous_findings=previous_findings)
        try:
            return {key: await self.ai_client.generate_json_response(prompt, self._load_asset_json(task["schema_path"]), request_context_log=f"Chapter-3 Summary: {key}")}
        except Exception as e:
            return {key: {"error": str(e)}}

    def _get_findings_from_results(self, results_list: List[Dict]) -> str:
        """Extracts and formats findings from a list of results for summary prompts."""
        findings = []
        for res_dict in results_list:
            if not res_dict: continue
            result_data = list(res_dict.values())[0]
            if isinstance(result_data, dict) and isinstance(result_data.get('finding'), dict):
                finding = result_data['finding']
                if finding.get('category') != "OK":
                    findings.append(f"- [{finding.get('category')}]: {finding.get('description')}")
        return "\n".join(findings) if findings else "No specific findings were generated."

    async def run(self, force_overwrite: bool = False) -> dict:
        """Executes the dynamically generated plan for Chapter 3."""
        logging.info(f"Executing dynamically generated plan for stage: {self.STAGE_NAME}")
        
        aggregated_results, processed_results = {}, []
        custom_tasks = [t for t in self.execution_plan if t and t.get("type") == "custom_logic"]
        ai_tasks = [t for t in self.execution_plan if t and (t.get("type") == "ai_driven" or t.get("key") == "modellierungsdetails")]
        summary_tasks = [t for t in self.execution_plan if t and t.get("type") == "summary"]

        for task in custom_tasks:
            key = task['key']
            logging.info(f"--- Processing custom logic task: {key} ---")
            if key == 'detailsZumItGrundschutzCheck':
                result = await self._process_details_zum_it_grundschutz_check()
                processed_results.append(result)
                aggregated_results.update(result)
        
        if ai_tasks:
            ai_coroutines = [self._process_ai_subchapter(task) for task in ai_tasks]
            ai_results = await asyncio.gather(*ai_coroutines)
            processed_results.extend(ai_results)
            for res in ai_results: aggregated_results.update(res)

        if summary_tasks:
            findings_text = self._get_findings_from_results(processed_results)
            summary_coroutines = [self._process_summary_subchapter(task, findings_text) for task in summary_tasks]
            for res in await asyncio.gather(*summary_coroutines): aggregated_results.update(res)

        logging.info(f"Successfully aggregated results for all of stage {self.STAGE_NAME}")
        return aggregated_results
==== bsi-audit-automator/src/audit/stages/stage_4_pruefplan.py ====
# src/audit/stages/stage_4_pruefplan.py
import logging
import json
import asyncio
from typing import Dict, Any
from google.cloud.exceptions import NotFound

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient

class Chapter4Runner:
    """
    Handles generating the audit plan for Chapter 4 "Erstellung eines Prüfplans".
    It uses the ground-truth system map (generated by Chapter 3) to create a realistic
    and accurate audit plan.
    """
    STAGE_NAME = "Chapter-4"
    PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"
    GROUND_TRUTH_MAP_PATH = "output/results/intermediate/system_structure_map.json"


    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self.prompt_config = self._load_asset_json(self.PROMPT_CONFIG_PATH)
        self.subchapter_definitions = self._load_subchapter_definitions()
        self.ground_truth_map = None
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _load_ground_truth_map(self) -> None:
        """Loads the ground truth system structure map from GCS."""
        try:
            self.ground_truth_map = self.gcs_client.read_json(self.GROUND_TRUTH_MAP_PATH)
            logging.info(f"Successfully loaded ground truth map for Chapter 4 planning.")
        except NotFound:
            logging.warning(f"Ground truth map not found at '{self.GROUND_TRUTH_MAP_PATH}'. Audit plan will be less accurate.")
            self.ground_truth_map = {} # Ensure it's not None
        except Exception as e:
            logging.error(f"Failed to load or parse ground truth map: {e}", exc_info=True)
            self.ground_truth_map = {}

    def _load_subchapter_definitions(self) -> Dict[str, Any]:
        """
        Loads definitions for all Chapter 4 subchapters from the central prompt config.
        The logic for which Baustein selection to run is now based on the AUDIT_TYPE.
        """
        logging.info(f"Loading Chapter 4 definitions for audit type: {self.config.audit_type}")
        definitions = {}
        ch4_config = self.prompt_config["stages"]["Chapter-4"]

        # AI-driven Baustein selection, conditional on the specific audit type
        if self.config.audit_type == "Zertifizierungsaudit":
            logging.info("Loading definitions for 'Zertifizierungsaudit'.")
            definitions["auswahlBausteineErstRezertifizierung"] = ch4_config["auswahlBausteineErstRezertifizierung"]
        elif self.config.audit_type == "1. Überwachungsaudit":
            logging.info("Loading definitions for '1. Überwachungsaudit'.")
            definitions["auswahlBausteine1Ueberwachungsaudit"] = ch4_config["auswahlBausteine1Ueberwachungsaudit"]
        elif self.config.audit_type == "2. Überwachungsaudit":
            logging.info("Loading definitions for '2. Überwachungsaudit'.")
            definitions["auswahlBausteine2Ueberwachungsaudit"] = ch4_config["auswahlBausteine2Ueberwachungsaudit"]
        else:
            logging.warning(f"Unknown audit type '{self.config.audit_type}'. No Baustein selection definitions loaded.")
            
        # Common parts for all audit types
        definitions["auswahlStandorte"] = {
            "key": "4.1.4",
            "type": "deterministic",
            "table": {
                "rows": [{"Standort": "Hauptstandort", "Erst- bzw. Rezertifizierung": "Ja", "1. Überwachungsaudit": "Ja", "2. Überwachungsaudit": "Ja", "Begründung für die Auswahl": "Zentraler Standort mit kritischer Infrastruktur."}]
            }
        }
        definitions["auswahlMassnahmenAusRisikoanalyse"] = ch4_config["auswahlMassnahmenAusRisikoanalyse"]

        # Mark the type for processing
        for key in definitions:
            if "prompt" in definitions[key]:
                definitions[key]["type"] = "ai_driven"

        return definitions

    async def _process_single_subchapter(self, name: str, definition: dict) -> Dict[str, Any]:
        """Generates planning content for a single subchapter, supporting AI and deterministic modes."""
        logging.info(f"Starting planning for subchapter: {definition.get('key', name)} ({name})")
        
        if definition.get("type") == "deterministic":
            logging.info(f"Processing '{name}' deterministically.")
            return {name: {"table": definition["table"]}}

        # AI-driven
        prompt_template = definition["prompt"]
        # NEW: Inject the ground truth map into the prompt.
        prompt = prompt_template.format(
            ground_truth_map_json=json.dumps(self.ground_truth_map, indent=2, ensure_ascii=False)
        )
        schema = self._load_asset_json(definition["schema_path"])
        
        try:
            generated_data = await self.ai_client.generate_json_response(
                prompt=prompt,
                json_schema=schema,
                request_context_log=f"Chapter-4: {name}"
            )
            logging.info(f"Successfully generated plan for subchapter {definition.get('key', name)}")
            # The AI response is the table content itself, e.g. {"rows": [...]}.
            return {name: generated_data}
        except Exception as e:
            logging.error(f"Failed to generate plan for subchapter {definition.get('key', name)}: {e}", exc_info=True)
            return {name: {"rows": []}} # Return empty structure on failure

    async def run(self, force_overwrite: bool = False) -> dict:
        """
        Executes the planning logic for all of Chapter 4 in parallel.
        """
        logging.info(f"Executing stage: {self.STAGE_NAME}")

        # Load the map first, it's a dependency for the prompts
        self._load_ground_truth_map()

        if not self.subchapter_definitions:
            logging.warning(f"No subchapter definitions found. Skipping Chapter 4.")
            return {}

        tasks = [self._process_single_subchapter(name, definition) for name, definition in self.subchapter_definitions.items()]
        results_list = await asyncio.gather(*tasks)

        aggregated_results = {}
        for res_dict in results_list:
            aggregated_results.update(res_dict)
            
        logging.info(f"Successfully aggregated planning results for stage {self.STAGE_NAME}")
        return aggregated_results
==== bsi-audit-automator/src/audit/stages/stage_5_vor_ort_audit.py ====
# file: src/audit/stages/stage_5_vor_ort_audit.py
import logging
import json
from typing import Dict, Any, List, Tuple
from google.cloud.exceptions import NotFound

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient
from src.audit.stages.control_catalog import ControlCatalog

class Chapter5Runner:
    """
    Handles generating content for Chapter 5 "Vor-Ort-Audit".
    It deterministically prepares the control checklist for the manual audit,
    enriching it with data extracted in prior stages.
    """
    STAGE_NAME = "Chapter-5"
    INTERMEDIATE_CHECK_RESULTS_PATH = "output/results/intermediate/extracted_grundschutz_check_merged.json"
    GROUND_TRUTH_MAP_PATH = "output/results/intermediate/system_structure_map.json"

    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self.control_catalog = ControlCatalog()
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    def _load_system_structure_map(self) -> Dict[str, Any]:
        """
        Loads the ground-truth system structure map which contains the authoritative
        Baustein-to-Zielobjekt mappings.
        """
        try:
            system_map = self.gcs_client.read_json(self.GROUND_TRUTH_MAP_PATH)
            logging.info(f"Successfully loaded ground truth map from: {self.GROUND_TRUTH_MAP_PATH}")
            return system_map
        except NotFound:
            logging.error(f"FATAL: Ground truth map '{self.GROUND_TRUTH_MAP_PATH}' not found. Cannot generate Chapter 5 checklist. Please run the 'Grundschutz-Check-Extraction' stage first.")
            raise
        except Exception as e:
            logging.error(f"Failed to load or parse ground truth map: {e}", exc_info=True)
            raise

    def _load_extracted_check_data(self) -> Dict[Tuple[str, str], Dict[str, Any]]:
        """
        Loads the refined Grundschutz-Check data and creates a lookup map
        keyed by a composite tuple of (requirement_id, zielobjekt_kuerzel)
        for efficient, context-aware access.
        """
        try:
            data = self.gcs_client.read_json(self.INTERMEDIATE_CHECK_RESULTS_PATH)
            anforderungen_list = data.get("anforderungen", [])
            # The key is a tuple of the requirement ID and the target object's short ID (Kürzel).
            # This correctly handles the same requirement applied to multiple objects.
            lookup_map = {
                (item['id'], item['zielobjekt_kuerzel']): item 
                for item in anforderungen_list if 'id' in item and 'zielobjekt_kuerzel' in item
            }
            logging.info(f"Successfully loaded and mapped {len(lookup_map)} unique requirement-object pairs for Chapter 5.")
            return lookup_map
        except NotFound:
            logging.warning(f"Refined check data file '{self.INTERMEDIATE_CHECK_RESULTS_PATH}' not found. Checklist will not contain customer explanations. Please run the 'Grundschutz-Check-Extraction' stage first.")
            return {}
        except Exception as e:
            logging.error(f"Failed to load or parse refined check data: {e}", exc_info=True)
            return {}

    def _generate_control_checklist(self, chapter_4_data: Dict[str, Any], system_structure_map: Dict[str, Any], extracted_data_map: Dict[Tuple[str, str], Dict[str, Any]]) -> Dict[str, Any]:
        """
        Deterministically generates the control checklist for subchapter 5.5.2,
        using the specific Zielobjekt selected in the audit plan (Chapter 4)
        to select the exact instance to audit and populate with customer data.
        """
        name = "verifikationDesITGrundschutzChecks"
        logging.info(f"Generating control checklist for {name} based on the specific audit plan...")
        
        # Combine bausteine from all possible sections of chapter 4
        selected_bausteine = []
        baustein_sections = [
            "auswahlBausteineErstRezertifizierung", 
            "auswahlBausteine1Ueberwachungsaudit", 
            "auswahlBausteine2Ueberwachungsaudit"
        ]
        for section in baustein_sections:
            section_data = chapter_4_data.get(section, {})
            if isinstance(section_data, dict):
                 selected_bausteine.extend(section_data.get("rows", []))
        
        if not selected_bausteine:
            logging.warning("No Bausteine found in Chapter 4 results. Checklist for 5.5.2 will be empty.")
            return {name: {"einzelergebnisse": {"bausteinPruefungen": []}}}

        baustein_pruefungen_list = []
        for i, baustein_plan_item in enumerate(selected_bausteine):
            baustein_id_full = baustein_plan_item.get("Baustein", "")
            if not baustein_id_full: continue
            baustein_id = baustein_id_full.split(" ")[0]

            # --- ROBUSTNESS FIX (Task H) ---
            # Directly get the name and Kürzel from the plan. No more fragile name-based lookups.
            zielobjekt_name_from_plan = baustein_plan_item.get("Zielobjekt-Name")
            planned_zielobjekt_kuerzel = baustein_plan_item.get("Zielobjekt-Kürzel")

            if not planned_zielobjekt_kuerzel:
                logging.warning(f"Could not find 'Zielobjekt-Kürzel' in audit plan for Baustein '{baustein_id}'. Specific details for its controls will be missing.")

            controls = self.control_catalog.get_controls_for_baustein_id(baustein_id)
            
            anforderungen_list = []
            for control in controls:
                control_id = control.get("id", "N/A")
                # The lookup key is now robustly created using the Kürzel from the plan.
                lookup_key = (control_id, planned_zielobjekt_kuerzel) if planned_zielobjekt_kuerzel else None
                extracted_details = extracted_data_map.get(lookup_key, {})
                
                customer_explanation = extracted_details.get("umsetzungserlaeuterung", "Keine spezifische Angabe für dieses Zielobjekt im Grundschutz-Check gefunden.")
                bewertung_status_raw = extracted_details.get("umsetzungsstatus", "N/A")

                status_map = {"Ja": "Umgesetzt", "Nein": "Nicht umgesetzt", "teilweise": "Teilweise umgesetzt", "entbehrlich": "Entbehrlich"}
                final_bewertung_status = status_map.get(bewertung_status_raw, bewertung_status_raw)

                anforderungen_list.append({
                    "nummer": control_id,
                    "anforderung": control.get("title", "N/A"),
                    "bewertung": final_bewertung_status,
                    "dokuAntragsteller": customer_explanation,
                    "pruefmethode": { "D": False, "I": False, "C": False, "S": False, "A": False, "B": False },
                    "auditfeststellung": "",
                    "abweichungen": ""
                })
            
            # Create the new subchapter structure
            baustein_pruefungen_list.append({
                "subchapterNumber": f"5.5.2.{i+1}",
                "title": f"Prüfung für Baustein: {baustein_id_full}",
                "baustein": baustein_id_full,
                "bezogenAufZielobjekt": zielobjekt_name_from_plan,
                "auditiertAm": "",
                "auditor": "",
                "befragtWurde": "",
                "anforderungen": anforderungen_list
            })

        logging.info(f"Generated checklist with {len(baustein_pruefungen_list)} specific Baustein/Zielobjekt subchapters for manual audit.")
        return {name: {"einzelergebnisse": {"bausteinPruefungen": baustein_pruefungen_list}}}
        
    def _generate_risikoanalyse_checklist(self, chapter_4_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generates the checklist for risk analysis measures (5.6.2) based on
        the measures selected in Chapter 4.1.5.
        """
        name = "risikoanalyseA5"
        logging.info(f"Deterministically generating checklist for {name} (5.6.2)...")
        
        selected_measures = chapter_4_data.get("auswahlMassnahmenAusRisikoanalyse", {}).get("rows", [])
        
        if not selected_measures:
            logging.warning("No measures from risk analysis found in Chapter 4 results. Checklist for 5.6.2 will be empty.")
            return {name: {"einzelergebnisseDerRisikoanalyse": {"massnahmenPruefungen": []}}}

        massnahmen_pruefungen_list = []
        for measure in selected_measures:
            massnahmen_pruefungen_list.append({
                "massnahme": measure.get("Maßnahme", "N/A"),
                "zielobjekt": measure.get("Zielobjekt", "N/A"),
                "bewertung": "",
                "dokuAntragsteller": "",
                "pruefmethode": { "D": False, "I": False, "C": False, "S": False, "A": False, "B": False },
                "auditfeststellung": "",
                "abweichungen": ""
            })
            
        logging.info(f"Generated checklist with {len(massnahmen_pruefungen_list)} risk analysis measures.")
        return {name: {"einzelergebnisseDerRisikoanalyse": {"massnahmenPruefungen": massnahmen_pruefungen_list}}}

    async def run(self, force_overwrite: bool = False) -> dict:
        """
        Executes the generation logic for Chapter 5.
        """
        logging.info(f"Executing stage: {self.STAGE_NAME}")
        
        # Load all dependencies first
        try:
            ch4_results_path = f"{self.config.output_prefix}results/Chapter-4.json"
            chapter_4_data = self.gcs_client.read_json(ch4_results_path)
            logging.info("Successfully loaded dependency: Chapter 4 results.")
        except Exception as e:
            logging.error(f"Could not load Chapter 4 results, which are required for Chapter 5. Aborting stage. Error: {e}")
            raise

        system_structure_map = self._load_system_structure_map()
        extracted_check_data_map = self._load_extracted_check_data()
        
        checklist_result = self._generate_control_checklist(chapter_4_data, system_structure_map, extracted_check_data_map)
        risiko_result = self._generate_risikoanalyse_checklist(chapter_4_data)
        
        final_result = {**checklist_result, **risiko_result}
        
        logging.info(f"Successfully prepared data for stage {self.STAGE_NAME}")
        return final_result
==== bsi-audit-automator/src/audit/stages/stage_7_anhang.py ====
# src/audit/stages/stage_7_anhang.py
import logging
import json
from typing import Dict, Any

from src.config import AppConfig
from src.clients.gcs_client import GcsClient

class Chapter7Runner:
    """
    Handles generating the appendix for Chapter 7.
    - 7.1 is generated deterministically by listing GCS source files.
    - 7.2 (Deviations) is populated by the ReportGenerator from the central findings file.
    """
    STAGE_NAME = "Chapter-7"

    def __init__(self, config: AppConfig, gcs_client: GcsClient):
        self.config = config
        self.gcs_client = gcs_client
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    async def _generate_referenzdokumente_table(self) -> Dict[str, Any]:
        """Generates the table of reference documents by listing source files in GCS."""
        logging.info("Generating subchapter 7.1 (Referenzdokumente) from GCS file list.")
        try:
            source_files = self.gcs_client.list_files()
            rows = []
            for i, blob in enumerate(source_files):
                rows.append({
                    "Nummer": f"A.{i}",
                    "Kurzbezeichnung": blob.name.split('/')[-1],
                    "Dateiname / Verweis": blob.name,
                    "Version, Datum": blob.updated.strftime("%Y-%m-%d") if blob.updated else "N/A",
                    "Relevante Änderungen": "Initial eingereicht für Audit."
                })
            # The key must match the structure in master_report_template.json
            return {"referenzdokumente": {"table": {"rows": rows}}}
        except Exception as e:
            logging.error(f"Failed to generate Referenzdokumente table: {e}", exc_info=True)
            return {"referenzdokumente": {"table": {"rows": []}}}

    async def run(self, force_overwrite: bool = False) -> dict:
        """Executes the generation logic for Chapter 7."""
        logging.info(f"Executing stage: {self.STAGE_NAME}")
        # Only one task remains for this chapter.
        result = await self._generate_referenzdokumente_table()
        logging.info(f"Successfully generated data for stage {self.STAGE_NAME}")
        return result
==== bsi-audit-automator/src/audit/stages/stage_gs_check_extraction.py ====
# bsi-audit-automator/src/audit/stages/stage_gs_check_extraction.py
import logging
import json
import asyncio
from typing import Dict, Any
from collections import defaultdict

from google.cloud.exceptions import NotFound

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.document_ai_client import DocumentAiClient
from src.clients.ai_client import AiClient
from src.clients.rag_client import RagClient


class GrundschutzCheckExtractionRunner:
    """
    A dedicated stage for the "Ground-Truth-Driven Semantic Chunking" strategy.
    It orchestrates a two-stage process:
    1.  Uses Document AI to perform high-fidelity form parsing on the Grundschutz-Check PDF.
    2.  Uses Gemini to refine and structure the JSON output from Document AI.
    The stage is idempotent and saves intermediate results for debugging and cost-efficiency.
    """
    STAGE_NAME = "Grundschutz-Check-Extraction"
    PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"
    GROUND_TRUTH_MAP_PATH = "output/results/intermediate/system_structure_map.json"
    DOC_AI_RAW_OUTPUT_PATH = "output/results/intermediate/doc_ai_raw_output.json"
    FINAL_MERGED_OUTPUT_PATH = "output/results/intermediate/extracted_grundschutz_check_merged.json"

    def __init__(self, config: AppConfig, gcs_client: GcsClient, doc_ai_client: DocumentAiClient, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.gcs_client = gcs_client
        self.doc_ai_client = doc_ai_client
        self.ai_client = ai_client
        self.rag_client = rag_client
        self.prompt_config = self._load_asset_json(self.PROMPT_CONFIG_PATH)
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    async def _build_system_structure_map(self, force_remap: bool) -> Dict[str, Any]:
        """Orchestrates the creation of the ground truth map."""
        if force_remap:
            logging.info("Force remapping enabled. Generating new ground truth map.")
        else:
            try:
                map_data = await self.gcs_client.read_json_async(self.GROUND_TRUTH_MAP_PATH)
                logging.info(f"Using cached ground truth map from: {self.GROUND_TRUTH_MAP_PATH}")
                return map_data
            except NotFound:
                logging.info("Ground truth map not found. Generating...")

        # --- Generation logic only runs if force_remap is true or file not found ---
        zielobjekte_uris = self.rag_client.get_gcs_uris_for_categories(["Strukturanalyse"])
        zielobjekte_config = self.prompt_config["stages"]["Chapter-3-Ground-Truth"]["extract_zielobjekte"]
        zielobjekte_res = await self.ai_client.generate_json_response(
            prompt=zielobjekte_config["prompt"],
            json_schema=self._load_asset_json(zielobjekte_config["schema_path"]),
            gcs_uris=zielobjekte_uris,
            request_context_log="GT: Extract Zielobjekte"
        )
        zielobjekte_list = zielobjekte_res.get("zielobjekte", [])

        modellierung_uris = self.rag_client.get_gcs_uris_for_categories(["Modellierung"])
        mappings_config = self.prompt_config["stages"]["Chapter-3-Ground-Truth"]["extract_baustein_mappings"]
        mappings_res = await self.ai_client.generate_json_response(
            prompt=mappings_config["prompt"],
            json_schema=self._load_asset_json(mappings_config["schema_path"]),
            gcs_uris=modellierung_uris,
            request_context_log="GT: Extract Baustein Mappings"
        )
        
        # --- IMPROVEMENT: Correctly group the mappings into a dictionary ---
        baustein_mappings = defaultdict(list)
        for mapping in mappings_res.get("mappings", []):
            baustein_id = mapping.get("baustein_id")
            zielobjekt_kuerzel = mapping.get("zielobjekt_kuerzel")
            if baustein_id and zielobjekt_kuerzel:
                baustein_mappings[baustein_id].append(zielobjekt_kuerzel)
        
        final_map = {
            "zielobjekte": zielobjekte_list,
            "baustein_to_zielobjekt_mapping": baustein_mappings
        }
        
        await self.gcs_client.upload_from_string_async(
            json.dumps(final_map, indent=2, ensure_ascii=False), 
            self.GROUND_TRUTH_MAP_PATH
        )
        logging.info(f"Successfully created and saved ground truth map to {self.GROUND_TRUTH_MAP_PATH}")
        return final_map

    async def run(self, force_overwrite: bool = False) -> Dict[str, Any]:
        """
        Main execution method for the stage. It creates the ground-truth map and
        the refined Grundschutz-Check data, saving them to GCS.
        """
        logging.info(f"Executing stage: {self.STAGE_NAME}")
        
        # --- IDEMPOTENCY CHECK 1: CHECK FOR FINAL OUTPUT ---
        if not force_overwrite and self.gcs_client.blob_exists(self.FINAL_MERGED_OUTPUT_PATH):
            logging.info(f"Final merged output file already exists at '{self.FINAL_MERGED_OUTPUT_PATH}'. Skipping entire stage.")
            return {"status": "skipped", "reason": "Final output file already exists."}

        # Build the ground truth map first, it's a dependency for the Gemini prompt
        ground_truth_map = await self._build_system_structure_map(force_remap=force_overwrite)
        
        doc_ai_output = None
        # --- IDEMPOTENCY CHECK 2: CHECK FOR INTERMEDIATE DOC AI OUTPUT ---
        if not force_overwrite and self.gcs_client.blob_exists(self.DOC_AI_RAW_OUTPUT_PATH):
            logging.info(f"Found existing Document AI output at '{self.DOC_AI_RAW_OUTPUT_PATH}'. Skipping Document AI step.")
            doc_ai_output = await self.gcs_client.read_json_async(self.DOC_AI_RAW_OUTPUT_PATH)
        else:
            logging.info("Intermediate Document AI output not found or --force used. Running Document AI processing.")
            # STEP 1: Run Document AI Processing
            check_uris = self.rag_client.get_gcs_uris_for_categories(["Grundschutz-Check", "test.pdf"])
            if not check_uris:
                raise FileNotFoundError("Could not find document with category 'Grundschutz-Check' or 'test.pdf'. This is required.")
            
            # If both real and test docs are present, prefer the test one.
            doc_uri_to_process = next((uri for uri in check_uris if 'test.pdf' in uri), check_uris[0])

            doc_ai_output = await self.doc_ai_client.process_document_async(doc_uri_to_process)
        
        if not doc_ai_output:
            raise RuntimeError("Document AI processing failed to produce an output.")

        # STEP 2: Run Gemini for Refinement
        logging.info("Starting Gemini refinement of Document AI output.")
        config = self.prompt_config["stages"]["Chapter-3"]["detailsZumItGrundschutzCheck_extraction"]
        prompt_template = config["prompt"]
        schema = self._load_asset_json(config["schema_path"])

        # Prepare the context for the refiner prompt
        prompt = prompt_template.format(
            ground_truth_map_json=json.dumps(ground_truth_map, indent=2, ensure_ascii=False),
            document_ai_json=json.dumps(doc_ai_output, indent=2, ensure_ascii=False)
        )
        
        # The prompt is now self-contained; no gcs_uris are needed for the Gemini call
        refined_data = await self.ai_client.generate_json_response(
            prompt,
            schema,
            gcs_uris=[], # IMPORTANT: No files attached here
            request_context_log="GS-Check-Refinement"
        )
        
        # STEP 3: Save the final output
        await self.gcs_client.upload_from_string_async(
            json.dumps(refined_data, indent=2, ensure_ascii=False),
            self.FINAL_MERGED_OUTPUT_PATH
        )
        logging.info(f"Successfully created and saved refined Grundschutz-Check data to {self.FINAL_MERGED_OUTPUT_PATH}")
        
        return {"status": "success", "message": f"Successfully generated intermediate files."}
==== bsi-audit-automator/src/audit/stages/stage_previous_report_scan.py ====
# src/audit/stages/stage_previous_report_scan.py
import logging
import json
import asyncio
from typing import Dict, Any

from src.config import AppConfig
from src.clients.ai_client import AiClient
from src.clients.rag_client import RagClient

class PreviousReportScanner:
    """
    A dedicated stage to scan a previous audit report and extract key data.
    It runs three extraction tasks in parallel for maximum efficiency.
    """
    STAGE_NAME = "Scan-Report"
    PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"

    def __init__(self, config: AppConfig, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.ai_client = ai_client
        self.rag_client = rag_client
        self.prompt_config = self._load_asset_json(self.PROMPT_CONFIG_PATH)
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    async def _run_extraction_task(self, task_name: str, gcs_uri: str) -> Dict[str, Any]:
        """
        Runs a single AI extraction task for a part of the report.
        
        Args:
            task_name: The key from the prompt_config (e.g., 'extract_chapter_1').
            gcs_uri: The GCS URI of the previous audit report.

        Returns:
            A dictionary containing the extracted data for that task.
        """
        logging.info(f"Starting extraction for task: {task_name}")
        try:
            task_config = self.prompt_config["stages"][self.STAGE_NAME][task_name]
            prompt = task_config["prompt"]
            schema = self._load_asset_json(task_config["schema_path"])
            
            response = await self.ai_client.generate_json_response(
                prompt=prompt,
                json_schema=schema,
                gcs_uris=[gcs_uri],
                request_context_log=f"{self.STAGE_NAME}: {task_name}"
            )
            return response
        except Exception as e:
            logging.error(f"Extraction task '{task_name}' failed: {e}", exc_info=True)
            return {task_name: {"error": str(e)}} # Return error structure

    async def run(self, force_overwrite: bool = False) -> dict:
        """
        Executes the logic for scanning the previous audit report.
        """
        logging.info(f"Executing stage: {self.STAGE_NAME}")
        
        # 1. Find the previous audit report document
        report_uris = self.rag_client.get_gcs_uris_for_categories(["Vorheriger-Auditbericht"])
        if not report_uris:
            logging.warning("No document with category 'Vorheriger-Auditbericht' found. Skipping stage.")
            return {"status": "skipped", "reason": "No previous audit report found."}
        
        # Use the first report found if multiple are classified
        report_uri = report_uris[0]
        logging.info(f"Found previous audit report to scan: {report_uri}")

        # 2. Define and run all extraction tasks in parallel
        extraction_tasks = ["extract_chapter_1", "extract_chapter_4", "extract_chapter_7"]
        coroutines = [self._run_extraction_task(task_name, report_uri) for task_name in extraction_tasks]
        
        results_list = await asyncio.gather(*coroutines)

        # 3. Aggregate results into a single dictionary
        final_result = {}
        for result in results_list:
            final_result.update(result)
            
        logging.info(f"Successfully completed all extractions for stage {self.STAGE_NAME}")
        return final_result
==== bsi-audit-automator/src/clients/ai_client.py ====
# src/clients/ai_client.py
import logging
import json
import asyncio
import time
import datetime
from typing import List, Dict, Any

from google.cloud import aiplatform
from google.api_core import exceptions as api_core_exceptions
from vertexai.generative_models import GenerativeModel, GenerationConfig, Part

from src.config import AppConfig

GENERATIVE_MODEL_NAME = "gemini-2.5-pro"
MAX_RETRIES = 5
PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"


class AiClient:
    """A client for all Vertex AI model interactions, using the aiplatform SDK."""

    def __init__(self, config: AppConfig):
        self.config = config
        
        with open(PROMPT_CONFIG_PATH, 'r', encoding='utf-8') as f:
            prompt_config = json.load(f)
        
        base_system_message = prompt_config.get("system_message", "")
        if not base_system_message:
            logging.warning("System message is empty. AI calls will not have a predefined persona.")

        # Append the current date to the system prompt
        current_date = datetime.date.today().strftime("%Y-%m-%d")
        self.system_message = f"{base_system_message}\n\nImportant: Today's date is {current_date}."

        aiplatform.init(project=config.gcp_project_id, location=config.region)
        self.generative_model = GenerativeModel(
            GENERATIVE_MODEL_NAME, system_instruction=self.system_message
        )
        self.semaphore = asyncio.Semaphore(config.max_concurrent_ai_requests)

        logging.info(f"Vertex AI Client instantiated for project '{config.gcp_project_id}' in region '{config.region}'.")
        logging.info(f"System Message Context includes today's date: {current_date}")

    async def generate_json_response(self, prompt: str, json_schema: Dict[str, Any], gcs_uris: List[str] = None, request_context_log: str = "Generic AI Request") -> Dict[str, Any]:
        """
        Generates a JSON response from the AI model, enforcing a specific schema and
        optionally providing GCS files as context. Implements an async retry loop
        with exponential backoff and connection limiting.

        Args:
            prompt: The text prompt for the model.
            json_schema: The JSON schema to enforce on the model's output.
            gcs_uris: A list of 'gs://...' URIs pointing to PDF files for context.
            request_context_log: A string to identify the request source in logs.

        Returns:
            The parsed JSON response from the model.
        """
        try:
            schema_for_api = json.loads(json.dumps(json_schema))
            schema_for_api.pop("$schema", None)
        except Exception as e:
            logging.error(f"Failed to process JSON schema before API call: {e}")
            raise ValueError("Invalid JSON schema provided.") from e

        gen_config = GenerationConfig(
            response_mime_type="application/json",
            response_schema=schema_for_api,
            max_output_tokens=65536, # Increased from 65k to 8k as 65k is invalid. Standard is 2048. 8192 is a safe max.
            temperature=0.2,
        )

        # Build the content list. The system message is now handled by the model constructor.
        contents = [prompt]
        if gcs_uris:
            for uri in gcs_uris:
                contents.append(Part.from_uri(uri, mime_type="application/pdf"))
            if self.config.is_test_mode:
                logging.info(f"Attaching {len(gcs_uris)} GCS files to the prompt.")

        async with self.semaphore:
            for attempt in range(MAX_RETRIES):
                try:
                    if self.config.is_test_mode:
                        logging.info(f"[{request_context_log}] Attempt {attempt + 1}/{MAX_RETRIES}: Calling Gemini model '{GENERATIVE_MODEL_NAME}'...")
                    response = await self.generative_model.generate_content_async(
                        contents=contents,
                        generation_config=gen_config,
                    )

                    if not response.candidates:
                        raise ValueError("The model response contained no candidates.")

                    finish_reason = response.candidates[0].finish_reason.name
                    if finish_reason not in ["STOP", "MAX_TOKENS"]:
                        raise ValueError(f"Model finished with non-OK reason: '{finish_reason}'")

                    response_json = json.loads(response.text)
                    if self.config.is_test_mode:
                        logging.info(f"[{request_context_log}] Successfully generated and parsed JSON response on attempt {attempt + 1}.")
                    return response_json

                except (api_core_exceptions.GoogleAPICallError, Exception) as e:
                    wait_time = 2 ** attempt
                    if attempt == MAX_RETRIES - 1:
                        logging.critical(f"[{request_context_log}] AI generation failed after all {MAX_RETRIES} retries.", exc_info=True)
                        raise

                    if isinstance(e, api_core_exceptions.GoogleAPICallError):
                        logging.warning(f"[{request_context_log}] Generation attempt {attempt + 1} failed with Google API Error (Code: {e.code}): {e.message}. Retrying in {wait_time}s...")
                    else:
                        logging.warning(f"[{request_context_log}] Generation attempt {attempt + 1} failed with an exception: {e}. Retrying in {wait_time}s...")

                    await asyncio.sleep(wait_time)

        raise RuntimeError("AI generation failed unexpectedly after exhausting all retries.")
==== bsi-audit-automator/src/clients/document_ai_client.py ====
# bsi-audit-automator/src/clients/document_ai_client.py
import logging
import asyncio
import json
from typing import Dict, Any, Optional
from google.cloud import documentai_v1 as documentai
from google.cloud.documentai_v1.types import (
    BatchDocumentsInputConfig,
    BatchProcessRequest,
    DocumentOutputConfig,
    GcsDocument,
    GcsDocuments
)

from google.api_core.client_options import ClientOptions
from google.api_core.exceptions import GoogleAPICallError

from src.config import AppConfig
from src.clients.gcs_client import GcsClient

# hard path, change later
DOC_AI_RAW_OUTPUT_PATH = "output/results/intermediate/doc_ai_raw_output.json"

class DocumentAiClient:
    """A client for handling interactions with Google Cloud Document AI."""

    def __init__(self, config: AppConfig, gcs_client: GcsClient):
        """
        Initializes the Document AI client.

        Args:
            config: The application configuration object.
            gcs_client: An instance of the GCS client for reading results.
        """
        self.config = config
        self.gcs_client = gcs_client

        if not self.config.doc_ai_processor_name:
            raise ValueError("Document AI processor name is not configured.")

        # The processor name is 'projects/PROJECT/locations/LOCATION/processors/PROCESSOR_ID'
        # The client needs the location for its regional endpoint.
        try:
            self.processor_name = self.config.doc_ai_processor_name
            self.location = self.processor_name.split('/')[3]
            opts = ClientOptions(api_endpoint=f"{self.location}-documentai.googleapis.com")
            self.client = documentai.DocumentProcessorServiceClient(client_options=opts)
            logging.info(f"DocumentAI Client initialized for processor in location '{self.location}'.")
        except (IndexError, TypeError) as e:
            logging.error(f"Could not parse location from Document AI processor name: '{self.processor_name}'")
            raise ValueError("Invalid Document AI processor name format.") from e


    async def process_document_async(self, gcs_input_uri: str) -> Optional[Dict[str, Any]]:
        """
        Processes a single document from GCS using batch processing and returns the
        structured Document object as a dictionary.

        Args:
            gcs_input_uri: The 'gs://' path to the input PDF document.

        Returns:
            The parsed JSON content of the processed document, or None on failure.
        """
        file_name = gcs_input_uri.split('/')[-1]
        # Create a unique output location for this specific processing job
        gcs_output_uri = f"gs://{self.config.bucket_name}/{self.config.output_prefix}doc_ai_results/{file_name}/"
        
        logging.info(f"Starting Document AI batch processing for '{gcs_input_uri}'.")
        logging.info(f"Output will be stored in '{gcs_output_uri}'.")

        input_config = documentai.GcsDocument(gcs_uri=gcs_input_uri, mime_type="application/pdf")
        batch_input_config = documentai.BatchDocumentsInputConfig(gcs_documents=documentai.GcsDocuments(documents=[input_config]))
        
        # Correctly construct the output configuration object
        gcs_output_config = documentai.GcsOutputConfig(gcs_uri=gcs_output_uri)
        output_config = documentai.DocumentOutputConfig(gcs_output_config=gcs_output_config)

        request = documentai.BatchProcessRequest(
            name=self.processor_name,
            input_documents=batch_input_config,
            document_output_config=output_config,
        )

        try:
            operation = self.client.batch_process_documents(request=request)
            
            # Use asyncio.to_thread to run the blocking 'result()' call in a separate thread
            logging.info("Waiting for Document AI batch operation to complete... This may take several minutes.")
            await asyncio.to_thread(operation.result)
            logging.info("Document AI batch operation completed successfully.")

            # After completion, find the resulting JSON file in the output GCS path
            temp_output_prefix = gcs_output_uri.replace(f"gs://{self.config.bucket_name}/", "")
            output_blobs = self.gcs_client.list_files(prefix=temp_output_prefix)
            json_results = [blob for blob in output_blobs if blob.name.endswith(".json")]

            if not json_results:
                logging.error(f"No JSON result file found in Document AI output path: {gcs_output_uri}")
                return None

            # For a single input document, we expect a single result JSON
            source_result_blob_name = json_results[0].name
            logging.info(f"Found Document AI result file: {source_result_blob_name}")

            # Copy the result to our standardized intermediate path for idempotency and debugging
            logging.info(f"Copying raw result to standardized path: {DOC_AI_RAW_OUTPUT_PATH}")
            await self.gcs_client.copy_blob_async(source_result_blob_name, DOC_AI_RAW_OUTPUT_PATH)

            # Read the file from its new permanent location and return its content
            document_data = await self.gcs_client.read_json_async(DOC_AI_RAW_OUTPUT_PATH)
            return document_data

        except GoogleAPICallError as e:
            logging.error(f"Document AI batch processing failed with an API error: {e}", exc_info=True)
            return None
        except Exception as e:
            logging.error(f"An unexpected error occurred during Document AI processing: {e}", exc_info=True)
            return None
==== bsi-audit-automator/src/clients/gcs_client.py ====
# src/clients/gcs_client.py
import logging
import asyncio
from google.cloud import storage
from src.config import AppConfig

class GcsClient:
    """A client for all Google Cloud Storage interactions."""

    def __init__(self, config: AppConfig):
        """
        Initializes the GCS client.

        Args:
            config: The application configuration object.
        """
        self.config = config
        self.storage_client = storage.Client(project=config.gcp_project_id)
        # We derive the bucket name from the config, which should be set by an env var
        # that comes from the terraform output.
        if not config.bucket_name:
            raise ValueError("GCS Bucket name is not configured in the environment.")
        self.bucket = self.storage_client.bucket(config.bucket_name)
        logging.info(f"GCS Client initialized for bucket: gs://{self.bucket.name}")

    def list_files(self, prefix: str = None) -> list[storage.Blob]:
        """
        Lists all files from a given GCS prefix.

        Returns:
            A list of GCS blob objects.
        """
        list_prefix = prefix if prefix is not None else self.config.source_prefix
        logging.info(f"Listing files from prefix: {list_prefix}")
        blobs = self.storage_client.list_blobs(
            self.bucket.name, prefix=list_prefix
        )
        # Filter for common document types, ignore empty "directory" blobs
        files = [blob for blob in blobs if "." in blob.name]
        logging.info(f"Found {len(files)} source files to process.")
        return files

    def download_blob_as_bytes(self, blob: storage.Blob) -> bytes:
        """Downloads a blob from GCS into memory as bytes."""
        logging.debug(f"Downloading blob: {blob.name}")
        return blob.download_as_bytes()

    async def upload_from_string_async(self, content: str, destination_blob_name: str, content_type: str = 'application/json'):
        """Asynchronously uploads a string content to a specified blob in GCS."""
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(
            None, self.upload_from_string, content, destination_blob_name, content_type
        )

    def upload_from_string(self, content: str, destination_blob_name: str, content_type: str = 'application/json'):
        """
        Synchronously uploads a string content to a specified blob in GCS.

        Args:
            content: The string content to upload.
            destination_blob_name: The full path for the object in the bucket.
            content_type: The MIME type of the content.
        """
        logging.info(f"Uploading string content to gs://{self.bucket.name}/{destination_blob_name}")
        blob = self.bucket.blob(destination_blob_name)
        blob.upload_from_string(content, content_type=content_type)
        logging.info(f"Upload complete for {destination_blob_name}.")

    async def read_json_async(self, blob_name: str) -> dict:
        """Asynchronously downloads and parses a JSON file from GCS."""
        loop = asyncio.get_running_loop()
        # Use asyncio.to_thread in Python 3.9+ for a cleaner syntax
        return await loop.run_in_executor(None, self.read_json, blob_name)

    def read_json(self, blob_name: str) -> dict:
        """Downloads and parses a JSON file from GCS."""
        import json
        logging.info(f"Attempting to read JSON from: gs://{self.bucket.name}/{blob_name}")
        blob = self.bucket.blob(blob_name)
        content = blob.download_as_text() # This raises NotFound if not present.
        return json.loads(content)

    def read_text_file(self, blob_name: str) -> str:
        """Downloads and returns the content of a text-based file from GCS."""
        logging.info(f"Attempting to read text from: gs://{self.bucket.name}/{blob_name}")
        blob = self.bucket.blob(blob_name)
        return blob.download_as_text()

    def blob_exists(self, blob_name: str) -> bool:
        """Checks if a blob exists in the GCS bucket."""
        logging.debug(f"Checking for existence of blob: gs://{self.bucket.name}/{blob_name}")
        blob = self.bucket.blob(blob_name)
        return blob.exists()

    def copy_blob(self, source_blob_name: str, destination_blob_name: str):
        """Copies a blob within the same bucket."""
        source_blob = self.bucket.blob(source_blob_name)
        self.bucket.copy_blob(source_blob, self.bucket, destination_blob_name)
        logging.info(f"Copied gs://{self.bucket.name}/{source_blob_name} to gs://{self.bucket.name}/{destination_blob_name}")

    async def copy_blob_async(self, source_blob_name: str, destination_blob_name: str):
        """Asynchronously copies a blob within the same bucket."""
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(
            None, self.copy_blob, source_blob_name, destination_blob_name
        )
==== bsi-audit-automator/src/clients/rag_client.py ====
# src/clients/rag_client.py
import logging
import json
import asyncio
from typing import List, Dict, Any, Optional

from google.cloud.exceptions import NotFound

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient

DOC_MAP_PATH = "output/document_map.json"
MAX_FILES_TEST_MODE = 3
PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"


class RagClient:
    """
    Client to find relevant documents for audit tasks. It manages a map of
    document filenames to BSI categories, creating this map on-demand if it
    doesn't exist. This client is the replacement for the Vector Search RAG pipeline.
    Its name is kept for consistency in the project structure.
    """

    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self._document_category_map: Optional[Dict[str, List[str]]] = None
        self._all_source_files: List[str] = []
        self.prompt_config = self._load_asset_json(PROMPT_CONFIG_PATH)

    @classmethod
    async def create(cls, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient, force_remap: bool = False):
        """Asynchronous factory to create and initialize the client."""
        instance = cls(config, gcs_client, ai_client)
        await instance._initialize(force_remap=force_remap)
        return instance

    async def _initialize(self, force_remap: bool = False):
        """Initializes the client by ensuring the document map is ready."""
        logging.info("Initializing Document Finder (RagClient)...")
        self._all_source_files = [blob.name for blob in self.gcs_client.list_files()]
        await self._ensure_document_map_exists(force_remap=force_remap)

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f: return json.load(f)

    async def _create_document_map(self) -> None:
        """
        Uses an AI model to classify source documents into predefined BSI categories
        based on their filenames. Saves the result to a map file in GCS.
        The map stores the full GCS object path for each file.
        Falls back to classifying all documents as 'Sonstiges' on failure.
        """
        logging.info("Starting AI-driven document classification...")
        
        basename_to_fullpath_map = {name.split('/')[-1]: name for name in self._all_source_files}
        filenames = list(basename_to_fullpath_map.keys())

        if not filenames:
            logging.warning("No source files found to classify.")
            self.gcs_client.upload_from_string("{}", DOC_MAP_PATH)
            return

        etl_config = self.prompt_config["stages"]["ETL"]["classify_documents"]
        prompt_template = etl_config["prompt"]
        schema = self._load_asset_json(etl_config["schema_path"])
        
        filenames_json = json.dumps(filenames, indent=2)
        prompt = prompt_template.format(filenames_json=filenames_json)

        try:
            classification_result = await self.ai_client.generate_json_response(
                prompt,
                schema,
                request_context_log="Document Classification"
            )
            
            for item in classification_result.get("document_map", []):
                basename = item.get("filename")
                if basename in basename_to_fullpath_map:
                    item["filename"] = basename_to_fullpath_map[basename]
                else:
                    logging.warning(f"AI returned a filename '{basename}' not found in the source file list. It will be ignored.")

            content_to_upload = json.dumps(classification_result, indent=2, ensure_ascii=False)
            logging.info("Successfully created document map via AI with full file paths.")

        except Exception as e:
            logging.critical(
                f"AI-driven document classification failed: {e}. "
                f"Creating a fallback map with all documents as 'Sonstiges'. "
                "Document selection will be impaired.",
                exc_info=True
            )
            fallback_map = {"document_map": [{"filename": full_path, "category": "Sonstiges"} for full_path in self._all_source_files]}
            content_to_upload = json.dumps(fallback_map, indent=2, ensure_ascii=False)
        
        self.gcs_client.upload_from_string(
            content=content_to_upload,
            destination_blob_name=DOC_MAP_PATH
        )
        logging.info(f"Saved document map to '{DOC_MAP_PATH}'.")

    async def _ensure_document_map_exists(self, force_remap: bool = False) -> None:
        """
        Loads the document classification map from GCS. If it doesn't exist,
        or if `force_remap` is True, it triggers the creation process.
        """
        if force_remap or not self.gcs_client.blob_exists(DOC_MAP_PATH):
            if force_remap:
                logging.info("--force flag is set. Re-creating document classification map.")
            else:
                logging.warning(f"Document map not found at '{DOC_MAP_PATH}'. Triggering creation.")
            await self._create_document_map()
        else:
             logging.info(f"Using existing document map from '{DOC_MAP_PATH}'.")

        try:
            map_data = self.gcs_client.read_json(DOC_MAP_PATH)
        except NotFound:
            logging.critical(f"FATAL: Document map '{DOC_MAP_PATH}' could not be loaded, even after creation attempt. Cannot proceed.")
            raise
        
        category_map = {}
        doc_map_list = map_data.get("document_map", [])
        for item in doc_map_list:
            category = item.get("category")
            filename = item.get("filename")
            if category and filename:
                if category not in category_map:
                    category_map[category] = []
                category_map[category].append(filename)
        
        self._document_category_map = category_map
        logging.info(f"Successfully built document category map with {len(category_map)} categories.")

    def get_gcs_uris_for_categories(self, source_categories: List[str] = None) -> List[str]:
        """
        Finds the GCS URIs for documents belonging to the specified categories.

        Args:
            source_categories: A list of BSI categories (e.g., 'Strukturanalyse').
                               If None, all source document URIs are returned.

        Returns:
            A list of 'gs://...' URIs for the model to use as context.
        """
        if self._document_category_map is None:
            raise RuntimeError("Document map has not been initialized. Call `await RagClient.create()`.")
            
        selected_filenames = set()
        
        if source_categories:
            for category in source_categories:
                filenames = self._document_category_map.get(category, [])
                selected_filenames.update(filenames)
            if not selected_filenames:
                 logging.warning(f"No documents found for categories: {source_categories}. Returning all documents as a fallback.")
                 selected_filenames.update(self._all_source_files)
        else:
            selected_filenames.update(self._all_source_files)

        uris = [f"gs://{self.config.bucket_name}/{fname}" for fname in sorted(list(selected_filenames))]
        
#        if self.config.is_test_mode and len(uris) > MAX_FILES_TEST_MODE:
#            logging.warning(f"TEST MODE: Limiting context files from {len(uris)} to {MAX_FILES_TEST_MODE}.")
#            return uris[:MAX_FILES_TEST_MODE]
            
        return uris
==== bsi-audit-automator/src/config.py ====
# src/config.py
import os
from dataclasses import dataclass
from typing import Optional
from dotenv import load_dotenv

@dataclass(frozen=True)
class AppConfig:
    """
    Dataclass to hold all application configuration. It's frozen to prevent
    accidental modification after initialization.
    """
    gcp_project_id: str
    source_prefix: str
    output_prefix: str
    audit_type: str
    region: str
    doc_ai_processor_name: str
    max_concurrent_ai_requests: int
    is_test_mode: bool
    bucket_name: Optional[str] = None 

def load_config_from_env() -> AppConfig:
    """
    Loads configuration from environment variables, validates them,
    and returns a frozen AppConfig dataclass.

    Raises:
        ValueError: If a required environment variable is missing.

    Returns:
        AppConfig: The validated application configuration.
    """
    # Load .env file for local development. In a cloud environment, these
    # will be set directly.
    load_dotenv()

    required_vars = [
        "GCP_PROJECT_ID", "SOURCE_PREFIX", "OUTPUT_PREFIX", "AUDIT_TYPE", "REGION", "DOC_AI_PROCESSOR_NAME", "BUCKET_NAME"
    ]
    
    config_values = {}
    for var in required_vars:
        value = os.getenv(var)
        if not value:
            raise ValueError(f"Configuration Error: Missing required environment variable: {var}")
        # Convert to lowercase to match dataclass fields
        config_values[var.lower()] = value

    # Handle special case and boolean variables
    config_values["is_test_mode"] = os.getenv("TEST", "false").lower() == "true"
    
    # Load the new concurrency limit, defaulting to 5 if not set or invalid
    max_reqs_str = os.getenv("MAX_CONCURRENT_AI_REQUESTS", "5")
    config_values["max_concurrent_ai_requests"] = int(max_reqs_str) if max_reqs_str.isdigit() else 5

    return AppConfig(**config_values)

# Create a singleton instance to be imported by other modules.
# The try/except block ensures the application exits gracefully if config is invalid.
try:
    config = load_config_from_env()
except ValueError as e:
    print(e)
    exit(1)
==== bsi-audit-automator/src/logging_setup.py ====
# src/logging_setup.py
import logging
import sys
from src.config import AppConfig

def setup_logging(config: AppConfig):
    """
    Sets up the root logger based on the execution mode from the config.

    Args:
        config: The application configuration object.
    """
    # In test mode, we want detailed logs at INFO level.
    # In production, we want high-level INFO, with details at DEBUG.
    log_level = logging.INFO if config.is_test_mode else logging.DEBUG
    
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        stream=sys.stdout,
    )

    # In production, set the root logger to INFO to see high-level status,
    # while our application-specific logs can be at the DEBUG level.
    if not config.is_test_mode:
        logging.getLogger().setLevel(logging.INFO)

        # Suppress noisy third-party library logs for cleaner production output
        logging.getLogger("google.auth").setLevel(logging.WARNING)
        logging.getLogger("google.api_core").setLevel(logging.WARNING)
        logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)
        logging.info("Production logging enabled. Set root to INFO, app logs to DEBUG, and suppressed noisy libs.")
    else:
        logging.info("Test mode logging enabled. All INFO logs will be visible.")
==== bsi-audit-automator/src/main.py ====
# src/main.py
import argparse
import logging
import asyncio

from .config import config
from .logging_setup import setup_logging
from .clients.gcs_client import GcsClient
from .clients.rag_client import RagClient
from .clients.ai_client import AiClient
from .audit.controller import AuditController
from .audit.report_generator import ReportGenerator

async def main_async():
    """
    Asynchronous main function to handle all pipeline operations.
    """
    setup_logging(config)

    parser = argparse.ArgumentParser(
        description="BSI Grundschutz Audit Automation Pipeline."
    )
    group = parser.add_mutually_exclusive_group(required=True)
    
    group.add_argument(
        '--scan-previous-report',
        action='store_true',
        help='Run the stage to scan a previous audit report.'
    )
    group.add_argument(
        '--run-gs-check-extraction',
        action='store_true',
        help='Run only the Grundschutz-Check data extraction and mapping stage.'
    )
    group.add_argument(
        '--run-stage',
        type=str,
        help='Run a single audit stage (e.g., --run-stage Chapter-1).'
    )
    group.add_argument(
        '--run-all-stages',
        action='store_true',
        help='Run all audit generation stages sequentially.'
    )
    group.add_argument(
        '--generate-report',
        action='store_true',
        help='Assemble the final report from completed stage stubs.'
    )

    parser.add_argument(
        '--force',
        action='store_true',
        help='Force re-running of completed stages and re-classification of source documents.'
    )

    args = parser.parse_args()

    gcs_client = GcsClient(config)
    ai_client = AiClient(config)

    if args.generate_report:
        logging.info("Starting final report assembly...")
        generator = ReportGenerator(config, gcs_client)
        await generator.assemble_report()
        return
        
    # For all other tasks, we need the RagClient (Document Finder)
    logging.info("Initializing Document Finder Client...")
    try:
        rag_client = await RagClient.create(config, gcs_client, ai_client, force_remap=args.force)
    except Exception as e:
        logging.critical(f"Failed to initialize the Document Finder client. This can happen if no source documents are present. Error: {e}", exc_info=True)
        exit(1)

    controller = AuditController(config, gcs_client, ai_client, rag_client)

    if args.scan_previous_report:
        # For single stage runs, we always overwrite the stage result.
        await controller.run_single_stage("Scan-Report", force_overwrite=True)
    elif args.run_gs_check_extraction:
        # For single stage runs, we always overwrite the stage result.
        await controller.run_single_stage("Grundschutz-Check-Extraction", force_overwrite=True)
    elif args.run_stage:
        # The --force flag will have already re-mapped the documents if it was passed.
        await controller.run_single_stage(args.run_stage, force_overwrite=True)
    elif args.run_all_stages:
        await controller.run_all_stages(force_overwrite=args.force)


def main():
    """
    Main entry point for the BSI Audit Automator.
    Parses command-line arguments and runs the appropriate async task.
    """
    try:
        asyncio.run(main_async())
        logging.info("Pipeline step completed successfully.")
    except Exception as e:
        logging.critical(f"A critical error occurred in the pipeline: {e}", exc_info=True)
        exit(1)

if __name__ == "__main__":
    main()
