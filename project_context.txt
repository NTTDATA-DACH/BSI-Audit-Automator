==== bsi-audit-automator/.dockerignore ====
# Git / version control
.git
.gitignore

# Docker
Dockerfile
.dockerignore

# Python cache and virtual environment
__pycache__/
*.pyc
.venv
venv/

# Environment variables - should be passed at runtime, not baked into the image
.env

# IDE / Editor specific
.vscode/
.idea/
==== bsi-audit-automator/Dockerfile ====
# Stage 1: Use an official Python runtime as a parent image
# Using a "slim" image to keep the final image size down.
FROM python:3.11-slim-bookworm

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1

# Set the working directory in the container
WORKDIR /app

# Add the non-root user's local bin to the PATH.
# This prevents warnings during pip install.
ENV PATH="/app/.local/bin:${PATH}"

# It's good practice to upgrade pip to the latest version
RUN pip install --upgrade pip

# Create a non-root user and group to run the application
# This avoids the "Running pip as root" warning and is a security best practice.
RUN groupadd -r appgroup && useradd -r -g appgroup -d /app -s /sbin/nologin -c "Docker image user" appuser
RUN chown -R appuser:appgroup /app

# Copy the requirements file into the container
# This is done before copying the rest of the code to leverage Docker's layer caching.
COPY requirements.txt .

# Switch to the non-root user before installing dependencies
USER appuser

# Install any needed packages specified in requirements.txt
# --no-cache-dir reduces image size by not storing the download cache.
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application's source code into the container
COPY . .

# Specify the command to run on container start.
ENTRYPOINT ["python", "-m", "src.main"]

# Set a default command to show the help message if no other command is provided.
CMD ["--help"]
==== bsi-audit-automator/assets/json/BSI_GS_OSCAL_current_2023_benutzerdefinierte.json ====
{
  "catalog": {
    "uuid": "d186e90d-ecbd-4d07-8311-4603deb60225",
    "metadata": {
      "title": "Gesamtkatalog BSI Grundschutz Kompendium 2023",
      "last-modified": "2025-06-27T17:42:20.097143+00:00",
      "version": "1.0.0",
      "oscal-version": "1.1.2"
    },
    "groups": [
      {
        "id": "ISMS",
        "title": "ISMS: Sicherheitsmanagement",
        "class": "main-group",
        "groups": [
          {
            "id": "ISMS.1",
            "title": "Sicherheitsmanagement",
            "class": "baustein",
            "controls": [
              {
                "id": "ISMS.1.A1",
                "title": "Übernahme der Gesamtverantwortung für Informationssicherheit durch die Leitung (B)",
                "class": "Management",
                "props": [
                  {
                    "name": "level",
                    "value": "1",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "phase",
                    "value": "Initiation",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "practice",
                    "value": "GOV",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_c",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_i",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_a",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  }
                ],
                "parts": [
                  {
                    "id": "isms.1.a1-m1",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 1: Partial",
                    "class": "maturity-level-partial",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Institutionsleitung hat die Verantwortung für Informationssicherheit nur mündlich oder informell anerkannt. Die Übernahme der Verantwortung ist nicht dokumentiert und wird nicht aktiv kommuniziert, was zu Unklarheiten bei den Mitarbeitenden führt."
                      },
                      {
                        "name": "guidance",
                        "prose": "In dieser Stufe wird die Verantwortung oft nur reaktiv im Falle eines Sicherheitsvorfalls oder einer externen Prüfung erwähnt. Es gibt keine formalen Dokumente wie eine Ernennungsurkunde für den ISB oder eine offizielle Leitlinie."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer befragt Führungskräfte und stellt fest, ob sie ihre Verantwortung für die Informationssicherheit verstehen und wahrnehmen. Es wird geprüft, ob eine formale Zuweisung der Gesamtverantwortung fehlt."
                      }
                    ]
                  },
                  {
                    "id": "isms.1.a1-m2",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 2: Foundational",
                    "class": "maturity-level-foundational",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Gesamtverantwortung für die Informationssicherheit ist formal der Institutionsleitung zugewiesen, z. B. in einer Geschäftsordnung. Die Leitung nimmt diese Verantwortung jedoch nur passiv wahr und delegiert die Aufgaben vollständig ohne aktive Steuerung oder Kontrolle."
                      },
                      {
                        "name": "guidance",
                        "prose": "Die Zuweisung der Verantwortung ist in einem offiziellen Dokument festgehalten. Die Leitung genehmigt Budgets, greift aber nicht proaktiv in die Gestaltung des Sicherheitsprozesses ein und informiert sich nur unregelmäßig über den Status."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer sichtet Dokumente wie die Geschäftsordnung, um die formale Zuweisung der Verantwortung zu verifizieren. Es wird durch Interviews geprüft, inwieweit die Leitung den Sicherheitsprozess aktiv steuert und kontrolliert."
                      }
                    ]
                  },
                  {
                    "id": "isms.1.a1-m3",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 3: Defined",
                    "class": "maturity-level-defined",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Institutionsleitung MUSS die Gesamtverantwortung für Informationssicherheit in der Institution übernehmen. Dies MUSS für alle Beteiligten deutlich erkennbar sein. Die Institutionsleitung MUSS den Sicherheitsprozess initiieren, steuern und kontrollieren. Die Institutionsleitung MUSS die Zuständigkeiten für Informationssicherheit festlegen und die zuständigen Mitarbeitenden mit den erforderlichen Kompetenzen und Ressourcen ausstatten. Die Institutionsleitung MUSS sich regelmäßig über den Status der Informationssicherheit sowie über mögliche Risiken und Konsequenzen aufgrund fehlender Sicherheitsmaßnahmen informieren lassen."
                      },
                      {
                        "name": "guidance",
                        "prose": "Die Übernahme der Verantwortung wird durch eine von der Leitung unterzeichnete Sicherheitsleitlinie dokumentiert. Regelmäßige Meetings (z. B. quartalsweise) zwischen der Leitung und dem ISB werden etabliert, um den Status und die Risiken zu besprechen. Die Ergebnisse dieser Meetings werden protokolliert."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer prüft die unterzeichnete Sicherheitsleitlinie. Es werden Protokolle der Management-Meetings zur Informationssicherheit eingesehen. Interviews mit Mitarbeitenden bestätigen, dass die Rolle der Leitung im Sicherheitsprozess klar und bekannt ist."
                      }
                    ]
                  },
                  {
                    "id": "isms.1.a1-m4",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 4: Enhanced",
                    "class": "maturity-level-enhanced",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Institutionsleitung lebt die Informationssicherheit aktiv vor ('Tone at the Top'). Sie hinterfragt die erhaltenen Statusberichte kritisch, fordert tiefere Analysen bei Abweichungen und verfolgt die Umsetzung von Maßnahmen zur Risikobehandlung konsequent nach. Die Einhaltung der Sicherheitspolicies wird auch auf Leitungsebene demonstriert."
                      },
                      {
                        "name": "guidance",
                        "prose": "Die Leitung nimmt aktiv an wichtigen Sensibilisierungsmaßnahmen teil. Entscheidungen zur Risikobehandlung werden nicht nur dokumentiert, sondern auch mit einer klaren Begründung für die gewählte Option (z. B. Akzeptanz, Minderung) versehen. Ein dediziertes Risikokomitee unter Vorsitz eines Mitglieds der Institutionsleitung wird eingerichtet."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer analysiert die Protokolle des Risikokomitees und die dokumentierten Risikoentscheidungen auf ihre Nachvollziehbarkeit. Es wird geprüft, ob die Leitung bei der Zuweisung von Ressourcen für Sicherheitsmaßnahmen eine klare Priorisierung basierend auf dem Geschäftsrisiko vornimmt."
                      }
                    ]
                  },
                  {
                    "id": "isms.1.a1-m5",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 5: Comprehensive",
                    "class": "maturity-level-comprehensive",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Informationssicherheitsziele sind vollständig in die übergeordneten Geschäftsziele und die strategische Planung der Institution integriert. Die Leistung im Bereich der Informationssicherheit ist Teil der Zielvereinbarungen und Leistungsbeurteilungen für das obere Management. Die Leitung fördert aktiv eine positive Sicherheitskultur und investiert proaktiv in zukünftige Sicherheitstechnologien."
                      },
                      {
                        "name": "guidance",
                        "prose": "In den jährlichen Geschäftsberichten wird über die Informationssicherheitslage berichtet. Die Zielerreichung wird anhand von Key Performance Indicators (KPIs) gemessen, die in einem Management-Dashboard visualisiert werden. Die Leitung initiiert und finanziert Forschungsprojekte zur Abwehr zukünftiger Bedrohungen."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer überprüft die strategischen Planungsdokumente und die Zielvereinbarungen des Managements auf die Integration von Sicherheitszielen. Es wird die Existenz und Nutzung eines KPI-Dashboards verifiziert. Die Wirksamkeit der Sicherheitskultur wird durch Mitarbeiterbefragungen und die Analyse von Sicherheitsvorfällen bewertet."
                      }
                    ]
                  }
                ]
              },
              {
                "id": "ISMS.1.A2",
                "title": "Festlegung der Sicherheitsziele und -strategie (B)",
                "class": "Management",
                "props": [
                  {
                    "name": "level",
                    "value": "1",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "phase",
                    "value": "Initiation",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "practice",
                    "value": "GOV",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_c",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_i",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_a",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  }
                ],
                "parts": [
                  {
                    "id": "isms.1.a2-m1",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 1: Partial",

[... File truncated. Only first 200 of 204472 lines included. ...]

==== bsi-audit-automator/assets/json/master_report_template.json ====
{
  "bsiAuditReport": {
    "titlePage": {
      "reportTitle": "Auditbericht im Rahmen der Zertifizierung nach ISO 27001 auf der Basis von IT-Grundschutz",
      "auditedInstitution": "",
      "certificationID": ""
    },
    "allgemeines": {
      "chapterNumber": "1.",
      "title": "Allgemeines",
      "versionshistorie": {
        "title": "Versionshistorie",
        "subchapterNumber": "1.1",
        "table": {
          "headers": ["Datum", "Version", "Verfasser", "Bemerkungen"],
          "rows": []
        }
      },
      "auditierteInstitution": {
        "title": "Auditierte Institution",
        "subchapterNumber": "1.2",
        "kontaktinformationenAntragsteller": {
          "title": "Kontaktinformationen des Antragstellers (auditierte Institution):",
          "table": {
              "headers": ["Institution", "Straße / Hausnummer", "PLZ / Ort", "E-Mail"],
              "rows": []
          }
      },
      "ansprechpartnerZertifizierung": {
          "title": "Ansprechpartner für die Zertifizierung beim Antragsteller:",
          "table": {
              "headers": ["Name", "Funktion", "Telefon", "E-Mail", "Optional: Abweichende Anschrift"],
              "rows": []
          }
        }
      },
      "auditteam": {
        "title": "Auditteam",
        "subchapterNumber": "1.3",
        "auditteamleiter": {
            "title": "Auditteamleiter",
            "table": {
                "headers": ["Name", "Institution", "Zertifizierungsnummer", "Gültigkeit Zertifikat", "Straße / Hausnummer", "PLZ / Ort", "Telefon", "E-Mail", "Datum der Freigabe der Unabhängigkeitserklärung"],
                "rows": []
            }
        },
        "auditor": {
            "title": "Auditor",
            "table": {
                "headers": ["Name", "Institution", "Zertifizierungsnummer", "Gültigkeit Zertifikat", "Straße / Hausnummer", "PLZ / Ort", "Telefon", "E-Mail", "Datum der Freigabe der Unabhängigkeitserklärung"],
                "rows": []
            }
        },
        "fachexperte": {
            "title": "Fachexperte",
            "table": {
                "headers": ["Name", "Institution", "Zertifizierungsnummer", "Gültigkeit Zertifikat", "Straße / Hausnummer", "PLZ / Ort", "Telefon", "E-Mail", "Datum der Freigabe der Unabhängigkeitserklärung"],
                "rows": []
            }
        },
        "berater": {
            "title": "Berater",
            "table": {
                "headers": ["Name", "Institution", "Zertifizierungsnummer (falls vorhanden)"],
                "rows": []
            }
        }
      },
      "informationsverbund": {
        "title": "Informationsverbund",
        "subchapterNumber": "1.4",
        "description": "Die nachfolgenden Informationen können dem Zertifizierungsantrag oder dem gültigen Zertifikat entnommen werden.",
        "content": [
          {
            "type": "prose",
            "label": "Kurzbezeichnung:",
            "text": ""
          },
          {
            "type": "prose",
            "label": "Kurzbeschreibung (entspricht Geltungsbereich):",
            "text": ""
          }
        ]
      },
      "audittyp": {
        "title": "Audittyp",
        "subchapterNumber": "1.5",
        "content": "Zertifizierungsaudit"
      },
      "auditprojektierung": {
        "title": "Auditprojektierung",
        "subchapterNumber": "1.6",
        "table": {
            "headers": ["Audit-Phasen", "Datum / Zeitraum", "Aufwand (in PT)"],
            "rows": [
                {"Audit-Phasen": "Voraudit (nur bei Erstzertifizierung)"},
                {"Audit-Phasen": "Übergabe der Referenzdokumente"},
                {"Audit-Phasen": "Sichtung der Referenzdokumente"},
                {"Audit-Phasen": "Vor-Ort-Audit"},
                {"Audit-Phasen": "Prüfung der Nachbesserungen"},
                {"Audit-Phasen": "Erstellung des Auditberichts"},
                {"Audit-Phasen": "Nachforderungen der Zertifizierungsstelle"},
                {"Audit-Phasen": "Bearbeitung der Nachforderungen und ggf. Nachbesserungen durch den Antragssteller"},
                {"Audit-Phasen": "Abschluss der Auditierung"}
            ]
        },
        "content": [
            { "type": "prose", "label": "Anmerkungen zur Auditprojektierung:", "text": ""}
        ]
      },
      "absprachen": {
        "title": "Absprachen",
        "subchapterNumber": "1.7",
        "table": {
            "headers": ["Absprache zu", "Angabe von Datum, Beteiligten, Art usw."],
            "rows": []
        }
      },
      "formaleGrundlagen": {
          "title": "Formale Grundlagen der Auditierung",
          "subchapterNumber": "1.8",
          "table": {
              "headers": ["Bezeichnung", "Version / Edition, Datum"],
              "rows": [
                  {"Bezeichnung": "Zertifizierung nach ISO 27001 auf der Basis von IT-Grundschutz - Zertifizierungsschema"},
                  {"Bezeichnung": "Zertifizierung nach ISO 27001 auf der Basis von IT-Grundschutz – Auditierungsschema"},
                  {"Bezeichnung": "BSI-Standard 200-2 – IT-Grundschutz-Methodik"},
                  {"Bezeichnung": "IT-Grundschutz-Kompendium"},
                  {"Bezeichnung": "Vorlage Auditbericht"},
                  {"Bezeichnung": "Grundlage für die Risikoanalyse"}
              ]
          }
      },
      "inhaltlicheGrundlagen": {
          "title": "Inhaltliche Grundlagen",
          "subchapterNumber": "1.9",
          "content": [
              {"type": "prose", "label": "Verweis der Liste für Referenzdokumente:", "text": "Siehe Anhang 7.1"}
          ]
      },
      "toolbasierteUnterstuetzung": {
          "title": "Toolbasierte Audit-Unterstützung",
          "subchapterNumber": "1.10",
          "content": [
              {"type": "prose", "label": "Tool:", "text": ""},
              {"type": "prose", "label": "Version:", "text": ""},
              {"type": "prose", "label": "Stand des IT-Grundschutz-Kompendiums:", "text": ""}
          ]
      }
    },
    "voraudit": {
      "chapterNumber": "2.",
      "title": "Voraudit",
      "content": [
          {"type": "question", "questionText": "Wurde ein Voraudit durchgeführt?", "answer": null},
          {"type": "prose", "label": "Prüfumfang des Voraudits:", "text": ""},
          {"type": "prose", "label": "Feststellung:", "text": ""}
      ]
    },
    "dokumentenpruefung": {
      "chapterNumber": "3.",
      "title": "Dokumentenprüfung",
      "aktualitaetDerReferenzdokumente": {
        "title": "Aktualität der Referenzdokumente",
        "subchapterNumber": "3.1",
        "content": [
          {"type": "question", "questionText": "Wurden alle Referenzdokumente A.0 gemäß den Vorgaben von A.0.3 Lenkung von Dokumenten überarbeitet?", "answer": null},
          {"type": "question", "questionText": "Wurden alle Dateien zu den Referenzdokumenten A.1, A.2, A.3, A.5 und A.6 für das Audit neu erstellt?", "answer": null},
          {"type": "question", "questionText": "Wurden alle Maßnahmen im A.4 IT-Grundschutz-Check innerhalb der letzten 12 Monate neu bewertet?", "answer": null},
          {"type": "question", "questionText": "Datum der letzten inhaltlichen Änderung im Referenzdokument A.4 IT-Grundschutz-Check.", "answer": ""},
          {"type": "finding", "label": "Feststellung:", "findingText": ""}
        ]
      },
      "sicherheitsleitlinieUndRichtlinienInA0": {
        "title": "Sicherheitsleitlinie und -richtlinien in A.0",
        "subchapterNumber": "3.2",
        "content": [
          {"type": "question", "questionText": "Ist die Leitlinie zur Informationssicherheit (A.0.1) sinnvoll und angemessen für den Antragsteller? Erfüllt die Leitlinie zur Informationssicherheit alle Aspekte gemäß den Anforderungen aus dem Baustein ISMS.1?", "answer": null},
          {"type": "question", "questionText": "Decken sich die Sicherheitsziele aus der Leitlinie mit den Sicherheitsanforderungen der restlichen Referenzdokumente (A.0.2 bis A.0.5)?", "answer": null},
          {"type": "question", "questionText": "Werden die Sicherheitsrichtlinien (A.0.1 bis A.0.5) durch das Management getragen und wurden sie veröffentlicht?", "answer": null},
          {"type": "finding", "label": "Feststellung:", "findingText": ""}
        ]
      },
      "strukturanalyseA1": {
        "title": "Strukturanalyse A.1",
        "subchapterNumber": "3.3",
        "definitionDesInformationsverbundes": {
            "title": "Definition des Informationsverbundes",
            "subchapterNumber": "3.3.1",
            "content": [
                {"type": "question", "questionText": "Ist der Informationsverbund eindeutig abgegrenzt?", "answer": null},
                {"type": "question", "questionText": "Sind alle infrastrukturellen, organisatorischen, personellen und technischen Komponenten im Informationsverbund enthalten, die zur Aufgabenerfüllung notwendig sind?", "answer": null},
                {"type": "question", "questionText": "Sind die Schnittstellen zu allen weiteren Prozessen definiert?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "bereinigterNetzplan": {
            "title": "Bereinigter Netzplan",
            "subchapterNumber": "3.3.2",
            "content": [
                {"type": "question", "questionText": "Optional: Liegt ein aktueller und vollständiger bereinigter Netzplan vor?", "answer": null},
                {"type": "question", "questionText": "Sind alle Komponenten im Netzplan mit den korrekten Bezeichnern versehen?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "listeDerGeschaeftsprozesse": {
            "title": "Liste der Geschäftsprozesse",
            "subchapterNumber": "3.3.3",
            "content": [
                {"type": "question", "questionText": "Enthält die Liste der Geschäftsprozesse alle benötigten Informationen (eindeutige Bezeichnung, Name, Prozessverantwortlicher, kurze Beschreibung, benötigte Anwendungen)?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "listeDerAnwendungen": {
            "title": "Liste der Anwendungen",
            "subchapterNumber": "3.3.4",
            "content": [
                {"type": "question", "questionText": "Enthält die Liste der Anwendungen alle benötigten Informationen?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "listeDerItSysteme": {
            "title": "Liste der IT-Systeme",
            "subchapterNumber": "3.3.5",
            "content": [
                {"type": "question", "questionText": "Enthält die Liste der IT-Systeme alle benötigten Informationen?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "listeDerRaeumeGebaeudeStandorte": {
            "title": "Liste der Räume, Gebäude und Standorte",
            "subchapterNumber": "3.3.6",
            "content": [
                {"type": "question", "questionText": "Enthält die Liste der Räume, Gebäude und Standorte alle benötigten Informationen?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "listeDerKommunikationsverbindungen": {
            "title": "Liste der Kommunikationsverbindungen",
            "subchapterNumber": "3.3.7",
            "content": [
                {"type": "question", "questionText": "Enthält die Liste der Kommunikationsverbindungen alle benötigten Informationen?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "stichprobenDokuStrukturanalyse": {
            "title": "Stichprobendokumentation der Strukturanalyse",
            "subchapterNumber": "3.3.8",
            "content": [
              {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "listeDerDienstleister": {
            "title": "Liste der Dienstleister",
            "subchapterNumber": "3.3.9",
            "content": [
                {"type": "question", "questionText": "Liegt eine aktuelle und vollständige Liste aller externen Dienstleister vor?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "ergebnisDerStrukturanalyse": {
            "title": "Ergebnis der Strukturanalyse",
            "subchapterNumber": "3.3.10",
            "content": [
                {"type": "prose", "label": "Votum Strukturanalyse:", "text": ""}
            ]
        }
      },
      "schutzbedarfsfeststellungA2": {
        "title": "Schutzbedarfsfeststellung A.2",
        "subchapterNumber": "3.4",
        "definitionDerSchutzbedarfskategorien": {
          "title": "Definition der Schutzbedarfskategorien",
          "subchapterNumber": "3.4.1",
          "content": [
            {"type": "question", "questionText": "Ist die Definition der Schutzbedarfskategorien plausibel und für den Informationsverbund angemessen?", "answer": null},
            {"type": "question", "questionText": "Wurden mehr als drei Schutzbedarfskategorien definiert?", "answer": null},
            {"type": "finding", "label": "Feststellung:", "findingText": ""}
          ]
        },
        "schutzbedarfGeschaeftsprozesse": {
            "title": "Schutzbedarf der Geschäftsprozesse",
            "subchapterNumber": "3.4.2",
            "content": [
                {"type": "question", "questionText": "Ist der Schutzbedarf der Geschäftsprozesse vollständig dokumentiert?", "answer": null},
                {"type": "question", "questionText": "Ist der Schutzbedarf der Geschäftsprozesse nachvollziehbar begründet?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "schutzbedarfAnwendungen": {
            "title": "Schutzbedarf der Anwendungen",
            "subchapterNumber": "3.4.3",
            "content": [
                {"type": "question", "questionText": "Ist der Schutzbedarf der Anwendungen vollständig dokumentiert?", "answer": null},
                {"type": "question", "questionText": "Ist der Schutzbedarf der Anwendungen nachvollziehbar begründet?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "schutzbedarfItSysteme": {
            "title": "Schutzbedarf der IT-Systeme",
            "subchapterNumber": "3.4.4",
            "content": [
                {"type": "question", "questionText": "Ist der Schutzbedarf der IT-Systeme vollständig dokumentiert?", "answer": null},
                {"type": "question", "questionText": "Ist der Schutzbedarf der IT-Systeme nachvollziehbar begründet?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "schutzbedarfRaeume": {
            "title": "Schutzbedarf der Räume, Gebäude und Standorte",
            "subchapterNumber": "3.4.5",
            "content": [
                {"type": "question", "questionText": "Ist der Schutzbedarf der Räume, Gebäude und Standorte vollständig dokumentiert?", "answer": null},
                {"type": "question", "questionText": "Ist der Schutzbedarf der Räume, Gebäude und Standorte nachvollziehbar begründet?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "schutzbedarfKommunikationsverbindungen": {
            "title": "Schutzbedarf der Kommunikationsverbindungen",
            "subchapterNumber": "3.4.6",
            "content": [
                {"type": "question", "questionText": "Ist der Schutzbedarf der Außenverbindungen und kritischen Kommunikationsverbindungen vollständig dokumentiert?", "answer": null},
                {"type": "question", "questionText": "Ist der Schutzbedarf der Kommunikationsverbindungen nachvollziehbar begründet?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "stichprobenDokuSchutzbedarf": {
            "title": "Stichprobendokumentation der Schutzbedarfsfeststellung",
            "subchapterNumber": "3.4.7",
            "content": [
              {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "ergebnisDerSchutzbedarfsfeststellung": {
            "title": "Ergebnis der Schutzbedarfsfeststellung",
            "subchapterNumber": "3.4.8",
            "content": [
                {"type": "prose", "label": "Votum Schutzbedarfsfeststellung:", "text": ""}
            ]
        }
      },
      "modellierungDesInformationsverbundesA3": {
        "title": "Modellierung des Informationsverbundes A.3",
        "subchapterNumber": "3.5",
        "modellierungsdetails": {
          "title": "Modellierungsdetails",
          "subchapterNumber": "3.5.1",
          "content": [
            {"type": "question", "questionText": "Ist jeder Baustein des IT-Grundschutz-Kompendiums auf alle relevanten Zielobjekte angewandt?", "answer": null},
            {"type": "question", "questionText": "Ist für jeden Baustein des IT-Grundschutz-Kompendiums, der nicht angewandt wurde, eine plausible Begründung vorhanden?", "answer": null},
            {"type": "question", "questionText": "Wurden alle Zielobjekte angemessen berücksichtigt, für die keine Bausteine des IT-Grundschutz-Kompendiums vorhanden sind?", "answer": null},
            {"type": "question", "questionText": "Gibt es benutzerdefinierte Bausteine?", "answer": null},
            {"type": "finding", "label": "Feststellung:", "findingText": ""}
          ]
        },
        "ergebnisDerModellierung": {
          "title": "Ergebnis der Modellierung",
          "subchapterNumber": "3.5.2",
          "content": [
            {"type": "prose", "label": "Votum Modellierung:", "text": ""}
          ]
        }
      },
      "itGrundschutzCheckA4": {
        "title": "IT-Grundschutz-Check A.4",
        "subchapterNumber": "3.6",
        "detailsZumItGrundschutzCheck": {
          "title": "Details zum IT-Grundschutz-Check",
          "subchapterNumber": "3.6.1",
          "content": [
            {"type": "question", "questionText": "Wurde zu jeder Anforderung der Umsetzungsstatus erhoben?", "answer": null},
            {"type": "question", "questionText": "Wurden alle Anforderungen mit Umsetzungsstatus „entbehrlich“ plausibel begründet?", "answer": null},
            {"type": "question", "questionText": "Sind alle MUSS-Teilanforderungen erfüllt?", "answer": null},
            {"type": "question", "questionText": "Wurden die nicht oder nur teilweise umgesetzten Anforderungen im Referenzdokument A.6 dokumentiert?", "answer": null},
            {"type": "question", "questionText": "Sind alle Anforderungen innerhalb der letzten 12 Monate überprüft worden?", "answer": null},
            {"type": "finding", "label": "Feststellung:", "findingText": ""}
          ]
        },
        "benutzerdefinierteBausteine": {
            "title": "Benutzerdefinierte Bausteine",
            "subchapterNumber": "3.6.2",
            "content": [
                {"type": "question", "questionText": "Wurden benutzerdefinierte Bausteine erstellt und modelliert?", "answer": null},
                {"type": "question", "questionText": "Sind alle Anforderungen der Institution in den benutzerdefinierten Bausteinen enthalten?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "ergebnisItGrundschutzCheck": {
            "title": "Ergebnis IT-Grundschutz-Check",
            "subchapterNumber": "3.6.3",
            "content": [
                {"type": "prose", "label": "Votum IT-Grundschutz-Check:", "text": ""}
            ]
        }
      },
      "risikoanalyseA5": {
          "title": "Risikoanalyse A.5",
          "subchapterNumber": "3.7",
          "risikoanalyse": {
              "title": "Ergebnis Risikoanalyse",
              "subchapterNumber": "3.7.1",
              "content": [
                  {"type": "question", "questionText": "Wurde für alle Zielobjekte, deren Schutzbedarf über „normal“ liegt, eine Risikoanalyse durchgeführt?", "answer": null},
                  {"type": "question", "questionText": "Ist die Risikoanalyse aussagekräftig und nachvollziehbar?", "answer": null},
                  {"type": "question", "questionText": "Ist das vom Management getragene Restrisiko für den Informationsverbund angemessen?", "answer": null},
                  {"type": "question", "questionText": "Wurde der Management-Report zur Risikoanalyse von der Institutionsleitung unterschrieben?", "answer": null},
                  {"type": "finding", "label": "Feststellung:", "findingText": ""}
              ]
          }
      },
      "realisierungsplanA6": {
          "title": "Realisierungsplan A.6",
          "subchapterNumber": "3.8",
          "realisierungsplan": {
              "title": "Ergebnis Realisierungsplan",
              "subchapterNumber": "3.8.1",
              "content": [
                  {"type": "question", "questionText": "Liegt ein Realisierungsplan A.6 vor?", "answer": null},
                  {"type": "question", "questionText": "Werden die bestehenden Risiken nachvollziehbar dokumentiert?", "answer": null},
                  {"type": "question", "questionText": "Besteht ein Umsetzungsplan für die Reduzierung des Restrisikos?", "answer": null},
                  {"type": "finding", "label": "Feststellung:", "findingText": ""}
              ]
          }
      },
      "ergebnisDerDokumentenpruefung": {
        "title": "Ergebnis der Dokumentenprüfung",
        "subchapterNumber": "3.9",
        "content": [
          {
            "type": "question",
            "questionText": "Ist eine Fortführung des Audits mit der Vor-Ort-Prüfung möglich?",
            "answer": null
          },
          {
            "type": "prose",
            "label": "Votum Dokumentenprüfung:",
            "text": ""
          }
        ]
      }
    },
    "erstellungEinesPruefplans": {
      "chapterNumber": "4.",
      "title": "Erstellung eines Prüfplans",
      "auditplanung": {
        "title": "Auditplanung",
        "subchapterNumber": "4.1",
        "auswahlBausteineErstRezertifizierung": {
          "title": "Auswahl Bausteine Erst-/Rezertifizierungsverfahren",
          "subchapterNumber": "4.1.1",
          "table": {
            "headers": ["Schicht", "Baustein", "Zielobjekt-Name", "Zielobjekt-Kürzel", "Begründung zur Auswahl"],
            "rows": []
          }
        },
        "auswahlBausteine1Ueberwachungsaudit": {
            "title": "Auswahl Bausteine 1. Überwachungsaudit",
            "subchapterNumber": "4.1.2",
            "table": {
              "headers": ["Schicht", "Baustein", "Zielobjekt-Name", "Zielobjekt-Kürzel", "Begründung zur Auswahl"],
              "rows": []
            }
        },
        "auswahlBausteine2Ueberwachungsaudit": {
            "title": "Auswahl Bausteine 2. Überwachungsaudit",
            "subchapterNumber": "4.1.3",
            "table": {
              "headers": ["Schicht", "Baustein", "Zielobjekt-Name", "Zielobjekt-Kürzel", "Begründung zur Auswahl"],
              "rows": []
            }
        },
        "auswahlStandorte": {
            "title": "Auswahl Standorte",
            "subchapterNumber": "4.1.4",
            "table": {
                "headers": ["Standort", "Erst- bzw. Rezertifizierung", "1. Überwachungsaudit", "2. Überwachungsaudit", "Begründung für die Auswahl"],
                "rows": []
            }
        },
        "auswahlMassnahmenAusRisikoanalyse": {
            "title": "Auswahl Maßnahmen aus der Risikoanalyse",
            "subchapterNumber": "4.1.5",
            "table": {
              "headers": ["Maßnahme", "Risikoanalyse", "Zielobjekt", "Begründung zur Auswahl"],
              "rows": []
            }
        }
      }
    },
    "vorOrtAudit": {
      "chapterNumber": "5.",
      "title": "Vor-Ort-Audit",
      "wirksamkeitSicherheitsmanagementsystem": {
          "title": "Wirksamkeit des Sicherheitsmanagementsystems",
          "subchapterNumber": "5.1",
          "content": [
              {"type": "question", "questionText": "Ist das ISMS effektiv und effizient im Einsatz? (Interview, Gesamteindruck)", "answer": null},
              {"type": "question", "questionText": "Ist der Sicherheitsprozess konform zum BSI-Standard 200-2, Kapitel 4 organisiert?", "answer": null},
              {"type": "question", "questionText": "Werden die in den Sicherheitsleitlinien vorgegebenen Ziele erreicht?", "answer": null},
              {"type": "question", "questionText": "Werden alle wichtigen Prozesse des Informationsverbundes dokumentiert?", "answer": null},
              {"type": "question", "questionText": "Ist der Sicherheitsprozess konform zum BSI-Standard 200-2, Kapitel 5 dokumentiert?", "answer": null},
              {"type": "question", "questionText": "Sind die Informationen konform zum BSI-Standard 200-2, Kapitel 5.1 klassifiziert?", "answer": null},
              {"type": "question", "questionText": "Wird der Verbesserungsprozess gelebt und das ISMS kontinuierlich verbessert?", "answer": null},
              {"type": "question", "questionText": "Erfolgt die Überprüfung des Informationssicherheitsprozesses anhand von Kennzahlen?", "answer": null},
              {"type": "question", "questionText": "Erfolgt die Bewertung des ISMS mit Hilfe eines Reifegradmodells?", "answer": null},
              {"type": "question", "questionText": "Wird die Umsetzung von Maßnahmen des Realisierungsplans überprüft?", "answer": null},
              {"type": "question", "questionText": "Erfolgt die Übernahme der Gesamtverantwortung durch eine Managementbewertung?", "answer": null},
              {"type": "prose", "label": "Datum der letzten Managementbewertung:", "text": ""},
              {"type": "finding", "label": "Feststellung:", "findingText": ""}
          ]
      },
      "aenderungenAmInformationsverbund": {
          "title": "Änderungen am Informationsverbund",
          "subchapterNumber": "5.2",
          "content": [
              {"type": "question", "questionText": "Liegen Änderungen am Informationsverbund vor?", "answer": null},
              {"type": "question", "questionText": "Sind alle Änderungen am Informationsverbund der Zertifizierungsstelle des BSI mitgeteilt wurden?", "answer": null},
              {"type": "question", "questionText": "Erfordern die Änderungen am Informationsverbund eine Rezertifizierung?", "answer": null},
              {"type": "question", "questionText": "Sind die Änderungen in der Dokumentation des Sicherheitskonzeptes kontinuierlich eingeflossen?", "answer": null},
              {"type": "question", "questionText": "Sind die dokumentierten Änderungen gemäß IT-Grundschutz-Methodik (BSI-Standard 200-2) und des IT-Grundschutz-Kompendiums umgesetzt?", "answer": null},
              {"type": "question", "questionText": "Wird durch den Wegfall / die Hinzunahme von Komponenten die Sicherheit des Informationsverbundes beeinträchtigt?", "answer": null},
              {"type": "finding", "label": "Feststellung:", "findingText": ""}
          ]
      },
      "behebungAbweichungen": {
          "title": "Behebung der Abweichungen und Empfehlungen",
          "subchapterNumber": "5.3",
          "content": [
              {"type": "question", "questionText": "Gab es im vorhergehenden Auditbericht Abweichungen und Empfehlungen?", "answer": null},
              {"type": "question", "questionText": "Sind alle Abweichungen fristgerecht behoben worden? Die Dokumentation hierzu befindet sich in Kapitel 7.2 'Abweichungen und Empfehlungen'.", "answer": null},
              {"type": "question", "questionText": "Wurden alle Empfehlungen angemessen berücksichtigt? Die Dokumentation hierzu befindet sich in Kapitel 7.2 'Abweichungen und Empfehlungen'.", "answer": null},
              {"type": "finding", "label": "Feststellung:", "findingText": ""}
          ]
      },
      "einhaltungAuflagen": {
          "title": "Einhaltung der Auflagen",
          "subchapterNumber": "5.4",
          "content": [
              {"type": "question", "questionText": "Gab es im vorhergehenden Audit Auflagen?", "answer": null}
          ],
          "table": {
              "headers": ["Nummer", "Beschreibung der Auflage", "Auflage erteilt am"],
              "rows": []
          },
          "bearbeitungAuflagen": {
              "content": [
                  {"type": "question", "questionText": "Wurden die Auflagen fristgerecht bearbeitet?", "answer": null},
                  {"type": "question", "questionText": "Wurden die Auflagen vollständig bearbeitet?", "answer": null},
                  {"type": "finding", "label": "Feststellung:", "findingText": ""}
              ]
          }
      },
      "verifikationDesITGrundschutzChecks": {
        "title": "Verifikation des IT-Grundschutz-Checks",
        "subchapterNumber": "5.5",
        "zusammenfassung": {
            "title": "Zusammenfassung des IT-Grundschutz-Checks",
            "subchapterNumber": "5.5.1",
            "content": [
                {"type": "question", "questionText": "Stimmt der im IT-Grundschutz-Check festgestellte Umsetzungsstatus zu den Anforderungen mit dem tatsächlich vorhandenen Informationssicherheitszustand des jeweiligen Zielobjekts überein?", "answer": null},
                {"type": "question", "questionText": "Ist die Begründung der entbehrlichen Anforderungen zulässig und nachvollziehbar? Der Auditor begründet diesen Sachverhalt explizit in der Bausteinprüfung (siehe Kapitel 5.5.2 'Einzelergebnisse des IT-Grundschutz-Checks')", "answer": null},
                {"type": "question", "questionText": "Sind alle Anforderungen mit dem Umsetzungsstatus 'teilweise' oder 'nein' im Referenzdokument A.6 enthalten?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "einzelergebnisse": {
          "title": "Einzelergebnisse des IT-Grundschutz-Checks",
          "subchapterNumber": "5.5.2",
          "bausteinPruefungen": [
            {
              "subchapterNumber": "5.5.2.1",
              "title": "Prüfung für Baustein: [Baustein-Name]",
              "baustein": "",
              "bezogenAufZielobjekt": "",
              "auditiertAm": "",
              "auditor": "",
              "befragtWurde": "",
              "anforderungen": [
                {
                  "nummer": "",
                  "anforderung": "",
                  "bewertung": "",
                  "dokuAntragsteller": "",
                  "pruefmethode": { "D": false, "I": false, "C": false, "S": false, "A": false, "B": false },
                  "auditfeststellung": "",
                  "abweichungen": ""
                }
              ]
            }
          ]
        },
        "ergebnis": {
            "title": "Ergebnis Verifikation des IT-Grundschutz-Checks",
            "subchapterNumber": "5.5.3",
            "content": [
                {"type": "prose", "label": "Votum Verifikation des IT-Grundschutz-Checks:", "text": ""}
            ]
        }
      },
      "risikoanalyseA5": {
        "title": "Risikoanalyse A.5",
        "subchapterNumber": "5.6",
        "zusammenfassungDerRisikoanalyse": {
            "title": "Zusammenfassung der Risikoanalyse",
            "subchapterNumber": "5.6.1",
            "content": [
                {"type": "question", "questionText": "Gibt es im Informationsverbund Zielobjekte mit einem Schutzbedarf 'hoch' oder 'sehr hoch'?", "answer": null},
                {"type": "question", "questionText": "Stimmt der im IT-Grundschutz-Check festgestellte Umsetzungsstatus zusätzlicher Maßnahmen für Zielobjekte mit hohem oder sehr hohem Schutzbedarf mit dem tatsächlich vorhandenen Informationssicherheitszustand des jeweiligen Zielobjekts überein?", "answer": null},
                {"type": "question", "questionText": "Sind alle zusätzlichen Maßnahmen aus der Risikoanalyse als umgesetzt gekennzeichnet?", "answer": null},
                {"type": "question", "questionText": "Sind alle zusätzlichen Maßnahmen mit dem Umsetzungsstatus 'teilweise' oder 'nein' im Referenzdokument A.6 enthalten?", "answer": null},
                {"type": "finding", "label": "Feststellung:", "findingText": ""}
            ]
        },
        "einzelergebnisseDerRisikoanalyse": {
            "title": "Einzelergebnisse der Risikoanalyse",
            "subchapterNumber": "5.6.2",
            "massnahmenPruefungen": [
              {
                "massnahme": "",
                "zielobjekt": "",
                "bewertung": "",
                "dokuAntragsteller": "",
                "pruefmethode": { "D": false, "I": false, "C": false, "S": false, "A": false, "B": false },
                "auditfeststellung": ""
              }
            ]
        },
        "ergebnisDerRisikoanalyse": {
            "title": "Ergebnis Risikoanalyse",
            "subchapterNumber": "5.6.3",
            "content": [
                {"type": "prose", "label": "Votum Risikoanalyse:", "text": ""}
            ]
        }
      }
    },
    "gesamtvotum": {
        "chapterNumber": "6.",
        "title": "Gesamtvotum",
        "gesamteinschaetzung": "",
        "votum": {
          "entspricht": null,
          "datum": "",
          "unterschrift": "Unterschrift des Auditteamleiters"
        }        
    },
    "anhang": {
      "chapterNumber": "7.",
      "title": "Anhang",
      "referenzdokumente": {
        "title": "Referenzdokumente",
        "subchapterNumber": "7.1",
        "table": {
          "headers": ["Nummer", "Kurzbezeichnung", "Dateiname / Verweis", "Version, Datum", "Relevante Änderungen"],
          "rows": []
        }
      },
      "abweichungenUndEmpfehlungen": {
        "title": "Abweichungen und Empfehlungen",
        "subchapterNumber": "7.2",
        "schwerwiegendeAbweichungen": {
          "title": "Schwerwiegende Abweichungen",
          "table": {
            "headers": ["Nummer", "Beschreibung der Abweichung", "Quelle (Kapitel)", "Behebungsfrist", "Status"],
            "rows": []
          }
        },
        "geringfuegigeAbweichungen": {
          "title": "Geringfügige Abweichungen",
          "table": {
            "headers": ["Nummer", "Beschreibung der Abweichung", "Quelle (Kapitel)", "Behebungsfrist", "Status"],
            "rows": []
          }
        },
        "empfehlungen": {
          "title": "Empfehlungen",
          "table": {
            "headers": ["Nummer", "Beschreibung der Empfehlung", "Quelle (Kapitel)", "Behebungsfrist", "Status"],
            "rows": []
          }
        }
      }
    }
  }
}
==== bsi-audit-automator/assets/json/prompt_config.json ====
{
  "system_message": "You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.\n\nRules:\n1. Imperative: Based *only* on the attached customer documentation files or provided context, answer the following questions!\n2. Imperative: If the attached documents are insufficient to answer a question, state that clearly in your answer (e.g., \"Cannot be determined from the provided documents.\").\n3. While you try to answer the questions, check if the documents contain deviations from BSI Grundschutz.\n4. Generate a finding for each deviation.\n5. Categorize the finding as 'AG' (Minor Deviation), 'AS' (Major Deviation, very seldom, use carefully!), 'E' (Recommendation).\n6. Provide evidence and a reference to the specific document that caused the finding in the finding's description.\n\nYour response MUST be a single JSON object that strictly adheres to the provided JSON schema.\n6. You answer in professional german, you are precise and on the point with ecamples and reasons for your decision.",
  "stages": {
    "ETL": {
      "classify_documents": {
        "prompt": "You are an expert BSI (German Federal Office for Information Security) audit assistant. Your task is to analyze a list of document filenames and classify each one into a single, most appropriate category from a predefined list. The filenames often contain clues like \"A.1\", \"A.4\", \"Netzplan\", \"Sicherheitsleitlinie\", \"Auditbericht\", etc.\n\n**Predefined Categories:**\n- \"Sicherheitsleitlinie\": The main, high-level security policy document.\n- \"Organisations-Richtlinie\": Other policies, guidelines, or organizational rules.\n- \"Informationsverbund\": Documents describing the scope and boundary of the information network.\n- \"Netzplan\": Network diagrams or topology plans.\n- \"Strukturanalyse\": The core structural analysis document (often A.1).\n- \"Schutzbedarfsfeststellung\": Protection needs assessment document (often A.2).\n- \"Modellierung\": The IT Grundschutz modeling document (often A.3).\n- \"Grundschutz-Check\": The implementation check of controls (often A.4).\n- \"Risikoanalyse\": Risk analysis documents (often A.5).\n- \"Realisierungsplan\": The risk treatment or implementation plan (often A.6).\n- \"Dienstleister-Liste\": Lists of external service providers.\n- \"Vorheriger-Auditbericht\": A previous, complete audit report.\n- \"Sonstiges\": Any other document that does not fit the above categories.\n\nAnalyze the following list of filenames and return a structured JSON response mapping each filename to its category.\n\n**Filenames to Classify:**\n---\n{filenames_json}\n---",
        "schema_path": "assets/schemas/etl_classify_documents_schema.json"
      }
    },
    "Chapter-3-Ground-Truth": {
      "extract_zielobjekte": {
        "prompt": "You are an expert data extraction system. From the attached Strukturanalyse document (A.1), extract a complete list of all Zielobjekte (target objects). For each Zielobjekt, provide its unique ID (Kürzel) and its full descriptive name (Name). In the header on the first page, there is a Zielobjekt 'Informationsverbund' and a Kürzel for it as well, make sure to add that to the list as first entry!",
        "schema_path": "assets/schemas/stage_3_gt_zielobjekte_schema.json"
      },
      "extract_baustein_mappings": {
        "prompt": "You are an expert data extraction system. From the attached Modellierung document (A.3), analyze which Zielobjekt (target object) each Baustein (building block) is applied to. Return a list of mappings, where each mapping contains the Baustein ID and the Kürzel (unique ID) of the Zielobjekt it's mapped to. Ignore Bausteine from layers ISMS, ORP, CON, OPS, and DER.",
        "schema_path": "assets/schemas/stage_3_gt_baustein_mappings_schema.json"
      }
    },
    "Chapter-1": {
      "informationsverbund": {
        "prompt": "Your task is to analyze the provided context, which describes the scope of an information security network (Informationsverbund), and extract two key pieces of information:\n1.  **kurzbezeichnung**: A short, official name or title for the Informationsverbund.\n2.  **kurzbeschreibung**: A concise paragraph summarizing the scope, including key business processes, applications, and physical locations. This should also serve as the main description for the 'Geltungsbereich'.\n\nIf the provided context is insufficient or empty, state that the scope could not be fully determined from the documents and must be clarified manually.",
        "schema_path": "assets/schemas/stage_1_4_informationsverbund_schema.json"
      }
    },
    "Chapter-3": {
      "generic_question": {
        "prompt": "The questions to answer are:\n{questions}"
      },
      "targeted_question": {
        "prompt": "You are a BSI auditor. Based *only* on the following structured JSON data and, if provided, the attached context document(s), answer the question: {question}\n\n**JSON Data:**\n---\n{json_data}\n---"
      },
      "generic_summary": {
        "prompt": "You are a BSI security auditor providing a final verdict on the {summary_topic}.\nBased on the summary of findings from the previous sections provided below, provide a summary verdict (\"Votum\").\n\n**Summary of Previous Findings:**\n---\n{previous_findings}\n---"
      },
      "refine_layout_parser_group": {
        "prompt": "You are an expert system for structuring BSI Grundschutz data. Your input is a JSON object containing a list of layout blocks from a Document AI Layout Parser. These blocks all belong to a single Zielobjekt (target object). Your task is to analyze these blocks and assemble them into a final, structured list of requirements according to the provided schema. The goal is to find rows or groups of text that represent a single security requirement and extract its ID, title, status, explanation, and last-checked date.\n\n**Input Document AI Layout Blocks:**\n---\n{zielobjekt_blocks_json}\n---",
        "schema_path": "assets/schemas/stage_3_6_1_extract_check_data_schema.json"
      },
      "aktualitaetDerReferenzdokumente": {
        "schema_path": "assets/schemas/stage_3_1_aktualitaet_schema.json",
        "prompt": "Ignore the missing A.4 GRundschutz-Check.",
        "source_categories": ["Sicherheitsleitlinie", "Organisations-Richtlinie", "Netzplan", "Strukturanalyse", "Dienstleister-Liste", "Realisierungsplan", "Risikoanalyse", "Schutzbedarfsfeststellung", "Modellierung"]
      },
      "sicherheitsleitlinieUndRichtlinienInA0": {
        "schema_path": "assets/schemas/stage_3_2_sicherheitsleitlinie_schema.json",
        "source_categories": ["Sicherheitsleitlinie", "Organisations-Richtlinie"]
      },
      "definitionDesInformationsverbundes": {
        "schema_path": "assets/schemas/stage_3_3_1_informationsverbund_schema.json",
        "source_categories": ["Informationsverbund", "Strukturanalyse"]
      },
      "bereinigterNetzplan": {
        "schema_path": "assets/schemas/stage_3_3_2_netzplan_schema.json",
        "source_categories": ["Netzplan", "Strukturanalyse"]
      },
      "listeDerGeschaeftsprozesse": {
        "schema_path": "assets/schemas/stage_3_3_3_geschaeftsprozesse_schema.json",
        "source_categories": ["Strukturanalyse"]
      },
      "listeDerAnwendungen": {
        "schema_path": "assets/schemas/generic_1_question_schema.json",
        "source_categories": ["Strukturanalyse"]
      },
      "listeDerItSysteme": {
        "schema_path": "assets/schemas/generic_1_question_schema.json",
        "source_categories": ["Strukturanalyse"]
      },
      "listeDerRaeumeGebaeudeStandorte": {
        "schema_path": "assets/schemas/generic_1_question_schema.json",
        "source_categories": ["Strukturanalyse"]
      },
      "listeDerKommunikationsverbindungen": {
        "schema_path": "assets/schemas/generic_1_question_schema.json",
        "source_categories": ["Strukturanalyse"]
      },
      "listeDerDienstleister": {
        "schema_path": "assets/schemas/generic_1_question_schema.json",
        "source_categories": ["Strukturanalyse", "Dienstleister-Liste"]
      },
      "stichprobenDokuStrukturanalyse": {
        "schema_path": "assets/schemas/generic_0_question_schema.json",
        "source_categories": ["Strukturanalyse"]
       },
      "stichprobenDokuSchutzbedarf": {
         "schema_path": "assets/schemas/generic_0_question_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },      
      "ergebnisDerStrukturanalyse": {
        "type": "summary",
        "schema_path": "assets/schemas/stage_3_summary_schema.json"
      },
      "definitionDerSchutzbedarfskategorien": {
        "schema_path": "assets/schemas/stage_3_4_1_schutzbedarfskategorien_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },
      "schutzbedarfGeschaeftsprozesse": {
        "schema_path": "assets/schemas/generic_2_question_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },
      "schutzbedarfAnwendungen": {
        "schema_path": "assets/schemas/generic_2_question_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },
      "schutzbedarfItSysteme": {
        "schema_path": "assets/schemas/generic_2_question_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },
      "schutzbedarfRaeume": {
        "schema_path": "assets/schemas/generic_2_question_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },
      "schutzbedarfKommunikationsverbindungen": {
        "schema_path": "assets/schemas/generic_2_question_schema.json",
        "source_categories": ["Schutzbedarfsfeststellung"]
      },
      "ergebnisDerSchutzbedarfsfeststellung": {
        "type": "summary",
        "schema_path": "assets/schemas/stage_3_summary_schema.json"
      },
      "modellierungsdetails": {
        "schema_path": "assets/schemas/stage_3_5_1_modellierungsdetails_schema.json",
        "prompt": "The questions to answer are:\n{questions}\n\nAdditionally, review the attached Modellierung document against the authoritative list of Zielobjekte provided below. Does the Modellierung correctly account for all Zielobjekte from this list that are relevant?\n\n**Authoritative List of Zielobjekte (Ground Truth):**\n---\n{zielobjekte_json}\n---",
        "source_categories": ["Modellierung"]
      },
      "ergebnisDerModellierung": {
        "type": "summary",
        "schema_path": "assets/schemas/stage_3_summary_schema.json"
      },
      "detailsZumItGrundschutzCheck": {
        "type": "custom_logic"
      },
      "benutzerdefinierteBausteine": {
        "schema_path": "assets/schemas/generic_2_question_schema.json",
        "source_categories": ["Modellierung"]
      },
      "ergebnisItGrundschutzCheck": {
        "type": "summary",
        "schema_path": "assets/schemas/stage_3_summary_schema.json"
      },
      "risikoanalyse": {
        "schema_path": "assets/schemas/stage_3_7_risikoanalyse_schema.json",
        "source_categories": ["Risikoanalyse"]
      },
      "questions": {
        "entbehrlich": "Sind die Begründungen für 'entbehrlich' plausibel? BSI-Regel: 1. Wenn eine alternative Schutzmaßnahme beschrieben ist\n2. Eine Anforderung mit Level 5 ist immer entbehrlich, **außer** wenn sie durch in der beigefügten Risikoanalyse explizit gefordert wird. Eine Formulierung, dass diese Anforderung nicht von der Risikoanalyse gefordert wird, ist also für Anforderungen im Level 5 immer akzeptabel! Auch ist es akzeptabel, wenn für Level 5 keine Begründung für Entbehrlich vorhanden ist!\n\nFüge eine Liste der nicht ausreichenden begründeten Entbehrlichen Anforderungen mit begründung der Feststellung hinzu!",
        "muss_anforderungen": "Sind alle diese MUSS-Anforderungen (Level 1) mit Status 'Ja' umgesetzt? Füge eine Liste der nicht umgesetzten MUSS Anforderungen der Feststellung hinzu!",
        "nicht_umgesetzt": "Sind diese nicht oder teilweise umgesetzten Anforderungen im angehängten Realisierungsplan (A.6) dokumentiert? Füge eine kurze Auswahl der im A.6 enthaltenen nicht umgesetzten Anforderungen der Feststellung hinzu!"
      },
      "realisierungsplan": {
        "schema_path": "assets/schemas/generic_3_question_schema.json",
        "source_categories": ["Realisierungsplan"]
      },
      "ergebnisDerDokumentenpruefung": {
        "type": "summary",
        "schema_path": "assets/schemas/stage_3_9_ergebnis_schema.json"
      }
    },
    "Chapter-4": {
      "auswahlBausteineErstRezertifizierung": {
        "key": "4.1.1",
        "prompt": "You are a BSI Lead Auditor planning an initial certification audit (Erstzertifizierung). Your plan must strictly adhere to the rules outlined in the official BSI \"Auditierungsschema\".\n\n**CRITICAL INSTRUCTION:** Your plan MUST be based on the customer's actual system structure provided below in the 'System Structure Map'. When selecting a Zielobjekt, you MUST use the corresponding `name` and `kuerzel` from the `zielobjekte` list. The selected `kuerzel` MUST be one that the chosen Baustein is actually mapped to in the `baustein_to_zielobjekt_mapping`.\n\n**System Structure Map (Ground Truth):**\n---\n{ground_truth_map_json}\n---\n\n**Official Rules for Baustein Selection (from Auditierungsschema, Chapter 4.3):**\n1.  **Minimum Count:** You MUST audit at least 6 Bausteine.\n2.  **Mandatory Baustein:** The Baustein 'ISMS.1 Sicherheitsmanagement' MUST be included in your selection. Its Zielobjekt is ALWAYS 'Gesamter Informationsverbund' (Kürzel: 'Informationsverbund').\n3.  **Risk-Oriented Selection:** You must select the other Bausteine based on risk, ensuring that you cover a variety of layers (Schichten) like ORP, CON, OPS, NET, INF, and SYS.\n4.  **Justification:** You MUST provide a concise, professional justification ('Begründung zur Auswahl') for the selection of each Baustein. The justification must be a full sentence explaining the risk-based reason for the selection. Example: 'Überprüfung der zentralen Firewall-Regeln, da diese den gesamten Informationsverbund nach außen absichern.'\n\nBased on the ground-truth map and the official rules, generate a realistic, accurate, and compliant audit plan with the required columns 'Schicht', 'Baustein', 'Zielobjekt-Name', 'Zielobjekt-Kürzel', and 'Begründung zur Auswahl'.",
        "schema_path": "assets/schemas/stage_4_1_1_auswahl_bausteine_erst_schema.json"
      },
      "auswahlBausteine1Ueberwachungsaudit": {
        "key": "4.1.2",
        "prompt": "You are a BSI Lead Auditor planning the **first surveillance audit (1. Überwachungsaudit)**. Your plan must strictly adhere to the rules outlined in the official BSI \"Auditierungsschema\".\n\n**CRITICAL INSTRUCTION:** Your plan MUST be based on the customer's actual system structure provided below in the 'System Structure Map'. When selecting a Zielobjekt, you MUST use the corresponding `name` and `kuerzel` from the `zielobjekte` list. The selected `kuerzel` MUST be one that the chosen Baustein is actually mapped to in the `baustein_to_zielobjekt_mapping`.\n\n**System Structure Map (Ground Truth):**\n---\n{ground_truth_map_json}\n---\n\n**Official Rules for Baustein Selection for a Surveillance Audit:**\n1.  **Mandatory Baustein:** The Baustein 'ISMS.1 Sicherheitsmanagement' MUST be included. Its Zielobjekt is ALWAYS 'Gesamter Informationsverbund' (Kürzel: 'Informationsverbund').\n2.  **Minimum Count:** You MUST select at least **two** other Bausteine in addition to ISMS.1.\n3.  **Risk-Oriented Selection:** You must select the other Bausteine based on significant changes, previous deviations, or risk. Ensure you cover a variety of layers (Schichten) like ORP, CON, OPS, NET, INF, and SYS.\n4.  **Justification:** You must provide a concise, professional justification (Begründung zur Auswahl) for the selection of each Baustein.\n\nBased on the ground-truth map and the official rules, generate a realistic, accurate, and compliant audit plan with the required columns 'Schicht', 'Baustein', 'Zielobjekt-Name', 'Zielobjekt-Kürzel', and 'Begründung zur Auswahl'.",
        "schema_path": "assets/schemas/stage_4_1_2_auswahl_bausteine_ueberwachung_schema.json"
      },
        "auswahlBausteine2Ueberwachungsaudit": {
          "key": "4.1.3",
          "prompt_normal": "You are a BSI Lead Auditor planning the **second surveillance audit (2. Überwachungsaudit)**. Your plan must strictly adhere to the rules outlined in the official BSI \"Auditierungsschema\".\n\n**CRITICAL INSTRUCTION:** Your plan MUST be based on the customer's actual system structure provided below in the 'System Structure Map'. When selecting a Zielobjekt, you MUST use the corresponding `name` and `kuerzel` from the `zielobjekte` list. The selected `kuerzel` MUST be one that the chosen Baustein is actually mapped to in the `baustein_to_zielobjekt_mapping`.\n\n**System Structure Map (Ground Truth):**\n---\n{ground_truth_map_json}\n---\n\n**Official Rules for Baustein Selection for a Surveillance Audit:**\n1.  **Mandatory Baustein:** The Baustein 'ISMS.1 Sicherheitsmanagement' MUST be included. Its Zielobjekt is ALWAYS 'Gesamter Informationsverbund' (Kürzel: 'Informationsverbund').\n2.  **Minimum Count:** You MUST select at least **two** other Bausteine in addition to ISMS.1.\n3.  **No Repeats (where possible):** The selected Bausteine (other than ISMS.1) should ideally be different from those chosen in the first surveillance audit to ensure broad coverage over the certification lifecycle.\n4.  **Risk-Oriented Selection:** You must select the other Bausteine based on significant changes, previous deviations, or risk. Ensure you cover a variety of layers (Schichten) like ORP, CON, OPS, NET, INF, and SYS.\n5.  **Justification:** You must provide a concise, professional justification (Begründung zur Auswahl) for the selection of each Baustein.\n\nBased on the ground-truth map and the official rules, generate a realistic, accurate, and compliant audit plan with the required columns 'Schicht', 'Baustein', 'Zielobjekt-Name', 'Zielobjekt-Kürzel', and 'Begründung zur Auswahl'.",
          "prompt": "Imperative: you only return these values in the schema supplied: \"rows\": [\n              {\n                \"Schicht\": \"ISMS\",\n                \"Baustein\": \"ISMS.1 Sicherheitsmanagement\",\n                \"Zielobjekt-Name\": \"Gesamter Informationsverbund\",\n                \"Zielobjekt-Kürzel\": \"Informationsverbund\",\n                \"Begründung zur Auswahl\": \"Gemäß BSI-Vorgaben für Überwachungsaudits ist die Prüfung des ISMS-Betriebs obligatorisch, um die kontinuierliche Wirksamkeit und Weiterentwicklung des Managementsystems sowie die Behandlung von Abweichungen aus dem letzten Audit sicherzustellen.\"\n              },\n              {\n                \"Schicht\": \"APP\",\n                \"Baustein\": \"APP.2.1 Active Directory Domänendienste\",\n                \"Zielobjekt-Name\": \"Azure AD-Connect\",\n                \"Zielobjekt-Kürzel\": \"A-001\",\n                \"Begründung zur Auswahl\": \"Stichprobenartige Prüfung aufgrund der kritischen Funktion der Identitätssynchronisation zwischen On-Premise-AD und der Cloud-Umgebung (Microsoft 365). Änderungen in diesem Bereich haben weitreichende Auswirkungen auf die Zugriffssicherheit im gesamten Verbund.\"\n              },\n              {\n                \"Schicht\": \"NET\",\n                \"Baustein\": \"NET.1.1 Netzarchitektur und -design\",\n                \"Zielobjekt-Name\": \"Admin-LAN\",\n                \"Zielobjekt-Kürzel\": \"N-002\",\n                \"Begründung zur Auswahl\": \"Überprüfung der Netzarchitektur am Beispiel des hochkritischen Admin-LANs. Es soll sichergestellt werden, dass die Segmentierung und die Schutzmechanismen weiterhin dem Stand der Technik entsprechen und wirksam sind, insbesondere im Hinblick auf die Absicherung administrativer Zugriffe.\"\n              },\n              {\n                \"Schicht\": \"INF\",\n                \"Baustein\": \"INF.5 Raum- und Objektschutz\",\n                \"Zielobjekt-Name\": \"Technikraum Berlin\",\n                \"Zielobjekt-Kürzel\": \"BE-2\",\n                \"Begründung zur Auswahl\": \"Stichprobenartige Prüfung der physischen Sicherheitsmaßnahmen für einen zentralen Technikraum. Die Wirksamkeit von Zutrittskontrollen, Umgebungsüberwachung und Brandschutz ist essenziell für die Verfügbarkeit der darin betriebenen IT-Systeme.\"\n              }\n]",
          "schema_path": "assets/schemas/stage_4_1_3_auswahl_bausteine_ueberwachung_schema.json"
      },
      "auswahlMassnahmenAusRisikoanalyse": {
        "key": "4.1.5",
        "prompt": "You are a BSI Lead Auditor planning an audit. Your task is to select a small, representative sample of 2-3 additional security measures that would typically result from a risk analysis for assets with \"high\" or \"very high\" protection needs. These are measures BEYOND the standard IT-Grundschutz baseline requirements.\n\nFor each selected measure, provide a plausible and risk-oriented justification (key: 'Begründung zur Auswahl') for its inclusion in the on-site audit plan.\n\nExamples of good justifications:\n- \"To verify the effectiveness of the advanced DDoS mitigation for the public-facing web application, which is a critical business process.\"\n- \"To check the implementation of database encryption, as this measure was identified to protect highly sensitive customer PII.\"\n- \"To confirm the sandboxing of the email attachment analysis system, a key control against advanced persistent threats.\"",
        "schema_path": "assets/schemas/stage_4_1_5_auswahl_massnahmen_risiko_schema.json"
      }
    },
    "Scan-Report": {
      "extract_chapter_1": {
        "prompt": "You are an expert data extraction system. From the attached previous audit report, extract the complete content of the tables in subchapters 1.1 (Versionshistorie), 1.2 (Kontaktinformationen des Antragstellers AND Ansprechpartner), and 1.3 (Auditteam). The auditteam has multiple sub-tables; capture all of them.",
        "schema_path": "assets/schemas/scan_report_ch1_schema.json"
      },
      "extract_chapter_4": {
        "prompt": "You are an expert data extraction system. From the attached previous audit report, extract the 'Auswahl Bausteine' tables from subchapters 4.1.1 and 4.1.2, and the 'Auswahl Standorte' table from 4.1.4. Capture all rows from all three tables.",
        "schema_path": "assets/schemas/scan_report_ch4_schema.json"
      },
      "extract_chapter_7": {
        "prompt": "From the attached audit report, find chapter 7.2 'Abweichungen und Empfehlungen'. Scan this chapter and its sub-chapters for any tables containing findings. These tables might be titled 'Abweichungen', 'Schwerwiegende Abweichungen', 'Geringfügige Abweichungen', or 'Empfehlungen'. Extract every single finding you can identify into a single, flat list. For each finding, determine its category ('AS' for Schwerwiegend, 'AG' for Geringfügig, or 'E' for Empfehlung) and extract all its corresponding table columns like 'Nummer', 'Beschreibung', 'Quelle', 'Behebungsfrist', and 'Status'.",
        "schema_path": "assets/schemas/scan_report_ch7_schema.json"
      }
    }
  }
}
==== bsi-audit-automator/assets/schemas/bsi_gk_2023_oscal_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://example.com/oscal/bsi-grundschutz-catalog.final.schema.json",
  "title": "Final Valid OSCAL-Compliant BSI Grundschutz Catalog Schema",
  "description": "A corrected, valid OSCAL schema. This version correctly defines the 'parts' property within a group, making it fully compliant with OSCAL 1.1.2. It now includes 'prose_qs' for generated questions.",
  "type": "object",
  "required": ["catalog"],
  "properties": {
    "catalog": {
      "title": "OSCAL Catalog",
      "type": "object",
      "required": ["uuid", "metadata", "groups"],
      "properties": {
        "uuid": { "$ref": "#/definitions/common/uuid" },
        "metadata": { "$ref": "#/definitions/oscal/metadata" },
        "groups": { "type": "array", "items": { "$ref": "#/definitions/oscal/group" } }
      },
      "additionalProperties": false
    }
  },
  "definitions": {
    "oscal": {
      "metadata": {
        "title": "OSCAL Metadata",
        "type": "object",
        "required": ["title", "last-modified", "version", "oscal-version"],
        "properties": {
          "title": { "type": "string" },
          "last-modified": { "$ref": "#/definitions/common/dateTime" },
          "version": { "type": "string" },
          "oscal-version": { "type": "string", "const": "1.1.2" }
        },
        "additionalProperties": false
      },
      "group": {
        "title": "OSCAL Control Group",
        "description": "Represents a BSI layer (e.g., 'OPS') or a nested 'Baustein'. Uses 'id' for identification.",
        "type": "object",
        "required": ["id", "title"],
        "properties": {
          "id": { "$ref": "#/definitions/common/token" },
          "class": { "$ref": "#/definitions/common/token" },
          "title": { "type": "string" },
          "parts": {
            "description": "Contextual parts for a Baustein, like introduction or risks.",
            "type": "array",
            "items": { "$ref": "#/definitions/oscal/part" }
          },
          "groups": {
            "type": "array",
            "items": { "$ref": "#/definitions/oscal/group" }
          },
          "controls": {
            "type": "array",
            "items": { "$ref": "#/definitions/oscal/control" }
          }
        },
        "additionalProperties": false
      },
      "control": {
        "title": "OSCAL Control",
        "description": "A single BSI requirement.",
        "type": "object",
        "required": ["id", "title"],
        "properties": {
          "id": { "$ref": "#/definitions/common/token" },
          "class": { "$ref": "#/definitions/common/token" },
          "title": { "type": "string" },
          "props": { "type": "array", "items": { "$ref": "#/definitions/oscal/property" } },
          "parts": { "type": "array", "items": { "$ref": "#/definitions/oscal/part" } }
        },
        "additionalProperties": false
      },
      "part": {
        "title": "OSCAL Part",
        "description": "A textual component of a control or group.",
        "type": "object",
        "required": ["name"],
        "properties": {
          "id": { "$ref": "#/definitions/common/token" },
          "name": { "$ref": "#/definitions/common/token" },
          "title": { "type": "string" },
          "class": { "$ref": "#/definitions/common/token" },
          "prose": { "type": "string" },
          "prose_qs": { "type": "string" },
          "parts": { "type": "array", "items": { "$ref": "#/definitions/oscal/part" } }
        },
        "additionalProperties": false
      },
      "property": { "type": "object", "required": ["name", "value"], "properties": { "name": { "$ref": "#/definitions/common/token" }, "value": { "type": "string" }, "ns": { "type": "string", "format": "uri" } }, "additionalProperties": false }
    },
    "common": {
      "uuid": { "type": "string", "format": "uuid", "pattern": "^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$" },
      "token": { "type": "string", "pattern": "^\\S(.*\\S)?$" },
      "dateTime": { "type": "string", "format": "date-time" }
    }
  }
}
==== bsi-audit-automator/assets/schemas/etl_classify_documents_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Document Classification Schema",
    "description": "A schema for the classified list of source documents.",
    "type": "object",
    "properties": {
      "document_map": {
        "type": "array",
        "description": "An array mapping each source document to its BSI category.",
        "items": {
          "type": "object",
          "properties": {
            "filename": {
              "type": "string",
              "description": "The full original filename from GCS."
            },
            "category": {
              "type": "string",
              "description": "The BSI document category.",
              "enum": [
                "Sicherheitsleitlinie",
                "Organisations-Richtlinie",
                "Informationsverbund",
                "Netzplan",
                "Strukturanalyse",
                "Schutzbedarfsfeststellung",
                "Modellierung",
                "Grundschutz-Check",
                "Risikoanalyse",
                "Realisierungsplan",
                "Dienstleister-Liste",
                "Vorheriger-Auditbericht",
                "Sonstiges"
              ]
            }
          },
          "required": ["filename", "category"]
        }
      }
    },
    "required": ["document_map"]
  }
==== bsi-audit-automator/assets/schemas/generic_0_question_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic Schema for a finding only",
    "description": "A schema for a response that only contains a structured audit finding.",
    "type": "object",
    "properties": {
      "finding": {
        "type": "object",
        "properties": {
          "category": { "type": "string", "enum": ["AG", "AS", "E", "OK"] },
          "description": { "type": "string" }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["finding"]
  }
==== bsi-audit-automator/assets/schemas/generic_1_question_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic Schema for a Single Boolean Question",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "Answer to the question.",
        "items": { "type": "boolean" },
        "minItems": 1,
        "maxItems": 1
      },
      "finding": {
        "type": "object",
        "properties": {
          "category": { "type": "string", "enum": ["AG", "AS", "E", "OK"] },
          "description": { "type": "string" }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/generic_2_question_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic Schema for Two Boolean Questions",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "Answers to the questions.",
        "items": { "type": "boolean" },
        "minItems": 2,
        "maxItems": 2
      },
      "finding": {
        "type": "object",
        "properties": {
          "category": { "type": "string", "enum": ["AG", "AS", "E", "OK"] },
          "description": { "type": "string" }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/generic_3_question_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic 3-Question Response",
    "description": "A generic schema for answering 3 questions and providing a single audit finding.",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "An array of 3 answers, one for each question asked. The order must be preserved.",
        "items": { "type": "string" },
        "minItems": 3,
        "maxItems": 3
      },
      "finding": {
        "type": "object",
        "description": "A single, structured finding for this section.",
        "properties": {
          "category": {
            "type": "string",
            "description": "Category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK'.",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string",
            "description": "A clear, concise description of the finding, including evidence and document reference."
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/generic_4_question_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic 4-Question Response",
    "description": "A generic schema for answering 4 questions and providing a single audit finding.",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "An array of 4 answers, one for each question asked. The order must be preserved.",
        "items": { "type": "string" },
        "minItems": 4,
        "maxItems": 4
      },
      "finding": {
        "type": "object",
        "description": "A single, structured finding for this section.",
        "properties": {
          "category": {
            "type": "string",
            "description": "Category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK'.",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string",
            "description": "A clear, concise description of the finding, including evidence and document reference."
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/generic_5_question_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic 5-Question Response",
    "description": "A generic schema for answering 5 questions and providing a single audit finding.",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "An array of 5 answers, one for each question asked. The order must be preserved.",
        "items": { "type": "string" },
        "minItems": 5,
        "maxItems": 5
      },
      "finding": {
        "type": "object",
        "description": "A single, structured finding for this section.",
        "properties": {
          "category": {
            "type": "string",
            "description": "Category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK'.",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string",
            "description": "A clear, concise description of the finding, including evidence and document reference."
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/generic_summary_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Generic Summary Votum Response",
    "description": "A schema for a summary section that provides a final verdict ('Votum') and a structured finding.",
    "type": "object",
    "properties": {
      "votum": {
        "type": "string",
        "description": "The summary verdict or assessment for the entire section."
      },
      "finding": {
        "type": "object",
        "description": "A single, overarching finding for the entire summarized section. Set to 'OK' if the overall status is compliant.",
        "properties": {
          "category": {
            "type": "string",
            "description": "The overall category for the summary: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK'.",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string",
            "description": "A clear, concise description of the overall finding for the section."
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["votum", "finding"]
  }
==== bsi-audit-automator/assets/schemas/scan_report_ch1_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Scanned Previous Report Data (Chapter 1)",
    "description": "Extracts key informational tables from Chapter 1 of a previous audit report.",
    "type": "object",
    "properties": {
        "versionshistorie": {
            "type": "object",
            "properties": {
                "table": {
                    "type": "object",
                    "properties": {
                        "rows": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "Datum": { "type": "string" },
                                    "Version": { "type": "string" },
                                    "Verfasser": { "type": "string" },
                                    "Bemerkungen": { "type": "string" }
                                },
                                "required": ["Datum", "Version", "Verfasser", "Bemerkungen"]
                            }
                        }
                    },
                    "required": ["rows"]
                }
            },
            "required": ["table"]
        },
        "auditierteInstitution": {
            "type": "object",
            "properties": {
                "kontaktinformationenAntragsteller": {
                    "type": "object",
                    "properties": {
                        "table": {
                            "type": "object",
                            "properties": {
                                "rows": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "Institution": { "type": "string" },
                                            "Straße / Hausnummer": { "type": "string" },
                                            "PLZ / Ort": { "type": "string" },
                                            "E-Mail": { "type": "string" }
                                        },
                                        "required": ["Institution", "Straße / Hausnummer", "PLZ / Ort", "E-Mail"]
                                    }
                                }
                            },
                            "required": ["rows"]
                        }
                    },
                    "required": ["table"]
                },
                "ansprechpartnerZertifizierung": {
                    "type": "object",
                    "properties": {
                        "table": {
                            "type": "object",
                            "properties": {
                                "rows": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "Name": { "type": "string" },
                                            "Funktion": { "type": "string" },
                                            "Telefon": { "type": "string" },
                                            "E-Mail": { "type": "string" },
                                            "Optional: Abweichende Anschrift": { "type": "string" }
                                        }
                                    }
                                }
                            },
                            "required": ["rows"]
                        }
                    },
                    "required": ["table"]
                }
            },
            "required": ["kontaktinformationenAntragsteller", "ansprechpartnerZertifizierung"]
        },
        "auditteam": {
            "type": "object",
            "description": "Contains all sub-tables related to the audit team. Properties are optional.",
            "properties": {
                "auditteamleiter": {
                    "type": "object",
                    "properties": {
                        "table": {
                            "type": "object",
                             "properties": {
                                "rows": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "Name": { "type": "string" },
                                            "Institution": { "type": "string" }
                                        }
                                    }
                                }
                            },
                            "required": ["rows"]
                        }
                    }
                },
                "fachexperte": {
                    "type": "object",
                    "properties": {
                        "table": {
                            "type": "object",
                             "properties": {
                                "rows": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "Name": { "type": "string" },
                                            "Institution": { "type": "string" }
                                        }
                                    }
                                }
                            },
                            "required": ["rows"]
                        }
                    }
                },
                "auditor": {
                    "type": "object",
                    "properties": {
                        "table": {
                            "type": "object",
                             "properties": {
                                "rows": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "Name": { "type": "string" },
                                            "Institution": { "type": "string" }
                                        }
                                    }
                                }
                            },
                            "required": ["rows"]
                        }
                    }
                }
            }
        }
    },
    "required": ["versionshistorie", "auditierteInstitution", "auditteam"]
}
==== bsi-audit-automator/assets/schemas/scan_report_ch4_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Scanned Previous Report Data (Chapter 4)",
    "description": "Extracts previous audit scope tables from Chapter 4.",
    "type": "object",
    "properties": {
        "auswahlBausteineErstRezertifizierung": {
            "type": "object",
            "properties": {
                "table": {
                    "type": "object",
                    "properties": {
                        "rows": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "Schicht": { "type": "string" },
                                    "Baustein": { "type": "string" },
                                    "Zielobjekt": { "type": "string" },
                                    "Begründung zur Auswahl": { "type": "string" }
                                },
                                "required": ["Schicht", "Baustein", "Zielobjekt", "Begründung zur Auswahl"]
                            }
                        }
                    },
                    "required": ["rows"]
                }
            }
        },
        "auswahlBausteine1Ueberwachungsaudit": {
            "type": "object",
            "properties": {
                "table": {
                    "type": "object",
                    "properties": {
                        "rows": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "Schicht": { "type": "string" },
                                    "Baustein": { "type": "string" },
                                    "Zielobjekt": { "type": "string" },
                                    "Begründung zur Auswahl": { "type": "string" }
                                },
                                "required": ["Schicht", "Baustein", "Zielobjekt", "Begründung zur Auswahl"]
                            }
                        }
                    },
                    "required": ["rows"]
                }
            }
        },
        "auswahlStandorte": {
            "type": "object",
            "properties": {
                "table": {
                    "type": "object",
                    "properties": {
                        "rows": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "Standort": { "type": "string" },
                                    "Erst- bzw. Rezertifizierung": { "type": "string" },
                                    "1. Überwachungsaudit": { "type": "string" },
                                    "2. Überwachungsaudit": { "type": "string" },
                                    "Begründung für die Auswahl": { "type": "string" }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
==== bsi-audit-automator/assets/schemas/scan_report_ch7_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Scanned Previous Report Findings (Chapter 7.2)",
  "description": "Extracts a single, flat list of all findings (AS, AG, E) from tables in or around Chapter 7.2 of a previous audit report.",
  "type": "object",
  "properties": {
    "all_findings": {
      "type": "array",
      "description": "A single, flat list of all findings extracted from the report.",
      "items": {
        "type": "object",
        "properties": {
          "category": {
            "type": "string",
            "description": "The determined category of the finding.",
            "enum": ["AS", "AG", "E"]
          },
          "nummer": { "type": "string", "description": "The finding's number or ID." },
          "beschreibung": { "type": "string", "description": "The full description of the finding." },
          "quelle": { "type": "string", "description": "The source chapter or reference." },
          "behebungsfrist": { "type": "string", "description": "The deadline for correction." },
          "status": { "type": "string", "description": "The current status of the finding." }
        },
        "required": ["category", "nummer", "beschreibung"]
      }
    }
  },
  "required": ["all_findings"]
}
==== bsi-audit-automator/assets/schemas/stage_1_2_geltungsbereich_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for BSI Grundschutz Stage 1.2 - Scope of Certification",
    "description": "Defines the structure for the scope description and an associated finding.",
    "type": "object",
    "properties": {
      "text": {
        "type": "string",
        "description": "The formal text describing the scope of the audit (Geltungsbereich)."
      },
      "finding": {
        "type": "object",
        "description": "A structured finding regarding the quality and completeness of the scope definition.",
        "properties": {
          "category": {
            "type": "string",
            "description": "The category of the finding: 'AG', 'AS', 'E', or 'OK'.",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string",
            "description": "A detailed description of the finding related to the scope definition."
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["text", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_1_4_informationsverbund_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Informationsverbund Description",
    "description": "Schema for the AI's analysis of the Informationsverbund scope (Chapter 1.4).",
    "type": "object",
    "properties": {
      "kurzbezeichnung": {
        "description": "The short, official name of the Informationsverbund.",
        "type": "string"
      },
      "kurzbeschreibung": {
        "description": "A concise summary of the audit scope (Geltungsbereich), including business processes, locations, and applications.",
        "type": "string"
      },
      "finding": {
        "description": "A structured finding regarding the quality and clarity of the scope definition.",
        "type": "object",
        "properties": {
          "category": {
            "description": "The category of the finding: 'OK' if no issue, 'AG' (Minor Deviation), 'AS' (Major Deviation), or 'E' (Recommendation).",
            "type": "string",
            "enum": ["OK", "AG", "AS", "E"]
          },
          "description": {
            "description": "A detailed, evidence-based description of the finding. If category is 'OK', this should be a brief confirmation.",
            "type": "string"
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["kurzbezeichnung", "kurzbeschreibung", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_3_1_aktualitaet_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Subchapter 3.1 Aktualitaet der Referenzdokumente",
  "type": "object",
  "properties": {
    "answers": {
      "type": "array",
      "description": "The answers to the four questions regarding the actuality of the documents.",
      "items": {
        "anyOf": [
          { "type": "boolean" },
          { "type": "string", "format": "date" }
        ]
      },
      "minItems": 4,
      "maxItems": 4
    },
    "finding": {
      "type": "object",
      "description": "A structured finding based on the analysis.",
      "properties": {
        "category": {
          "type": "string",
          "description": "The category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK' (No Deviation).",
          "enum": ["AG", "AS", "E", "OK"]
        },
        "description": {
          "type": "string",
          "description": "A detailed description of the finding."
        }
      },
      "required": ["category", "description"]
    }
  },
  "required": ["answers", "finding"]
}
==== bsi-audit-automator/assets/schemas/stage_3_2_sicherheitsleitlinie_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Subchapter 3.2 Sicherheitsleitlinie und Richtlinien in A.0",
  "type": "object",
  "properties": {
    "answers": {
      "type": "array",
      "description": "The answers to the three questions regarding the security policy.",
      "items": {
        "type": "boolean"
      },
      "minItems": 3,
      "maxItems": 3
    },
    "finding": {
      "type": "object",
      "description": "A structured finding based on the analysis.",
      "properties": {
        "category": {
          "type": "string",
          "description": "The category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK' (No Deviation).",
          "enum": ["AG", "AS", "E", "OK"]
        },
        "description": {
          "type": "string",
          "description": "A detailed description of the finding."
        }
      },
      "required": ["category", "description"]
    }
  },
  "required": ["answers", "finding"]
}
==== bsi-audit-automator/assets/schemas/stage_3_3_1_informationsverbund_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Subchapter 3.3.1 Definition des Informationsverbundes",
  "type": "object",
  "properties": {
    "answers": {
      "type": "array",
      "description": "The answers to the three questions regarding the definition of the informational asset network.",
      "items": {
        "type": "boolean"
      },
      "minItems": 3,
      "maxItems": 3
    },
    "finding": {
      "type": "object",
      "description": "A structured finding based on the analysis.",
      "properties": {
        "category": {
          "type": "string",
          "description": "The category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK' (No Deviation).",
          "enum": ["AG", "AS", "E", "OK"]
        },
        "description": {
          "type": "string",
          "description": "A detailed description of the finding."
        }
      },
      "required": ["category", "description"]
    }
  },
  "required": ["answers", "finding"]
}
==== bsi-audit-automator/assets/schemas/stage_3_3_2_netzplan_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Chapter 3.3.2 Netzplan",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "Answers to the questions about the network plan.",
        "items": {
          "type": "boolean"
        },
        "minItems": 2,
        "maxItems": 2
      },
      "finding": {
        "type": "object",
        "properties": {
          "category": {
            "type": "string",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string"
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_3_3_3_geschaeftsprozesse_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Chapter 3.3.3 Liste der Geschäftsprozesse",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "Answer to the question about business processes.",
        "items": {
          "type": "boolean"
        },
        "minItems": 1,
        "maxItems": 1
      },
      "finding": {
        "type": "object",
        "properties": {
          "category": {
            "type": "string",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string"
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_3_4_1_schutzbedarfskategorien_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Chapter 3.4.1 Definition der Schutzbedarfskategorien",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "Answers to the questions about protection requirement categories.",
        "items": {
          "type": "boolean"
        },
        "minItems": 2,
        "maxItems": 2
      },
      "finding": {
        "type": "object",
        "properties": {
          "category": {
            "type": "string",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string"
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_3_5_1_modellierungsdetails_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Chapter 3.5.1 Modellierungsdetails",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "Answers to the questions about modeling details.",
        "items": {
          "type": "boolean"
        },
        "minItems": 4,
        "maxItems": 4
      },
      "finding": {
        "type": "object",
        "properties": {
          "category": {
            "type": "string",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string"
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_3_6_1_extract_check_data_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Extracted Grundschutz-Check Data",
  "description": "A schema to hold the structured list of requirements and chapter headings extracted from a Grundschutz-Check document chunk.",
  "type": "object",
  "properties": {
    "anforderungen": {
      "type": "array",
      "description": "A list of all security requirements found in the document chunk.",
      "items": {
        "type": "object",
        "properties": {
          "id": {
            "type": "string",
            "description": "The full ID of the requirement, e.g., 'ISMS.1.A1'."
          },
          "titel": {
            "type": "string",
            "description": "The title of the requirement."
          },
          "umsetzungsstatus": {
            "type": "string",
            "description": "The implementation status, normalized (e.g., 'Ja', 'Nein', 'teilweise', 'entbehrlich')."
          },
          "umsetzungserlaeuterung": {
            "type": "string",
            "description": "The full text of the implementation explanation."
          },
          "datumLetztePruefung": {
            "type": "string",
            "description": "The date of the last check, extracted as seen in the document. The fallback for a missing date is '1970-01-01'."
          },
          "pagenumber": {
            "type": "integer",
            "description": "The page number where this requirement was found."
          },
          "zielobjekt_kuerzel": {
            "type": "string",
            "description": "The Kürzel of the Zielobjekt, if directly associated on the same page by the AI."
          },
          "zielobjekt_name": {
            "type": "string",
            "description": "The Name of the Zielobjekt, if directly associated on the same page by the AI."
          }
        },
        "required": [
          "id",
          "umsetzungsstatus",
          "umsetzungserlaeuterung",
          "datumLetztePruefung",
          "pagenumber"
        ]
      }
    }

  },
  "required": ["anforderungen"]
}
==== bsi-audit-automator/assets/schemas/stage_3_7_risikoanalyse_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Risikoanalyse Response",
    "description": "A schema for answering the 4 questions of the risk analysis section and providing a single audit finding.",
    "type": "object",
    "properties": {
      "answers": {
        "type": "array",
        "description": "An array of 4 answers, one for each question asked. The order must be preserved.",
        "items": { "type": "string" },
        "minItems": 4,
        "maxItems": 4
      },
      "finding": {
        "type": "object",
        "description": "A single, structured finding for this section.",
        "properties": {
          "category": {
            "type": "string",
            "description": "Category of the finding: 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation), or 'OK'.",
            "enum": ["AG", "AS", "E", "OK"]
          },
          "description": {
            "type": "string",
            "description": "A clear, concise description of the finding, including evidence and document reference."
          }
        },
        "required": ["category", "description"]
      }
    },
    "required": ["answers", "finding"]
  }
==== bsi-audit-automator/assets/schemas/stage_3_9_ergebnis_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Subchapter 3.9 Ergebnis der Dokumentenpruefung",
  "type": "object",
  "properties": {
    "answers": {
      "type": "array",
      "description": "The single boolean answer to the question of whether the audit can proceed.",
      "items": {
        "type": "boolean"
      },
      "minItems": 1,
      "maxItems": 1
    },
    "votum": {
      "type": "string",
      "description": "The summary verdict or 'Votum' for the document review phase."
    }
  },
  "required": ["answers", "votum"]
}
==== bsi-audit-automator/assets/schemas/stage_3_gt_baustein_mappings_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Baustein-to-Zielobjekt Mappings",
    "description": "Validates the list of mappings extracted from the Modellierung document (A.3).",
    "type": "object",
    "properties": {
        "mappings": {
            "type": "array",
            "description": "A list of mappings.",
            "items": {
                "type": "object",
                "properties": {
                    "baustein_id": {
                        "type": "string",
                        "description": "The ID of the Baustein, e.g., 'SYS.1.1'."
                    },
                    "zielobjekt_kuerzel": {
                        "type": "string",
                        "description": "The unique ID (Kürzel) of the Zielobjekt it is mapped to."
                    }
                },
                "required": ["baustein_id", "zielobjekt_kuerzel"]
            }
        }
    },
    "required": ["mappings"]
}
==== bsi-audit-automator/assets/schemas/stage_3_gt_zielobjekte_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Zielobjekte Extraction",
    "description": "Validates the list of Zielobjekte extracted from the Strukturanalyse document (A.1).",
    "type": "object",
    "properties": {
        "zielobjekte": {
            "type": "array",
            "description": "A list of all target objects (Zielobjekte).",
            "items": {
                "type": "object",
                "properties": {
                    "kuerzel": {
                        "type": "string",
                        "description": "The unique, customer-defined ID of the Zielobjekt, e.g., 'A-001'."
                    },
                    "name": {
                        "type": "string",
                        "description": "The full, descriptive name of the Zielobjekt, e.g., 'Main Web Server'."
                    }
                },
                "required": ["kuerzel", "name"]
            }
        }
    },
    "required": ["zielobjekte"]
}
==== bsi-audit-automator/assets/schemas/stage_3_summary_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for a generic summary/votum subchapter",
    "type": "object",
    "properties": {
      "votum": {
        "type": "string",
        "description": "A summary verdict based on the provided findings."
      }
    },
    "required": ["votum"]
  }
==== bsi-audit-automator/assets/schemas/stage_4_1_1_auswahl_bausteine_erst_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Subchapter 4.1.1 Auswahl Bausteine Überwachung",
  "type": "object",
  "properties": {
    "rows": {
      "type": "array",
      "description": "A list of Bausteine selected for the audit.",
      "items": {
        "type": "object",
        "properties": {
          "Schicht": { "type": "string", "description": "e.g., 'ISMS', 'ORP', 'INF'" },
          "Baustein": { "type": "string", "description": "The full Baustein ID and name, e.g., 'ISMS.1 Sicherheitsmanagement'" },
          "Zielobjekt-Name": { "type": "string", "description": "The descriptive name of the target object, e.g., 'Gesamter Informationsverbund'" },
          "Zielobjekt-Kürzel": { "type": "string", "description": "The unique ID (Kürzel) of the target object." },
          "Begründung zur Auswahl": { "type": "string", "description": "Justification for selecting this Baustein for the audit." }
        },
        "required": [
          "Schicht",
          "Baustein",
          "Zielobjekt-Name",
          "Zielobjekt-Kürzel",
          "Begründung zur Auswahl"
        ]
      }
    }
  },
  "required": ["rows"]
}
==== bsi-audit-automator/assets/schemas/stage_4_1_2_auswahl_bausteine_ueberwachung_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Subchapter 4.1.2 Auswahl Bausteine Überwachung",
    "type": "object",
    "properties": {
      "rows": {
        "type": "array",
        "description": "A list of Bausteine selected for the audit.",
        "items": {
          "type": "object",
          "properties": {
            "Schicht": { "type": "string", "description": "e.g., 'ISMS', 'ORP', 'INF'" },            
            "Baustein": { "type": "string", "description": "The full Baustein ID and name, e.g., 'ISMS.1 Sicherheitsmanagement'" },
            "Zielobjekt-Name": { "type": "string", "description": "The descriptive name of the target object, e.g., 'Gesamter Informationsverbund'" },            "Zielobjekt-Kürzel": { "type": "string", "description": "The unique ID (Kürzel) of the target object." },
            "Begründung zur Auswahl": { "type": "string", "description": "Justification for selecting this Baustein for the audit." }
          },
           "required": [
            "Schicht",
            "Baustein",
            "Zielobjekt-Name",
            "Zielobjekt-Kürzel",
            "Begründung zur Auswahl"
          ]
        }
      }
    },
    "required": ["rows"]
  }
==== bsi-audit-automator/assets/schemas/stage_4_1_3_auswahl_bausteine_ueberwachung_schema.json ====
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Schema for Subchapter 4.1.3 Auswahl Bausteine Überwachung",
  "type": "object",
  "properties": {
    "rows": {
      "type": "array",
      "description": "A list of Bausteine selected for the audit.",
      "items": {
        "type": "object",
        "properties": {
          "Schicht": { "type": "string", "description": "e.g., 'ISMS', 'ORP', 'INF'" },            
          "Baustein": { "type": "string", "description": "The full Baustein ID and name, e.g., 'ISMS.1 Sicherheitsmanagement'" },
          "Zielobjekt-Name": { "type": "string", "description": "The descriptive name of the target object, e.g., 'Gesamter Informationsverbund'" },            "Zielobjekt-Kürzel": { "type": "string", "description": "The unique ID (Kürzel) of the target object." },
          "Begründung zur Auswahl": { "type": "string", "description": "Justification for selecting this Baustein for the audit." }
        },
         "required": [
          "Schicht",
          "Baustein",
          "Zielobjekt-Name",
          "Zielobjekt-Kürzel",
          "Begründung zur Auswahl"
        ]
      }
    }
  },
  "required": ["rows"]
}
==== bsi-audit-automator/assets/schemas/stage_4_1_5_auswahl_massnahmen_risiko_schema.json ====
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Schema for Subchapter 4.1.5 Auswahl Maßnahmen aus der Risikoanalyse",
    "type": "object",
    "properties": {
      "rows": {
        "type": "array",
        "description": "A list of security measures selected from the risk analysis for on-site verification.",
        "items": {
          "type": "object",
          "properties": {
            "Maßnahme": { "type": "string", "description": "A concise name for the security measure (e.g., 'Advanced DDoS Protection', 'Database Field-Level Encryption')." },
            "Risikoanalyse": { "type": "string", "description": "Reference to the risk analysis document or section (e.g., 'RA-2023-Web-App', 'Risikoanalyse PII-Datenbank')." },
            "Zielobjekt": { "type": "string", "description": "The target system or asset the measure applies to (e.g., 'Web Application Cluster', 'CRM Customer Database')." },
            "Begründung zur Auswahl": { "type": "string", "description": "The risk-based justification for selecting this measure for the audit." }
          },
          "required": ["Maßnahme", "Risikoanalyse", "Zielobjekt", "Begründung zur Auswahl"]
        }
      }
    },
    "required": ["rows"]
  }
==== bsi-audit-automator/check_import.py ====
from google.cloud import documentai
from google.api_core.client_options import ClientOptions

location = "us"  # or your region, e.g., "eu"
opts = ClientOptions(api_endpoint=f"{location}-documentai.googleapis.com")
client = documentai.DocumentProcessorServiceClient(client_options=opts)
print("Client initialized successfully!")

==== bsi-audit-automator/delete_old_prompts.sh ====
#!/bin/bash
# Deletes the now-obsolete individual prompt text files.

echo "Deleting old prompt files from ./assets/prompts/ ..."

rm -f ./assets/prompts/etl_classify_documents.txt
rm -f ./assets/prompts/generic_question_prompt.txt
rm -f ./assets/prompts/generic_summary_prompt.txt
rm -f ./assets/prompts/stage_1_2_geltungsbereich.txt
rm -f ./assets/prompts/stage_1_4_informationsverbund.txt
rm -f ./assets/prompts/stage_3_1_aktualitaet.txt
rm -f ./assets/prompts/stage_3_2_sicherheitsleitlinie.txt
rm -f ./assets/prompts/stage_3_3_1_informationsverbund.txt
rm -f ./assets/prompts/stage_3_3_2_netzplan.txt
rm -f ./assets/prompts/stage_3_3_3_geschaeftsprozesse.txt
rm -f ./assets/prompts/stage_3_4_1_schutzbedarfskategorien.txt
rm -f ./assets/prompts/stage_3_5_1_modellierungsdetails.txt
rm -f ./assets/prompts/stage_3_5_2_ergebnis_modellierung.txt
rm -f ./assets/prompts/stage_3_6_1_extract_check_data.txt
rm -f ./assets/prompts/stage_3_6_1_grundschutz_check.txt
rm -f ./assets/prompts/stage_3_9_ergebnis.txt
rm -f ./assets/prompts/stage_4_1_1_auswahl_bausteine_erst.txt
rm -f ./assets/prompts/stage_4_1_2_auswahl_bausteine_ueberwachung.txt
rm -f ./assets/prompts/stage_4_1_5_auswahl_massnahmen_risiko.txt
rm -f ./assets/prompts/stage_7_2_abweichungen.txt

# Also remove the now-empty directory
rmdir ./assets/prompts 2>/dev/null || true

echo "Deletion complete."
==== bsi-audit-automator/envs.sh ====
#!/bin/bash
#
# DYNAMIC Environment variable setup for local BSI Audit Automator development.
#
# This script dynamically fetches configuration from your Terraform state,
# ensuring your local environment matches the cloud deployment.
#
# It also defines a helper function `bsi-auditor` to simplify running the app.
#
# PREREQUISITES:
#   - You must have run 'terraform apply' in the ./terraform directory.
#   - You must have the 'terraform' CLI installed and in your PATH.
#
# USAGE:
#   Run this command from the project root (the 'bsi-audit-automator' directory):
#      source ./envs.sh
#
#   Then, you can run the application like this:
#      bsi-auditor --run-etl
#      bsi-auditor --run-stage Chapter-1
#
set -e # Exit on error

TERRAFORM_DIR="../terraform"

if [ ! -d "$TERRAFORM_DIR" ]; then
    echo "❌ Error: Terraform directory not found at '$TERRAFORM_DIR'. Please run this script from the project root."
    return 1
fi
if ! command -v terraform &> /dev/null; then
    echo "❌ Error: 'terraform' command not found. Please install Terraform."
    return 1
fi

echo "🔹 Fetching infrastructure details from Terraform..."

# --- Dynamic Values from Terraform ---
export GCP_PROJECT_ID="$(terraform -chdir=${TERRAFORM_DIR} output -raw project_id)"
export REGION="$(terraform -chdir=${TERRAFORM_DIR} output -raw region)"
export BUCKET_NAME="$(terraform -chdir=${TERRAFORM_DIR} output -raw gcs_bucket_name)"
export DOC_AI_PROCESSOR_NAME="$(terraform -chdir=${TERRAFORM_DIR} output -raw documentai_processor_name)"
# NEW: Fetch the public domain if it exists, otherwise set to empty string.

# --- Static Values for Local Development ---
# These prefixes now reflect the simpler GCS layout.
export SOURCE_PREFIX="source_documents/"
export OUTPUT_PREFIX="output/"

# Manually set the audit type and test mode for your local run
export AUDIT_TYPE="2. Überwachungsaudit"
export TEST="true"
export MAX_CONCURRENT_AI_REQUESTS=5 # New: Tunable concurrency limit

# --- NEW: Helper function for correct execution ---
# This alias ensures we always run the application as a module,
# which correctly resolves the relative imports in src/main.py.
bsi-auditor() {
    python -m src.main "$@"
}


set +e
echo "✅ Environment variables configured successfully'."
echo "   - GCP_PROJECT_ID: ${GCP_PROJECT_ID}"
echo "   - BUCKET_NAME:    ${BUCKET_NAME}"
echo "   - DOC_AI_PROC:    ${DOC_AI_PROCESSOR_NAME}"
echo "   - AUDIT_TYPE:     ${AUDIT_TYPE}"
echo "   - TEST mode:      ${TEST}"
echo ""
echo "👉 A new command 'bsi-auditor' is now available in your shell."
echo "   Run the app with: bsi-auditor --run-stage Chapter-1"
==== bsi-audit-automator/requirements.txt ====
# GCP and Vertex AI
google-cloud-storage
google-cloud-aiplatform
google-cloud-documentai

# For local development, to load .env files
python-dotenv
jsonschema # For validating AI model outputs
PyMuPDF # For PDF processing (provides 'fitz' module)
==== bsi-audit-automator/src/audit/controller.py ====
# src/audit/controller.py
import logging
import json
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict
from google.cloud.exceptions import NotFound

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient
from src.clients.document_ai_client import DocumentAiClient
from src.clients.rag_client import RagClient
from src.constants import STAGE_RESULTS_PATH, ALL_FINDINGS_PATH, EXTRACTED_CHECK_DATA_PATH, GROUND_TRUTH_MAP_PATH
from src.audit.stages.stage_previous_report_scan import PreviousReportScanner
from src.audit.stages.stage_1_general import Chapter1Runner
from src.audit.stages.stage_3_dokumentenpruefung import Chapter3Runner
from src.audit.stages.stage_4_pruefplan import Chapter4Runner
from src.audit.stages.stage_5_vor_ort_audit import Chapter5Runner
from src.audit.stages.stage_7_anhang import Chapter7Runner
from src.audit.stages.stage_gs_check_extraction import GrundschutzCheckExtractionRunner

class AuditController:
    """Orchestrates the entire staged audit process with lazy initialization of runners."""

    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self.rag_client = rag_client
        self.all_findings: List[Dict[str, Any]] = []
        self.finding_counters = defaultdict(int)

        self.stage_runner_classes = {
            "Scan-Report": PreviousReportScanner,
            "Grundschutz-Check-Extraction": GrundschutzCheckExtractionRunner,
            "Chapter-1": Chapter1Runner,
            "Chapter-3": Chapter3Runner,
            "Chapter-4": Chapter4Runner,
            "Chapter-5": Chapter5Runner,
            "Chapter-7": Chapter7Runner,
        }
        # This defines the exact order of dependencies for each runner's constructor.
        self.runner_dependencies = {
            "Scan-Report": (self.config, self.ai_client, self.rag_client),
            "Grundschutz-Check-Extraction": (self.config, self.gcs_client, None, self.ai_client, self.rag_client), # Placeholder for doc_ai_client
            "Chapter-1": (self.config, self.ai_client, self.rag_client),
            "Chapter-3": (self.config, self.gcs_client, self.ai_client, self.rag_client),
            "Chapter-4": (self.config, self.gcs_client, self.ai_client),
            "Chapter-5": (self.config, self.gcs_client, self.ai_client),
            "Chapter-7": (self.config, self.gcs_client),
        }
        logging.info("Audit Controller initialized with lazy stage loading and findings collector.")

    def _parse_finding_id(self, finding_id: str) -> Tuple[Optional[str], int]:
        """Parses a finding ID like 'AG-12' into its category 'AG' and number 12."""
        if not finding_id or '-' not in finding_id:
            return None, 0
        parts = finding_id.split('-')
        category = parts[0]
        try:
            num = int(parts[-1])
            return category, num
        except (ValueError, IndexError):
            return None, 0

    def _process_previous_findings(self, previous_findings: List[Dict[str, Any]]):
        """Processes findings from a previous report scan, preserving their IDs and updating counters."""
        logging.info(f"Processing {len(previous_findings)} findings from previous audit report.")
        for finding in previous_findings:
            finding_id = finding.get("nummer")
            if not finding_id:
                continue

            category, num = self._parse_finding_id(finding_id)
            if category and num > 0:
                # Update the counter to the highest number seen for this category
                self.finding_counters[category] = max(self.finding_counters[category], num)

            # Add the finding to the central list with its ID and details preserved
            self.all_findings.append({
                "id": finding_id,
                "category": finding.get("category"),
                "description": finding.get("beschreibung", "No description provided."),
                "source_chapter": f"Previous Audit ({finding.get('quelle', 'N/A')})",
                "status": finding.get("status"),
                "behebungsfrist": finding.get("behebungsfrist")
            })

    def _process_new_finding(self, finding: Dict[str, Any], stage_name: str):
        """Processes a newly generated finding, adding it to the central list to await ID assignment."""
        source_ref = stage_name.replace('Chapter-', '')
        # Add to central list without an ID, which will be assigned at the end.
        self.all_findings.append({
            "category": finding.get("category"),
            "description": finding.get("description"),
            "source_chapter": source_ref
        })
        logging.info(f"Collected new finding from {stage_name}: {finding.get('category')}")

    def _extract_findings_recursive(self, data: Any) -> List[Dict[str, Any]]:
        """
        Recursively traverses a data structure to find all structured `finding` objects.
        Returns a flat list of all findings discovered. This method does NOT handle
        the `all_findings` key from Scan-Report, as that is handled separately.
        """
        found = []
        if isinstance(data, dict):
            if 'finding' in data and isinstance(data['finding'], dict):
                finding_obj = data['finding']
                if finding_obj and finding_obj.get('category') != 'OK':
                    found.append(finding_obj)
            
            for value in data.values():
                found.extend(self._extract_findings_recursive(value))
        
        elif isinstance(data, list):
            for item in data:
                found.extend(self._extract_findings_recursive(item))
        
        return found

    def _extract_and_store_findings(self, stage_name: str, result_data: Dict[str, Any]) -> None:
        """
        Parses stage results, finds all structured `finding` objects recursively,
        and adds them to the central collection.
        """
        if not result_data:
            return

        # Special handling for Scan-Report which has a flat list of previous findings
        if stage_name == "Scan-Report" and 'all_findings' in result_data:
            self._process_previous_findings(result_data['all_findings'])
            # We don't do a recursive search for this stage type to avoid double counting
            return

        # For the extraction stage, there are no findings to process.
        if stage_name == "Grundschutz-Check-Extraction":
            return

        # Standard recursive search for newly generated findings
        newly_discovered_findings = self._extract_findings_recursive(result_data)
        for finding in newly_discovered_findings:
            self._process_new_finding(finding, stage_name)

    def _save_all_findings(self) -> None:
        """
        Saves the centrally collected list of all findings. It preserves existing IDs
        from previous reports and assigns new, sequential IDs for new findings.
        """
        if not self.all_findings:
            logging.info("No findings were collected during the audit. Skipping save.")
            return

        findings_with_ids = []
        for finding in self.all_findings:
            if 'id' in finding and finding['id']:
                # This is a finding from a previous report or an earlier run, ID is already set.
                findings_with_ids.append(finding)
            else:
                # This is a new finding, assign a new ID.
                category = finding['category']
                self.finding_counters[category] += 1
                finding_id = f"{category}-{self.finding_counters[category]}"
                
                # Add the new ID to the finding object
                finding_with_id = {"id": finding_id, **finding}
                findings_with_ids.append(finding_with_id)

        findings_path = f"{self.config.output_prefix}results/all_findings.json"
        self.gcs_client.upload_from_string(
            content=json.dumps(findings_with_ids, indent=2, ensure_ascii=False),
            destination_blob_name=findings_path
        )
        logging.info(f"Successfully saved {len(findings_with_ids)} findings with sequential IDs to {findings_path}")

    async def run_all_stages(self, force_overwrite: bool = False) -> None:
        """
        Runs all defined audit stages in a dependency-aware order. Each stage run
        will update and persist the central findings list.
        """
        # Step 0: Run the critical pre-processing step first.
        logging.info("Step 0: Running pre-processing stage 'Grundschutz-Check-Extraction'...")
        await self.run_single_stage("Grundschutz-Check-Extraction", force_overwrite=force_overwrite)
        logging.info("Completed pre-processing.")

        # Step 1: Run initial independent stages in parallel.
        initial_parallel_stages = ["Scan-Report", "Chapter-1", "Chapter-3", "Chapter-7"]
        logging.info(f"Step 1: Starting parallel execution for initial stages: {initial_parallel_stages}")
        await asyncio.gather(
            *(self.run_single_stage(stage_name, force_overwrite=force_overwrite) for stage_name in initial_parallel_stages)
        )
        logging.info("Completed initial parallel stages.")

        # Step 2: Run Chapter 4, which depends on Chapter 3's ground-truth map.
        logging.info("Step 2: Running stage Chapter-4...")
        await self.run_single_stage("Chapter-4", force_overwrite=force_overwrite)
        logging.info("Completed stage Chapter-4.")

        # Step 3: Run Chapter 5, which depends on Chapter 4's plan and Chapter 3's data.
        logging.info("Step 3: Running stage Chapter-5...")
        await self.run_single_stage("Chapter-5", force_overwrite=force_overwrite)
        logging.info("Completed stage Chapter-5.")
        
        logging.info("All audit stages completed.")

    async def run_single_stage(self, stage_name: str, force_overwrite: bool = False) -> Dict[str, Any]:
        """
        Runs a single, specified audit stage. It manages the central findings list by
        loading it, removing any previous findings from this specific stage, running
        the stage (or skipping if results exist), adding the new/existing findings
        back, and saving the updated central list.
        """
        if stage_name not in self.stage_runner_classes:
            logging.error(f"Unknown stage '{stage_name}'. Available: {list(self.stage_runner_classes.keys())}")
            raise ValueError(f"Unknown stage: {stage_name}")

        # 1. Load and filter the central findings list
        findings_path = ALL_FINDINGS_PATH
        current_findings = []
        try:
            if self.gcs_client.blob_exists(findings_path):
                current_findings = self.gcs_client.read_json(findings_path)
        except Exception as e:
            logging.warning(f"Could not load or parse existing findings file: {e}. Starting with an empty list.")

        source_ref_to_remove = stage_name.replace('Chapter-', '')
        
        if stage_name == "Scan-Report":
            self.all_findings = [f for f in current_findings if not str(f.get("source_chapter", "")).startswith("Previous Audit")]
        else:
            self.all_findings = [f for f in current_findings if f.get("source_chapter") != source_ref_to_remove]
        
        self.finding_counters = defaultdict(int)
        for finding in self.all_findings:
            category, num = self._parse_finding_id(finding.get("id"))
            if category and num > 0:
                self.finding_counters[category] = max(self.finding_counters[category], num)

        # 2. Execute the stage logic
        stage_output_path = STAGE_RESULTS_PATH.format(stage_name=stage_name)
        result_data = None

        if not force_overwrite:
            try:
                if stage_name == "Grundschutz-Check-Extraction":
                    if self.gcs_client.blob_exists(EXTRACTED_CHECK_DATA_PATH) and \
                       self.gcs_client.blob_exists(GROUND_TRUTH_MAP_PATH):
                        logging.info(f"Stage '{stage_name}' already completed (intermediate files exist). Skipping.")
                        result_data = {"status": "skipped", "reason": "intermediate files found"}
                else:
                    result_data = self.gcs_client.read_json(stage_output_path)
                    logging.info(f"Stage '{stage_name}' already completed. Skipping generation.")
            except NotFound:
                logging.info(f"No results for stage '{stage_name}' found. Generating...")
            except Exception as e:
                logging.warning(f"Could not read existing state for stage '{stage_name}': {e}. Proceeding.")

        if result_data is None:
            logging.info(f"Running generation for stage '{stage_name}'.")
            runner_class = self.stage_runner_classes[stage_name]
            
            if stage_name == "Grundschutz-Check-Extraction":
                doc_ai_client = DocumentAiClient(self.config, self.gcs_client)
                dependencies = (self.config, self.gcs_client, doc_ai_client, self.ai_client, self.rag_client)
            else:
                dependencies = self.runner_dependencies[stage_name]
            stage_runner = runner_class(*dependencies)
            logging.info(f"Initialized runner for stage: {stage_name}")

            try:
                result_data = await stage_runner.run(force_overwrite=force_overwrite)

                if stage_name != "Grundschutz-Check-Extraction":
                    self.gcs_client.upload_from_string(
                        content=json.dumps(result_data, indent=2, ensure_ascii=False),
                        destination_blob_name=stage_output_path
                    )
                    logging.info(f"Successfully saved results for stage '{stage_name}'.")
            except Exception as e:
                logging.error(f"Stage '{stage_name}' failed: {e}", exc_info=True)
                self._save_all_findings() # Save findings state even on failure
                raise

        # 3. Process findings from the result (either newly generated or from the skipped file)
        self._extract_and_store_findings(stage_name, result_data)

        # 4. Save the final, updated list of all findings
        self._save_all_findings()
        
        return result_data
==== bsi-audit-automator/src/audit/report_generator.py ====
# src/audit/report_generator.py
import logging
import json
import asyncio
from google.cloud.exceptions import NotFound
from typing import Dict, Any, List
from jsonschema import validate, ValidationError

from datetime import datetime

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.constants import FINAL_REPORT_PATH, ALL_FINDINGS_PATH, STAGE_RESULTS_PATH

class ReportGenerator:
    """Assembles the final audit report from individual stage stubs."""
    LOCAL_MASTER_TEMPLATE_PATH = "assets/json/master_report_template.json"
    STAGES_TO_AGGREGATE = ["Scan-Report", "Chapter-1", "Chapter-3", "Chapter-4", "Chapter-5", "Chapter-7"]

    def __init__(self, config: AppConfig, gcs_client: GcsClient):
        self.config = config
        self.gcs_client = gcs_client
        self.report_schema = self._load_report_schema()
        logging.info("Report Generator initialized.")
    
    def _load_report_schema(self) -> Dict[str, Any]:
        """Loads the master template to use as a validation schema."""
        try:
            with open(self.LOCAL_MASTER_TEMPLATE_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logging.error(f"FATAL: Could not load the master report schema from {self.LOCAL_MASTER_TEMPLATE_PATH}. Error: {e}")
            raise

    def _set_value_by_path(self, report: Dict, path: str, value: Any):
        """
        Safely sets a value in a nested dictionary using a dot-separated path.
        This is more robust than sequential `get` calls.
        """
        keys = path.split('.')
        current_level = report
        for i, key in enumerate(keys[:-1]):
            if not isinstance(current_level, dict):
                logging.warning(f"Path part '{key}' is not a dict in path '{path}'. Cannot set value.")
                return
            if key not in current_level:
                # Create missing dictionary keys if they don't exist
                current_level[key] = {}
            current_level = current_level[key]
        
        if isinstance(current_level, dict):
            current_level[keys[-1]] = value
        else:
            logging.warning(f"Target for path '{path}' is not a dictionary. Cannot set final key '{keys[-1]}'.")

    def _ensure_list_path_exists(self, report: Dict, path: str, min_length: int = 1, default_item: Dict = None) -> List:
        """
        Ensures a list at a given path exists and has a minimum length, padding it if necessary.
        Returns the list object for modification.
        """
        if default_item is None:
            default_item = {"type": "prose", "text": ""}

        keys = path.split('.')
        current_level = report
        for key in keys:
            if not isinstance(current_level, dict):
                logging.warning(f"Path part is not a dict in path '{path}' at key '{key}'. Cannot ensure list path.")
                return []
            if key not in current_level:
                current_level[key] = [] if key == keys[-1] else {}
            current_level = current_level[key]

        if not isinstance(current_level, list):
            logging.warning(f"Target for path '{path}' is not a list. Cannot pad.")
            return []
        
        while len(current_level) < min_length:
            current_level.append(default_item.copy())
            
        return current_level

    def _load_local_report_template(self) -> dict:
        """
        Loads the pristine report template from the local assets folder and
        injects initial configuration like the audit type.
        """
        logging.info(f"Loading pristine report template from local asset: {self.LOCAL_MASTER_TEMPLATE_PATH}")
        try:
            with open(self.LOCAL_MASTER_TEMPLATE_PATH, 'r', encoding='utf-8') as f:
                report = json.load(f)
            
            # Inject dynamic configuration into the fresh template
            self._set_value_by_path(report, 'bsiAuditReport.allgemeines.audittyp.content', self.config.audit_type)
            
            logging.info("Successfully loaded and configured local report template.")
            return report
        except Exception as e:
            logging.error(f"FATAL: Could not load the master report template from {self.LOCAL_MASTER_TEMPLATE_PATH}. Error: {e}")
            raise

    def _populate_chapter_1(self, report: dict, stage_data: dict) -> None:
        """Populates the 'Allgemeines' (Chapter 1) of the report defensively."""
        # Populate 1.4 Informationsverbund
        informationsverbund_data = stage_data.get('informationsverbund', {})
        if informationsverbund_data:
            path_prefix = 'bsiAuditReport.allgemeines.informationsverbund.content'
            content_list = self._ensure_list_path_exists(report, path_prefix, min_length=2)
            if content_list:
                content_list[0]['text'] = informationsverbund_data.get('kurzbezeichnung', '')
                content_list[1]['text'] = informationsverbund_data.get('kurzbeschreibung', '')

        # Populate 1.5 Audittyp
        audittyp_content = stage_data.get('audittyp', {}).get('content', self.config.audit_type)
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.audittyp.content', audittyp_content)

    async def assemble_report(self) -> None:
        """
        Main method to assemble the final report.
        """
        report = self._load_local_report_template()

        stage_read_tasks = [self.gcs_client.read_json_async(STAGE_RESULTS_PATH.format(stage_name=s)) for s in self.STAGES_TO_AGGREGATE]
        stage_results = await asyncio.gather(*stage_read_tasks, return_exceptions=True)
        
        stage_data_map = {}
        for i, result in enumerate(stage_results):
            stage_name = self.STAGES_TO_AGGREGATE[i]
            if isinstance(result, Exception):
                logging.warning(f"Result for stage '{stage_name}' not found or failed to load. Skipping. Error: {result}")
            # Add a check to ensure stage_data is a dictionary before processing.
            elif not result or not isinstance(result, dict):
                logging.warning(f"Result for stage '{stage_name}' is empty or not a dictionary. Skipping.")
            else:
                stage_data_map[stage_name] = result

        # The order of population matters. Populate from Scan-Report first as a baseline.
        if "Scan-Report" in stage_data_map:
            self._populate_from_scan_report(report, stage_data_map["Scan-Report"])

        # Then, let the other stages overwrite with fresher, generated data.
        for stage_name, stage_data in stage_data_map.items():
            if stage_name != "Scan-Report": # Avoid running it twice
                self._populate_report(report, stage_name, stage_data)
        
        # Populate the final aggregated findings last.
        self._populate_chapter_7_findings(report)

        try:
            validate(instance=report, schema=self.report_schema)
            logging.info("Final report successfully validated against the master schema.")
        except ValidationError as e:
            logging.error(f"CRITICAL: Final report failed schema validation. Report will not be saved. Error: {e.message}")
            return

        today = datetime.now()
        date_str = today.strftime("%y%m%d")
        
        await self.gcs_client.upload_from_string_async(
            content=json.dumps(report, indent=2, ensure_ascii=False),
            destination_blob_name=FINAL_REPORT_PATH
        )
        logging.info(f"Successfully generated final audit report at {FINAL_REPORT_PATH}")
        

    def _populate_chapter_3(self, report: dict, stage_data: dict) -> None:
        """Populates Chapter 3 (Dokumentenprüfung) content into the report."""
        base_path = "bsiAuditReport.dokumentenpruefung"
        key_to_path_map = {
            "aktualitaetDerReferenzdokumente": f"{base_path}.aktualitaetDerReferenzdokumente",
            "sicherheitsleitlinieUndRichtlinienInA0": f"{base_path}.sicherheitsleitlinieUndRichtlinienInA0",
            "definitionDesInformationsverbundes": f"{base_path}.strukturanalyseA1.definitionDesInformationsverbundes",
            "bereinigterNetzplan": f"{base_path}.strukturanalyseA1.bereinigterNetzplan",
            "listeDerGeschaeftsprozesse": f"{base_path}.strukturanalyseA1.listeDerGeschaeftsprozesse",
            "listeDerAnwendungen": f"{base_path}.strukturanalyseA1.listeDerAnwendungen",
            "listeDerItSysteme": f"{base_path}.strukturanalyseA1.listeDerItSysteme",
            "listeDerRaeumeGebaeudeStandorte": f"{base_path}.strukturanalyseA1.listeDerRaeumeGebaeudeStandorte",
            "listeDerKommunikationsverbindungen": f"{base_path}.strukturanalyseA1.listeDerKommunikationsverbindungen",
            "stichprobenDokuStrukturanalyse": f"{base_path}.strukturanalyseA1.stichprobenDokuStrukturanalyse",
            "listeDerDienstleister": f"{base_path}.strukturanalyseA1.listeDerDienstleister",
            "ergebnisDerStrukturanalyse": f"{base_path}.strukturanalyseA1.ergebnisDerStrukturanalyse",
            "definitionDerSchutzbedarfskategorien": f"{base_path}.schutzbedarfsfeststellungA2.definitionDerSchutzbedarfskategorien",
            "schutzbedarfGeschaeftsprozesse": f"{base_path}.schutzbedarfsfeststellungA2.schutzbedarfGeschaeftsprozesse",
            "schutzbedarfAnwendungen": f"{base_path}.schutzbedarfsfeststellungA2.schutzbedarfAnwendungen",
            "schutzbedarfItSysteme": f"{base_path}.schutzbedarfsfeststellungA2.schutzbedarfItSysteme",
            "schutzbedarfRaeume": f"{base_path}.schutzbedarfsfeststellungA2.schutzbedarfRaeume",
            "schutzbedarfKommunikationsverbindungen": f"{base_path}.schutzbedarfsfeststellungA2.schutzbedarfKommunikationsverbindungen",
            "stichprobenDokuSchutzbedarf": f"{base_path}.schutzbedarfsfeststellungA2.stichprobenDokuSchutzbedarf",
            "ergebnisDerSchutzbedarfsfeststellung": f"{base_path}.schutzbedarfsfeststellungA2.ergebnisDerSchutzbedarfsfeststellung",
            "modellierungsdetails": f"{base_path}.modellierungDesInformationsverbundesA3.modellierungsdetails",
            "ergebnisDerModellierung": f"{base_path}.modellierungDesInformationsverbundesA3.ergebnisDerModellierung",
            "detailsZumItGrundschutzCheck": f"{base_path}.itGrundschutzCheckA4.detailsZumItGrundschutzCheck",
            "benutzerdefinierteBausteine": f"{base_path}.itGrundschutzCheckA4.benutzerdefinierteBausteine",
            "ergebnisItGrundschutzCheck": f"{base_path}.itGrundschutzCheckA4.ergebnisItGrundschutzCheck",
            "risikoanalyse": f"{base_path}.risikoanalyseA5.risikoanalyse",
            "realisierungsplan": f"{base_path}.realisierungsplanA6.realisierungsplan",
            "ergebnisDerDokumentenpruefung": f"{base_path}.ergebnisDerDokumentenpruefung",
        }

        for subchapter_key, result in stage_data.items():
            if not isinstance(result, dict): continue
            
            target_path = key_to_path_map.get(subchapter_key)
            if not target_path: continue

            if 'finding' in result and isinstance(result.get('finding'), dict):
                finding = result['finding']
                finding_text = f"[{finding.get('category')}] {finding.get('description')}"
                finding_list = self._ensure_list_path_exists(report, f"{target_path}.content")
                if finding_list:
                    for item in finding_list:
                        if item.get("type") == "finding":
                            item["findingText"] = finding_text; break
            
            if "answers" in result:
                answers = result.get("answers", [])
                content_list = self._ensure_list_path_exists(report, f"{target_path}.content", len(answers))
                if content_list:
                    answer_idx = 0
                    for item in content_list:
                        if item.get("type") == "question":
                            if answer_idx < len(answers):
                                item["answer"] = answers[answer_idx]; answer_idx += 1

            if "votum" in result:
                content_list = self._ensure_list_path_exists(report, f"{target_path}.content")
                if content_list:
                    for item in content_list:
                        if item.get("type") == "prose": item["text"] = result.get("votum", ""); break
            
            if "table" in result and isinstance(result.get("table"), dict):
                self._set_value_by_path(report, f"{target_path}.table.rows", result['table'].get('rows', []))

    def _populate_chapter_7_findings(self, report: dict) -> None:
        """
        Populates the findings tables in Chapter 7.2 from the central findings file,
        ensuring the findings are sorted numerically by their ID.
        """
        logging.info("Populating Chapter 7.2 with collected findings...")
        try:
            all_findings = self.gcs_client.read_json(ALL_FINDINGS_PATH)
        except NotFound:
            logging.warning("Central findings file not found. Chapter 7.2 will be empty.")
            return

        # Use clean local lists for collection
        ag_table_rows, as_table_rows, e_table_rows = [], [], []

        for finding in all_findings:
            category = finding.get('category')
            row_data = {
                "Nummer": finding.get('id', 'N/A'),
                "Quelle (Kapitel)": finding.get('source_chapter', 'N/A')
            }
            if category == 'AG':
                row_data["Beschreibung der Abweichung"] = finding.get('description', 'N/A')                
                if finding.get('status') is not None:
                    row_data["Status"] = finding.get('status', 'Unbekannt')
                    row_data["Behebungsfrist"] = finding.get('behebungsfrist', 'N/A')
                else:
                    row_data["Status"] = "Offen"
                    row_data["Behebungsfrist"] = "30 Tage nach Audit"
                ag_table_rows.append(row_data)
            elif category == 'AS':
                row_data["Beschreibung der Abweichung"] = finding.get('description', 'N/A')                
                if finding.get('status') is not None:
                    row_data["Status"] = finding.get('status', 'Unbekannt')
                    row_data["Behebungsfrist"] = finding.get('behebungsfrist', 'N/A')
                else:
                    row_data["Status"] = "Offen"
                    row_data["Behebungsfrist"] = "Bis zum Abschluss des Audit"
                as_table_rows.append(row_data)
            elif category == 'E':
                row_data["Beschreibung der Empfehlung"] = finding.get('description', 'N/A')                
                if finding.get('status') is not None:
                    row_data["Status"] = finding.get('status', 'Unbekannt')
                    row_data["Behebungsfrist"] = finding.get('behebungsfrist', 'N/A')
                else:
                    row_data["Status"] = "Zur Umsetzung empfohlen"
                    row_data["Behebungsfrist"] = "N/A"
                e_table_rows.append(row_data)

        # --- FIX (Task J): Sort findings numerically by ID ---
        def sort_key(finding_dict: Dict[str, Any]) -> int:
            """Extracts the integer part of a finding ID for sorting."""
            try:
                # 'AG-12' -> '12' -> 12
                return int(finding_dict.get("Nummer", "0").split('-')[-1])
            except (ValueError, IndexError):
                return 0  # Fallback for malformed IDs

        ag_table_rows.sort(key=sort_key)
        as_table_rows.sort(key=sort_key)
        e_table_rows.sort(key=sort_key)
        logging.info("Sorted all findings tables numerically by ID.")
        # --- End of FIX ---

        # Now, set the sorted lists into the report dictionary
        self._set_value_by_path(report, 'bsiAuditReport.anhang.abweichungenUndEmpfehlungen.geringfuegigeAbweichungen.table.rows', ag_table_rows)
        self._set_value_by_path(report, 'bsiAuditReport.anhang.abweichungenUndEmpfehlungen.schwerwiegendeAbweichungen.table.rows', as_table_rows)
        self._set_value_by_path(report, 'bsiAuditReport.anhang.abweichungenUndEmpfehlungen.empfehlungen.table.rows', e_table_rows)

        logging.info(f"Populated Chapter 7.2 with {len(all_findings)} total findings.")

    def _populate_chapter_4(self, report: dict, stage_data: dict) -> None:
        """Populates Chapter 4 (Prüfplan) content into the report."""
        base_path = "bsiAuditReport.erstellungEinesPruefplans.auditplanung"
        # This map now consistently points to the 'rows' property inside a 'table' object.
        key_to_path_map = {
            "auswahlBausteineErstRezertifizierung": f"{base_path}.auswahlBausteineErstRezertifizierung.table.rows",
            "auswahlBausteine1Ueberwachungsaudit": f"{base_path}.auswahlBausteine1Ueberwachungsaudit.table.rows",
            "auswahlBausteine2Ueberwachungsaudit": f"{base_path}.auswahlBausteine2Ueberwachungsaudit.table.rows",
            "auswahlStandorte": f"{base_path}.auswahlStandorte.table.rows",
            "auswahlMassnahmenAusRisikoanalyse": f"{base_path}.auswahlMassnahmenAusRisikoanalyse.table.rows"
        }

        for key, data in stage_data.items():
            target_path = key_to_path_map.get(key)
            if not target_path: continue
            
            # Non-destructive update: Only write to the report if the stage data
            # for this section is non-empty. This preserves the baseline from Scan-Report.
            rows_data = data.get('rows', [])
            if rows_data:
                self._set_value_by_path(report, target_path, rows_data)


    def _populate_chapter_5(self, report: dict, stage_data: dict) -> None:
        """Populates Chapter 5 (Vor-Ort-Audit) content into the report."""
        if "verifikationDesITGrundschutzChecks" in stage_data:
            data = stage_data["verifikationDesITGrundschutzChecks"]
            path = "bsiAuditReport.vorOrtAudit.verifikationDesITGrundschutzChecks.einzelergebnisse.bausteinPruefungen"
            self._set_value_by_path(report, path, data.get("einzelergebnisse", {}).get("bausteinPruefungen", []))

        if "risikoanalyseA5" in stage_data:
            data = stage_data["risikoanalyseA5"]
            path = "bsiAuditReport.vorOrtAudit.risikoanalyseA5.einzelergebnisseDerRisikoanalyse.massnahmenPruefungen"
            self._set_value_by_path(report, path, data.get("einzelergebnisseDerRisikoanalyse", {}).get("massnahmenPruefungen", []))

    def _populate_chapter_7(self, report: dict, stage_data: dict) -> None:
        """Populates Chapter 7 (Anhang) content into the report."""
        ref_docs_data = stage_data.get('referenzdokumente', {})
        if isinstance(ref_docs_data.get('table'), dict):
            path = "bsiAuditReport.anhang.referenzdokumente.table.rows"
            self._set_value_by_path(report, path, ref_docs_data['table'].get('rows', []))

    def _populate_from_scan_report(self, report: dict, stage_data: dict) -> None:
        """Populates the report with baseline data from a scanned previous report."""
        logging.info("Populating baseline data from Scan-Report stage...")

        # 1. Populate Chapter 1 tables
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.versionshistorie.table.rows', stage_data.get('versionshistorie', {}).get('table', {}).get('rows', []))
        
        audit_institution_data = stage_data.get('auditierteInstitution', {})
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.auditierteInstitution.kontaktinformationenAntragsteller.table.rows', audit_institution_data.get('kontaktinformationenAntragsteller', {}).get('table', {}).get('rows', []))
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.auditierteInstitution.ansprechpartnerZertifizierung.table.rows', audit_institution_data.get('ansprechpartnerZertifizierung', {}).get('table', {}).get('rows', []))
        
        auditteam_data = stage_data.get('auditteam', {})
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.auditteam.auditteamleiter.table.rows', auditteam_data.get('auditteamleiter', {}).get('table', {}).get('rows', []))
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.auditteam.auditor.table.rows', auditteam_data.get('auditor', {}).get('table', {}).get('rows', []))
        self._set_value_by_path(report, 'bsiAuditReport.allgemeines.auditteam.fachexperte.table.rows', auditteam_data.get('fachexperte', {}).get('table', {}).get('rows', []))
        
        # 2. Populate Chapter 4 tables as a fallback/baseline
        self._set_value_by_path(report, 'bsiAuditReport.erstellungEinesPruefplans.auditplanung.auswahlBausteineErstRezertifizierung.table.rows', stage_data.get('auswahlBausteineErstRezertifizierung', {}).get('table', {}).get('rows', []))
        self._set_value_by_path(report, 'bsiAuditReport.erstellungEinesPruefplans.auditplanung.auswahlBausteine1Ueberwachungsaudit.table.rows', stage_data.get('auswahlBausteine1Ueberwachungsaudit', {}).get('table', {}).get('rows', []))
        self._set_value_by_path(report, 'bsiAuditReport.erstellungEinesPruefplans.auditplanung.auswahlStandorte.table.rows', stage_data.get('auswahlStandorte', {}).get('table', {}).get('rows', []))


    def _populate_report(self, report: dict, stage_name: str, stage_data: dict) -> None:
        """Router function to call the correct population logic for a given stage."""
        logging.info(f"Populating report with data from stage: {stage_name}")
        population_map = {
            "Chapter-1": self._populate_chapter_1,
            "Chapter-3": self._populate_chapter_3,
            "Chapter-4": self._populate_chapter_4,
            "Chapter-5": self._populate_chapter_5,
            "Chapter-7": self._populate_chapter_7,
        }
        
        populate_func = population_map.get(stage_name)
        if populate_func:
            populate_func(report, stage_data)
        else:
            logging.warning(f"No population logic defined for stage: {stage_name}")
==== bsi-audit-automator/src/audit/stages/control_catalog.py ====
# src/audit/stages/control_catalog.py
import logging
import json
from typing import List, Dict, Any, Optional

class ControlCatalog:
    """A utility to load and query the BSI Grundschutz OSCAL catalog."""
    
    def __init__(self, catalog_path: str = "assets/json/BSI_GS_OSCAL_current_2023_benutzerdefinierte.json"):
        self.catalog_path = catalog_path
        self._baustein_map = {}
        self._control_map = {}  # New: Map for direct control lookup
        try:
            self._load_and_parse_catalog()
            logging.info(f"Successfully loaded and parsed BSI Control Catalog from {catalog_path}.")
        except Exception as e:
            logging.error(f"Failed to initialize ControlCatalog: {e}", exc_info=True)
            raise

    def _load_and_parse_catalog(self):
        """Loads the JSON catalog and builds an efficient lookup map."""
        with open(self.catalog_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        catalog = data.get("catalog", {})
        # Layers like 'ISMS', 'ORP', 'INF', etc.
        for layer_group in catalog.get("groups", []):
            # Bausteine within each layer
            for baustein_group in layer_group.get("groups", []):
                baustein_id = baustein_group.get("id")
                if baustein_id:
                    controls = baustein_group.get("controls", [])
                    self._baustein_map[baustein_id] = controls
                    for control in controls:
                        self._control_map[control.get("id")] = control
    
    def get_controls_for_baustein_id(self, baustein_id: str) -> List[Dict[str, Any]]:
        """
        Retrieves all controls for a given Baustein ID.

        Args:
            baustein_id: The ID of the Baustein (e.g., 'ISMS.1').

        Returns:
            A list of control objects, or an empty list if not found.
        """
        controls = self._baustein_map.get(baustein_id, [])
        if not controls:
            logging.warning(f"No controls found for Baustein ID: {baustein_id}")
        return controls

    def get_control_level(self, control_id: str) -> Optional[str]:
        """
        Efficiently retrieves the 'level' property for a given control ID.

        Args:
            control_id: The ID of the control (e.g., 'ISMS.1.A1').

        Returns:
            The level as a string (e.g., '1', '5') or None if not found.
        """
        control = self._control_map.get(control_id)
        if control:
            for prop in control.get("props", []):
                if prop.get("name") == "level":
                    return prop.get("value")
        return None

    def get_level_1_control_ids(self) -> List[str]:
        """
        Scans the entire catalog and returns a list of all control IDs that
        are marked as Level 1 (MUSS-Anforderungen).

        Returns:
            A list of Level 1 control ID strings.
        """
        level_1_ids = []
        for baustein_id, controls in self._baustein_map.items():
            for control in controls:
                for prop in control.get("props", []):
                    if prop.get("name") == "level" and prop.get("value") == "1":
                        level_1_ids.append(control.get("id"))
                        break # Move to the next control once level is found
        logging.info(f"Found {len(level_1_ids)} Level 1 (MUSS) controls in the catalog.")
        return level_1_ids
==== bsi-audit-automator/src/audit/stages/gs_extraction/__init__.py ====
# bsi-audit-automator/src/audit/stages/gs_extraction/__init__.py
"""
Grundschutz Check Extraction package.

This package implements the Ground-Truth-Driven Semantic Chunking strategy
for extracting structured security requirements from BSI Grundschutz documents.
"""

from .ground_truth_mapper import GroundTruthMapper
from .document_processor import DocumentProcessor
from .block_grouper import BlockGrouper
from .ai_refiner import AiRefiner

__all__ = [
    'GroundTruthMapper',
    'DocumentProcessor', 
    'BlockGrouper',
    'AiRefiner'
]
==== bsi-audit-automator/src/audit/stages/gs_extraction/ai_refiner.py ====
# bsi-audit-automator/src/audit/stages/gs_extraction/ai_refiner.py
import logging
import json
import asyncio
import os
from typing import Dict, Any, List, Tuple, Optional

from src.clients.ai_client import AiClient
from src.clients.gcs_client import GcsClient
from src.constants import GROUPED_BLOCKS_PATH, EXTRACTED_CHECK_DATA_PATH, INDIVIDUAL_RESULTS_PREFIX, CHUNK_PROCESSING_MODEL


class AiRefiner:
    """
    Processes grouped blocks with AI to extract structured security requirements.
    Handles chunking, caching, error recovery, and result consolidation.
    """
    
    PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"
    
    # Chunking configuration
    MAX_BLOCKS_PER_CHUNK = 200
    MIN_BLOCKS_PER_CHUNK = 50

    def __init__(self, ai_client: AiClient, gcs_client: GcsClient):
        self.ai_client = ai_client
        self.gcs_client = gcs_client
        self.prompt_config = self._load_asset_json(self.PROMPT_CONFIG_PATH)

    def _load_asset_json(self, path: str) -> dict:
        """Load JSON configuration from assets."""
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    async def refine_grouped_blocks_with_ai(self, system_map: Dict[str, Any], force_overwrite: bool):
        """
        Process grouped blocks with AI to extract structured requirements.
        
        Args:
            system_map: Ground truth map containing zielobjekte information
            force_overwrite: If True, reprocess even if output exists
        """
        if not force_overwrite and self.gcs_client.blob_exists(EXTRACTED_CHECK_DATA_PATH):
            logging.info(f"Final extracted check results file exists. Skipping AI refinement.")
            return

        logging.info("Refining grouped blocks with AI to extract structured requirements...")
        
        # Load grouped blocks and configuration
        grouped_blocks_data = await self.gcs_client.read_json_async(GROUPED_BLOCKS_PATH)
        groups = grouped_blocks_data.get("zielobjekt_grouped_blocks", {})
        
        refine_config = self.prompt_config["stages"]["Chapter-3"]["refine_layout_parser_group"]
        prompt_template = refine_config["prompt"]
        schema = self._load_asset_json(refine_config["schema_path"])
        
        zielobjekt_map = {z['kuerzel']: z['name'] for z in system_map.get("zielobjekte", [])}

        # Filter valid groups and apply test mode limiting
        valid_groups = {k: v for k, v in groups.items() if k != "_UNGROUPED_" and v}
        
        if not valid_groups:
            logging.warning("No valid Zielobjekt groups found for processing")
            final_output = {"anforderungen": []}
        else:
            # Apply test mode limiting
            if os.getenv("TEST", "false").lower() == "true":
                limited_groups = dict(list(valid_groups.items())[:3])
                logging.info(f"Test mode: Processing only {len(limited_groups)} of {len(valid_groups)} groups")
                valid_groups = limited_groups
            
            logging.info(f"Processing {len(valid_groups)} Zielobjekt groups...")
            
            # Process all groups
            results = await self._process_all_groups(valid_groups, zielobjekt_map, prompt_template, schema)
            
            # Assemble final results
            final_output = self._assemble_final_results(results)
        
        # Save consolidated results
        await self.gcs_client.upload_from_string_async(
            json.dumps(final_output, indent=2, ensure_ascii=False), 
            EXTRACTED_CHECK_DATA_PATH
        )
        logging.info(f"Saved final refined check data with {len(final_output['anforderungen'])} requirements")

    async def _process_all_groups(self, valid_groups: Dict[str, List[Dict]], zielobjekt_map: Dict[str, str], 
                                 prompt_template: str, schema: Dict[str, Any]) -> List[Tuple[str, str, Optional[Dict[str, Any]]]]:
        """Process all valid groups concurrently."""
        tasks = [
            self._process_group_with_caching(kuerzel, blocks, zielobjekt_map, prompt_template, schema) 
            for kuerzel, blocks in valid_groups.items()
        ]
        return await asyncio.gather(*tasks)

    async def _process_group_with_caching(self, kuerzel: str, blocks: List[Dict], zielobjekt_map: Dict[str, str], 
                                         prompt_template: str, schema: Dict[str, Any]) -> Tuple[str, str, Optional[Dict[str, Any]]]:
        """Process a single group with caching support."""
        name = zielobjekt_map.get(kuerzel, "Unbekannt")
        
        # Check for cached result first
        cached_result = await self._get_cached_result(kuerzel)
        if cached_result is not None:
            return kuerzel, name, cached_result
        
        try:
            # Process with chunking if needed
            chunks = self._chunk_blocks(blocks, self.MAX_BLOCKS_PER_CHUNK)
            
            if len(chunks) == 1:
                # Single chunk - process normally
                result = await self._process_blocks_chunk(kuerzel, chunks[0], 0, 1, prompt_template, schema)
            else:
                # Multiple chunks - process each and merge results
                logging.info(f"Processing {len(chunks)} chunks for Zielobjekt '{kuerzel}'")
                chunk_tasks = [
                    self._process_blocks_chunk(kuerzel, chunk, idx, len(chunks), prompt_template, schema) 
                    for idx, chunk in enumerate(chunks)
                ]
                chunk_results = await asyncio.gather(*chunk_tasks)
                
                # Merge all anforderungen from chunks
                all_anforderungen = []
                for chunk_result in chunk_results:
                    if chunk_result and "anforderungen" in chunk_result:
                        all_anforderungen.extend(chunk_result["anforderungen"])
                
                result = {"anforderungen": all_anforderungen}
                logging.info(f"Merged {len(all_anforderungen)} requirements from {len(chunks)} chunks for '{kuerzel}'")
            
            # Cache the result
            if result:
                await self._save_result_to_cache(kuerzel, result)
            
            return kuerzel, name, result
            
        except Exception as e:
            logging.error(f"Complete processing failed for Zielobjekt '{kuerzel}': {e}")
            return kuerzel, name, None

    async def _get_cached_result(self, kuerzel: str) -> Optional[Dict[str, Any]]:
        """Check if we have a cached result for this kürzel."""
        cache_path = f"{INDIVIDUAL_RESULTS_PREFIX}{kuerzel}_result.json"
        if self.gcs_client.blob_exists(cache_path):
            try:
                cached_result = await self.gcs_client.read_json_async(cache_path)
                logging.info(f"Using cached result for Zielobjekt '{kuerzel}'")
                return cached_result
            except Exception as e:
                logging.warning(f"Failed to read cached result for '{kuerzel}': {e}")
        return None

    async def _save_result_to_cache(self, kuerzel: str, result_data: Dict[str, Any]):
        """Save individual result to cache."""
        cache_path = f"{INDIVIDUAL_RESULTS_PREFIX}{kuerzel}_result.json"
        try:
            await self.gcs_client.upload_from_string_async(
                json.dumps(result_data, indent=2, ensure_ascii=False), cache_path
            )
            logging.debug(f"Cached result for Zielobjekt '{kuerzel}' to {cache_path}")
        except Exception as e:
            logging.error(f"Failed to cache result for '{kuerzel}': {e}")

    def _chunk_blocks(self, blocks: List[Dict], max_blocks: int) -> List[List[Dict]]:
        """Split blocks into chunks of manageable size with 8% overlap."""
        if len(blocks) <= max_blocks:
            return [blocks]

        # Calculate overlap size (10% of max_blocks, minimum 10 blocks, maximum 20 blocks)
        overlap_size = max(10, min(20, int(max_blocks * 0.10)))
        
        chunks = []
        i = 0
        while i < len(blocks):
            # Calculate chunk boundaries
            start_idx = max(0, i - (overlap_size if i > 0 else 0))
            end_idx = min(len(blocks), i + max_blocks)
            
            # Extract chunk with overlap
            chunk = blocks[start_idx:end_idx]
            chunks.append(chunk)
            
            # Move to next chunk position (accounting for overlap)
            i += max_blocks - overlap_size
            
            # Break if we've covered all blocks
            if end_idx >= len(blocks):
                break
        
        logging.info(f"Split {len(blocks)} blocks into {len(chunks)} chunks with {overlap_size}-block overlap ({overlap_size/max_blocks*100:.1f}%)")
        return chunks

    def _preprocess_blocks_for_ai(self, blocks: List[Dict]) -> List[Dict]:
        """Preprocess blocks to avoid JSON generation issues."""
        processed_blocks = []
        
        for block in blocks:
            # Create a clean copy of the block
            clean_block = block.copy()
            
            # Clean text content to prevent JSON issues
            if 'textBlock' in clean_block and 'text' in clean_block['textBlock']:
                text = clean_block['textBlock']['text']
                # Remove or escape problematic characters
                text = text.replace('\r\n', ' ').replace('\n', ' ').replace('\r', ' ')
                text = text.replace('"', '\\"').replace('\t', ' ')
                # Limit extremely long text blocks that might cause issues
                if len(text) > 2000:
                    text = text[:1800] + "... [truncated]"
                clean_block['textBlock']['text'] = text
            
            processed_blocks.append(clean_block)
        
        return processed_blocks

    async def _process_blocks_chunk(self, kuerzel: str, chunk: List[Dict], chunk_idx: int, total_chunks: int,
                                   prompt_template: str, schema: Dict[str, Any]) -> Dict[str, Any]:
        """Process a single chunk of blocks for a kürzel."""
        # Preprocess blocks to prevent JSON issues
        clean_chunk = self._preprocess_blocks_for_ai(chunk)
        
        # Calculate rough content size for logging
        total_chars = sum(len(str(block)) for block in clean_chunk)
        logging.info(f"Processing chunk {chunk_idx + 1}/{total_chunks} for '{kuerzel}': {len(clean_chunk)} blocks, ~{total_chars} chars (using {CHUNK_PROCESSING_MODEL})")
        
        # Add chunk context to prompt if multiple chunks
        chunk_context = ""
        if total_chunks > 1:
            chunk_context = f"\n\nNote: This is chunk {chunk_idx + 1} of {total_chunks} for this Zielobjekt. Chunks have 8% overlap to maintain context continuity. Focus on extracting requirements from these specific blocks, avoiding duplication of requirements found in overlapping sections."
        
        prompt = prompt_template.format(zielobjekt_blocks_json=json.dumps(clean_chunk, indent=2)) + chunk_context
        
        try:
            # Try with flash model for faster processing
            result = await self.ai_client.generate_json_response(
                prompt=prompt,
                json_schema=schema,
                request_context_log=f"RefineGroup: {kuerzel} (chunk {chunk_idx + 1}/{total_chunks})",
                model_override=CHUNK_PROCESSING_MODEL
            )
            
            # Validate the result
            if result and "anforderungen" in result:
                return result
            else:
                logging.warning(f"Invalid result structure for {kuerzel} chunk {chunk_idx + 1}")
                return {"anforderungen": []}
                
        except Exception as e:
            logging.error(f"AI refinement failed for Zielobjekt '{kuerzel}' chunk {chunk_idx + 1}: {e}")
            
            # If this chunk is too large, try splitting it further
            if len(clean_chunk) > self.MIN_BLOCKS_PER_CHUNK and "token" in str(e).lower():
                logging.info(f"Attempting to split large chunk {chunk_idx + 1} for '{kuerzel}'")
                try:
                    # Split the chunk in half and process each part
                    mid_point = len(clean_chunk) // 2
                    part1 = clean_chunk[:mid_point]
                    part2 = clean_chunk[mid_point:]
                    
                    # Process both parts recursively
                    result1 = await self._process_blocks_chunk(kuerzel, part1, f"{chunk_idx}a", f"{total_chunks}+", prompt_template, schema)
                    result2 = await self._process_blocks_chunk(kuerzel, part2, f"{chunk_idx}b", f"{total_chunks}+", prompt_template, schema)
                    
                    # Combine results
                    combined_anforderungen = []
                    if result1 and "anforderungen" in result1:
                        combined_anforderungen.extend(result1["anforderungen"])
                    if result2 and "anforderungen" in result2:
                        combined_anforderungen.extend(result2["anforderungen"])
                    
                    return {"anforderungen": combined_anforderungen}
                    
                except Exception as split_error:
                    logging.error(f"Chunk splitting also failed for '{kuerzel}': {split_error}")
            
            return {"anforderungen": []}

    def _assemble_final_results(self, results: List[Tuple[str, str, Optional[Dict[str, Any]]]]) -> Dict[str, List[Dict]]:
        """Assemble final results from all processed groups."""
        all_anforderungen = []
        successful_count = 0
        failed_count = 0
        
        for kuerzel, name, result_data in results:
            if result_data and "anforderungen" in result_data:
                for anforderung in result_data["anforderungen"]:
                    anforderung['zielobjekt_kuerzel'] = kuerzel
                    anforderung['zielobjekt_name'] = name
                    all_anforderungen.append(anforderung)
                successful_count += 1
            else:
                failed_count += 1
                logging.warning(f"No valid requirements extracted for Zielobjekt '{kuerzel}'")

        logging.info(f"AI refinement completed: {successful_count} successful, {failed_count} failed")
        return {"anforderungen": all_anforderungen}
==== bsi-audit-automator/src/audit/stages/gs_extraction/block_grouper.py ====
# bsi-audit-automator/src/audit/stages/gs_extraction/block_grouper.py
import logging
import json
import sys
from typing import Dict, Any, List
from collections import defaultdict

from src.clients.gcs_client import GcsClient
from src.constants import FINAL_MERGED_LAYOUT_PATH, GROUPED_BLOCKS_PATH


class BlockGrouper:
    """
    Groups Document AI layout blocks by Zielobjekt context using a marker-based algorithm.
    Finds Zielobjekt identifiers as section markers and groups content between them.
    """

    def __init__(self, gcs_client: GcsClient):
        self.gcs_client = gcs_client

    async def group_layout_blocks_by_zielobjekt(self, system_map: Dict[str, Any], force_overwrite: bool):
        """
        Group layout blocks by Zielobjekt using a robust marker-based algorithm.
        
        Args:
            system_map: Ground truth map containing zielobjekte list
            force_overwrite: If True, reprocess even if output exists
        """
        if not force_overwrite and self.gcs_client.blob_exists(GROUPED_BLOCKS_PATH):
            logging.info(f"Grouped layout blocks file already exists. Skipping grouping.")
            return

        logging.info("Grouping layout blocks by Zielobjekt context using marker-based algorithm...")
        
        # Load layout data
        layout_data = await self.gcs_client.read_json_async(FINAL_MERGED_LAYOUT_PATH)
        all_blocks = layout_data.get("documentLayout", {}).get("blocks", [])

        # Initialize grouping structures
        grouped_blocks = defaultdict(list)
        
        # Flatten all blocks for consistent processing
        all_flattened_blocks = self._flatten_all_blocks(all_blocks)
        block_id_to_block_map = {int(b['blockId']): b for b in all_flattened_blocks}

        # Find Zielobjekt markers in the document
        markers = self._find_zielobjekt_markers(all_flattened_blocks, system_map)
        
        if not markers:
            # If no markers found, all blocks are ungrouped
            logging.warning("No Zielobjekt markers found in document. All blocks will be marked as ungrouped.")
            sys.exit()
        else:
            # Group blocks based on marker positions
            self._group_blocks_by_markers(markers, block_id_to_block_map, grouped_blocks)

        # Save grouped blocks
        await self.gcs_client.upload_from_string_async(
            json.dumps({"zielobjekt_grouped_blocks": dict(grouped_blocks)}, indent=2, ensure_ascii=False),
            GROUPED_BLOCKS_PATH
        )
        logging.info(f"Saved grouped layout blocks to {GROUPED_BLOCKS_PATH}")

    def _flatten_all_blocks(self, blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Flatten all blocks into a single list with hierarchical structure removed."""
        flattened = []
        
        def flatten_recursive(block_list):
            for block in block_list:
                # Add current block to flattened list
                flattened.append(block)
                
                # Process nested textBlock.blocks
                if 'textBlock' in block and 'blocks' in block['textBlock']:
                    flatten_recursive(block['textBlock']['blocks'])
                
                # Process table blocks
                if 'tableBlock' in block:
                    for row_type in ['headerRows', 'bodyRows']:
                        for row in block['tableBlock'].get(row_type, []):
                            for cell in row.get('cells', []):
                                if 'blocks' in cell:
                                    flatten_recursive(cell['blocks'])
        
        flatten_recursive(blocks)
        return flattened

    def _find_zielobjekt_markers(self, all_flattened_blocks: List[Dict[str, Any]], system_map: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Find Zielobjekt markers in the flattened blocks."""
        zielobjekte = system_map.get("zielobjekte", [])
        kuerzel_list = [item['kuerzel'] for item in zielobjekte]
        remaining_kuerzel = kuerzel_list.copy()
        markers = []
        
        # Search for exact matches of Zielobjekt kürzel in block text
        for block in all_flattened_blocks:
            direct_text = ""
            if 'textBlock' in block and 'text' in block['textBlock']:
                direct_text = block['textBlock']['text'].strip()
            
            if direct_text:
                for kuerzel in remaining_kuerzel.copy():
                    if direct_text == kuerzel:
                        block_id = int(block.get('blockId', 0))
                        markers.append({'kuerzel': kuerzel, 'block_id': block_id})
                        remaining_kuerzel.remove(kuerzel)
                        break
        
        logging.info(f"Found {len(markers)} Zielobjekt markers. Unfound kürzel ({len(remaining_kuerzel)}): {remaining_kuerzel}")
        return markers

    def _group_blocks_by_markers(self, markers: List[Dict[str, Any]], block_id_to_block_map: Dict[int, Dict[str, Any]], grouped_blocks: defaultdict):
        """Group blocks based on marker positions."""
        # Sort markers by block ID position
        markers.sort(key=lambda m: m['block_id'])
        logging.info(f"Sorted {len(markers)} Zielobjekt markers.")

        # Get all block IDs in order
        sorted_block_ids = sorted(block_id_to_block_map.keys())
        
        # Handle blocks before first marker (ungrouped)
        first_marker_id = markers[0]['block_id']
        ungrouped_ids = [bid for bid in sorted_block_ids if bid < first_marker_id]
        for bid in ungrouped_ids:
            grouped_blocks["_UNGROUPED_"].append(block_id_to_block_map[bid])
        
        # Group blocks between consecutive markers
        for i, marker in enumerate(markers):
            start_id = marker['block_id']
            end_id = markers[i+1]['block_id'] if i + 1 < len(markers) else max(sorted_block_ids) + 1
            
            kuerzel = marker['kuerzel']
            group_ids = [bid for bid in sorted_block_ids if start_id <= bid < end_id]
            
            for bid in group_ids:
                grouped_blocks[kuerzel].append(block_id_to_block_map[bid])
            
            logging.info(f"Assigned {len(group_ids)} blocks to '{kuerzel}' (IDs {start_id}-{end_id-1}).")
==== bsi-audit-automator/src/audit/stages/gs_extraction/document_processor.py ====
# bsi-audit-automator/src/audit/stages/gs_extraction/document_processor.py
import logging
import json
import asyncio
import fitz  # PyMuPDF
from typing import Dict, Any, List

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.document_ai_client import DocumentAiClient
from src.clients.rag_client import RagClient
from src.constants import FINAL_MERGED_LAYOUT_PATH, DOC_AI_CHUNK_RESULTS_PREFIX, TEMP_PDF_CHUNKS_PREFIX


class DocumentProcessor:
    """
    Handles Document AI Layout Parser workflow for processing the Grundschutz-Check PDF.
    Splits large PDFs into chunks, processes them with Document AI, and merges results.
    """

    PAGE_CHUNK_SIZE = 100

    def __init__(self, gcs_client: GcsClient, doc_ai_client: DocumentAiClient, rag_client: RagClient, config: AppConfig):
        self.gcs_client = gcs_client
        self.doc_ai_client = doc_ai_client
        self.rag_client = rag_client
        self.config = config
        self.block_counter = 1

    async def execute_layout_parser_workflow(self, force_overwrite: bool):
        """
        Execute the full Document AI Layout Parser workflow.
        
        Args:
            force_overwrite: If True, recreate layout even if it already exists
        """
        if not force_overwrite and self.gcs_client.blob_exists(FINAL_MERGED_LAYOUT_PATH):
            logging.info(f"Merged layout file already exists. Skipping Layout Parser workflow.")
            return

        logging.info("Starting Document AI Layout Parser workflow...")
        
        # Find the Grundschutz-Check document
        check_uris = self.rag_client.get_gcs_uris_for_categories(["Grundschutz-Check", "test.pdf"])
        if not check_uris:
            raise FileNotFoundError("Could not find 'Grundschutz-Check' or 'test.pdf' document.")
        
        # Download and split PDF
        source_blob_name = check_uris[0].replace(f"gs://{self.config.bucket_name}/", "")
        pdf_bytes = self.gcs_client.download_blob_as_bytes(self.gcs_client.bucket.blob(source_blob_name))
        
        chunk_count = await self._split_and_upload_pdf(pdf_bytes)
        
        # Process all chunks with Document AI
        await self._process_pdf_chunks(chunk_count)
        
        # Merge and finalize results
        await self._merge_and_save_results(chunk_count)

    async def _split_and_upload_pdf(self, pdf_bytes: bytes) -> int:
        """Split PDF into chunks and upload to GCS."""
        pdf_doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        upload_tasks = []
        
        for i in range(0, pdf_doc.page_count, self.PAGE_CHUNK_SIZE):
            chunk_doc = fitz.open()
            end_page = min(i + self.PAGE_CHUNK_SIZE, pdf_doc.page_count) - 1
            chunk_doc.insert_pdf(pdf_doc, from_page=i, to_page=end_page)
            
            destination_blob_name = f"{TEMP_PDF_CHUNKS_PREFIX}chunk_{i // self.PAGE_CHUNK_SIZE}.pdf"
            upload_tasks.append(
                self.gcs_client.upload_from_bytes_async(chunk_doc.tobytes(), destination_blob_name)
            )
            chunk_doc.close()
        
        await asyncio.gather(*upload_tasks)
        pdf_doc.close()
        
        chunk_count = len(upload_tasks)
        logging.info(f"Split PDF into {chunk_count} chunks and uploaded to GCS.")
        return chunk_count

    async def _process_pdf_chunks(self, chunk_count: int):
        """Process all PDF chunks with Document AI."""
        processing_tasks = [
            self.doc_ai_client.process_document_chunk_async(
                f"gs://{self.config.bucket_name}/{TEMP_PDF_CHUNKS_PREFIX}chunk_{i}.pdf", 
                DOC_AI_CHUNK_RESULTS_PREFIX
            ) for i in range(chunk_count)
        ]
        await asyncio.gather(*processing_tasks)
        logging.info(f"Processed {chunk_count} chunks with Document AI.")

    async def _merge_and_save_results(self, chunk_count: int):
        """Merge all chunk results and save final layout."""
        merged_blocks = []
        merged_text = ""
        
        # Collect results from all chunks
        for i in range(chunk_count):
            chunk_json_path = f"{DOC_AI_CHUNK_RESULTS_PREFIX}chunk_{i}.json"
            chunk_data = await self.gcs_client.read_json_async(chunk_json_path)
            merged_text += chunk_data.get("text", "")
            merged_blocks.extend(chunk_data.get("documentLayout", {}).get("blocks", []))

        # Re-index block IDs globally and clean up
        self.block_counter = 1
        self._reindex_and_prune_blocks(merged_blocks)
        
        # Create final layout structure
        final_layout_json = {
            "text": merged_text, 
            "documentLayout": {"blocks": merged_blocks}
        }
        
        # Save to GCS
        await self.gcs_client.upload_from_string_async(
            json.dumps(final_layout_json, indent=2, ensure_ascii=False),
            FINAL_MERGED_LAYOUT_PATH
        )
        logging.info(f"Successfully merged, re-indexed, and saved final layout to {FINAL_MERGED_LAYOUT_PATH}")

    def _reindex_and_prune_blocks(self, blocks: List[Dict[str, Any]]):
        """Recursively re-index blockId globally and remove pageSpan."""
        for block in blocks:
            # Remove page span information (not needed after merging)
            block.pop("pageSpan", None)
            
            # Assign new global block ID
            block["blockId"] = str(self.block_counter)
            self.block_counter += 1
            
            # Process nested text blocks
            if "blocks" in block.get("textBlock", {}):
                self._reindex_and_prune_blocks(block["textBlock"]["blocks"])
            
            # Process table blocks
            for row_type in ["headerRows", "bodyRows"]:
                for row in block.get("tableBlock", {}).get(row_type, []):
                    for cell in row.get("cells", []):
                        if "blocks" in cell:
                            self._reindex_and_prune_blocks(cell["blocks"])
==== bsi-audit-automator/src/audit/stages/gs_extraction/ground_truth_mapper.py ====
# bsi-audit-automator/src/audit/stages/gs_extraction/ground_truth_mapper.py
import logging
import json
import os
from typing import Dict, Any, List

from src.clients.ai_client import AiClient
from src.clients.rag_client import RagClient
from src.clients.gcs_client import GcsClient
from src.constants import GROUND_TRUTH_MAP_PATH, GROUND_TRUTH_MODEL


class GroundTruthMapper:
    """
    Responsible for creating the authoritative system structure map by extracting
    Zielobjekte and Baustein-to-Zielobjekt mappings from customer documents.
    """
    
    PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"

    def __init__(self, ai_client: AiClient, rag_client: RagClient, gcs_client: GcsClient):
        self.ai_client = ai_client
        self.rag_client = rag_client
        self.gcs_client = gcs_client
        self.prompt_config = self._load_asset_json(self.PROMPT_CONFIG_PATH)

    def _load_asset_json(self, path: str) -> dict:
        """Load JSON configuration from assets."""
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _structure_mappings(self, flat_mappings: List[Dict[str, str]]) -> Dict[str, List[str]]:
        """Convert flat mapping list from AI into structured dict of Baustein ID to Zielobjekt Kürzel list."""
        structured = {}
        for mapping in flat_mappings:
            baustein_id = mapping.get("baustein_id")
            kuerzel = mapping.get("zielobjekt_kuerzel")
            if baustein_id and kuerzel:
                if baustein_id not in structured:
                    structured[baustein_id] = []
                if kuerzel not in structured[baustein_id]:
                    structured[baustein_id].append(kuerzel)
        return structured

    async def create_system_structure_map(self, force_overwrite: bool) -> Dict[str, Any]:
        """
        Create the authoritative system structure map by extracting Zielobjekte (from A.1)
        and Baustein-to-Zielobjekt mappings (from A.3).
        This map serves as "Ground Truth".
        
        Args:
            force_overwrite: If True, regenerate even if map already exists
            
        Returns:
            Dict containing zielobjekte list and baustein_to_zielobjekt_mapping
        """
        if not force_overwrite and self.gcs_client.blob_exists(GROUND_TRUTH_MAP_PATH):
            logging.info(f"System structure map already exists. Loading from '{GROUND_TRUTH_MAP_PATH}'.")
            try:
                system_map = await self.gcs_client.read_json_async(GROUND_TRUTH_MAP_PATH)
                if not system_map.get("zielobjekte"):
                    logging.error("Loaded system structure map has empty 'zielobjekte'. Exiting.")
                    raise ValueError("No Zielobjekte found in loaded map. Cannot proceed.")
                return system_map
            except json.JSONDecodeError as e:
                logging.error(f"Invalid JSON in system structure map: {e}")
                raise
        
        logging.info("Generating new system structure map...")
        gt_config = self.prompt_config["stages"]["Chapter-3-Ground-Truth"]
        
        try:
            # Extract Zielobjekte from Strukturanalyse (A.1)
            z_task_config = gt_config["extract_zielobjekte"]
            z_uris = self.rag_client.get_gcs_uris_for_categories(["Strukturanalyse"])
            zielobjekte_result = await self.ai_client.generate_json_response(
                prompt=z_task_config["prompt"], 
                json_schema=self._load_asset_json(z_task_config["schema_path"]), 
                gcs_uris=z_uris, 
                request_context_log="GT: extract_zielobjekte",
                model_override=self.GROUND_TRUTH_MODEL
            )

            # Extract Mappings from Modellierung (A.3)
            m_task_config = gt_config["extract_baustein_mappings"]
            m_uris = self.rag_client.get_gcs_uris_for_categories(["Modellierung"])
            mappings_result = await self.ai_client.generate_json_response(
                prompt=m_task_config["prompt"], 
                json_schema=self._load_asset_json(m_task_config["schema_path"]), 
                gcs_uris=m_uris, 
                request_context_log="GT: extract_baustein_mappings",
                model_override=self.GROUND_TRUTH_MODEL
            )

            # Construct the system map
            system_map = {
                "zielobjekte": zielobjekte_result.get("zielobjekte", []),
                "baustein_to_zielobjekt_mapping": self._structure_mappings(mappings_result.get("mappings", []))
            }
            
            # Save to GCS
            await self.gcs_client.upload_from_string_async(
                json.dumps(system_map, indent=2, ensure_ascii=False), 
                GROUND_TRUTH_MAP_PATH
            )
            logging.info(f"Successfully created and saved system structure map to {GROUND_TRUTH_MAP_PATH}.")
            
            return system_map
            
        except Exception as e:
            logging.error(f"Failed to create system structure map: {e}", exc_info=True)
            raise
==== bsi-audit-automator/src/audit/stages/stage_1_general.py ====
# src/audit/stages/stage_1_general.py
import logging
import json
import asyncio
from typing import Dict, Any

from src.config import AppConfig
from src.clients.ai_client import AiClient
from src.clients.rag_client import RagClient

class Chapter1Runner:
    """Handles generating content for Chapter 1, with most sections being manual placeholders."""
    STAGE_NAME = "Chapter-1"
    PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"

    def __init__(self, config: AppConfig, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.ai_client = ai_client
        self.rag_client = rag_client
        self.prompt_config = self._load_asset_json(self.PROMPT_CONFIG_PATH)
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f: return json.load(f)

    async def _process_informationsverbund(self) -> Dict[str, Any]:
        """Handles 1.4 Informationsverbund using a filtered document query."""
        logging.info("Processing 1.4 Informationsverbund...")
        
        stage_config = self.prompt_config["stages"]["Chapter-1"]["informationsverbund"]
        prompt_template = stage_config["prompt"]
        schema = self._load_asset_json(stage_config["schema_path"])
        
        source_categories = ['Informationsverbund', 'Strukturanalyse']
        gcs_uris = self.rag_client.get_gcs_uris_for_categories(source_categories)
        
        if not gcs_uris:
            logging.warning(f"No documents found for categories {source_categories}. Generating deterministic response.")
            return {
                "kurzbezeichnung": "Nicht ermittelt",
                "kurzbeschreibung": "Der Geltungsbereich des Informationsverbunds konnte aus den bereitgestellten Dokumenten nicht eindeutig ermittelt werden. Dies muss manuell geklärt und dokumentiert werden.",
                "finding": {
                    "category": "AS",
                    "description": "Die Abgrenzung des Geltungsbereichs ist unklar, da keine Dokumente der Kategorien 'Informationsverbund' oder 'Strukturanalyse' gefunden wurden. Dies ist eine schwerwiegende Abweichung."
                }
            }
            
        return await self.ai_client.generate_json_response(
            prompt=prompt_template,
            json_schema=schema,
            gcs_uris=gcs_uris,
            request_context_log="Chapter-1: informationsverbund"
        )

    async def run(self, force_overwrite: bool = False) -> dict:
        """Executes the generation logic for Chapter 1."""
        logging.info(f"Executing stage: {self.STAGE_NAME}")
        
        informationsverbund_result = await self._process_informationsverbund()

        final_result = {
            "informationsverbund": informationsverbund_result,
            "audittyp": {
                "content": self.config.audit_type
            }
        }

        logging.info(f"Successfully generated data for stage {self.STAGE_NAME}")
        return final_result
==== bsi-audit-automator/src/audit/stages/stage_3_dokumentenpruefung.py ====
# file: src/audit/stages/stage_3_dokumentenpruefung.py
import logging
import json
import asyncio
from typing import Dict, Any, List, Tuple
from datetime import datetime, timedelta
from google.cloud.exceptions import NotFound
from collections import defaultdict

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient
from src.clients.rag_client import RagClient
from src.audit.stages.control_catalog import ControlCatalog
from src.constants import EXTRACTED_CHECK_DATA_PATH, GROUND_TRUTH_MAP_PATH

class Chapter3Runner:
    """
    Handles generating content for Chapter 3 "Dokumentenprüfung" by dynamically
    parsing the master report template and using the central prompt configuration.
    It now relies on the pre-computed `extracted_grundschutz_check_merged.json`
    file generated by the `Grundschutz-Check-Extraction` stage.
    """
    STAGE_NAME = "Chapter-3"
    TEMPLATE_PATH = "assets/json/master_report_template.json"
    PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"
    SUMMARY_DEPENDENCIES = {
        "ergebnisDerStrukturanalyse": [
            "definitionDesInformationsverbundes", "bereinigterNetzplan", "listeDerGeschaeftsprozesse",
            "listeDerAnwendungen", "listeDerItSysteme", "listeDerRaeumeGebaeudeStandorte",
            "listeDerKommunikationsverbindungen", "stichprobenDokuStrukturanalyse", "listeDerDienstleister"
        ],
        "ergebnisDerSchutzbedarfsfeststellung": [
            "definitionDerSchutzbedarfskategorien", "schutzbedarfGeschaeftsprozesse", "schutzbedarfAnwendungen",
            "schutzbedarfItSysteme", "schutzbedarfRaeume", "schutzbedarfKommunikationsverbindungen",
            "stichprobenDokuSchutzbedarf"
        ],
        "ergebnisDerModellierung": ["modellierungsdetails"],
        "ergebnisItGrundschutzCheck": ["detailsZumItGrundschutzCheck", "benutzerdefinierteBausteine"],
        # 'ergebnisDerDokumentenpruefung' is the final summary and will use all findings by default.
    }
    
    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self.rag_client = rag_client
        self.control_catalog = ControlCatalog()
        self.prompt_config = self._load_asset_json(self.PROMPT_CONFIG_PATH)
        self.execution_plan = self._build_execution_plan_from_template()
        self._doc_map = self.rag_client._document_category_map
        self._ground_truth_map = None # Lazy loaded
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME} with dynamic execution plan.")

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f: return json.load(f)

    async def _get_ground_truth_map(self) -> Dict[str, Any]:
        """Lazy loads the ground truth map and caches it."""
        if self._ground_truth_map is None:
            try:
                self._ground_truth_map = await self.gcs_client.read_json_async(GROUND_TRUTH_MAP_PATH)
            except NotFound:
                logging.error(f"FATAL: Ground truth map not found at '{GROUND_TRUTH_MAP_PATH}'. Please run the extraction stage.")
                raise
        return self._ground_truth_map

    async def _process_details_zum_it_grundschutz_check(self) -> Dict[str, Any]:
        """
        Uses the pre-computed/refined data to answer the five questions with a mix of
        deterministic and targeted AI-driven logic.
        """
        logging.info("Processing 3.6.1 'Details zum IT-Grundschutz-Check' using pre-computed data...")
        try:
            check_data = self.gcs_client.read_json(EXTRACTED_CHECK_DATA_PATH)
            anforderungen = check_data.get("anforderungen", [])
        except NotFound:
            logging.error(f"FATAL: The required intermediate file '{EXTRACTED_CHECK_DATA_PATH}' was not found. Please run the 'Grundschutz-Check-Extraction' stage first.")
            raise

        answers = [None] * 5
        findings = []
        ground_truth_map = await self._get_ground_truth_map()

        # Task E: Coverage Check
        all_mapped_kuerzel = {k for k_list in ground_truth_map.get("baustein_to_zielobjekt_mapping", {}).values() for k in k_list}
        all_checked_kuerzel = {a.get("zielobjekt_kuerzel") for a in anforderungen}
        missing_in_check = all_mapped_kuerzel - all_checked_kuerzel
        if missing_in_check:
            desc = f"Die Zielobjekte {sorted(list(missing_in_check))} sind in der Modellierung vorhanden, aber es wurden für sie keine Anforderungen im Grundschutz-Check gefunden oder verarbeitet."
            findings.append({"category": "AG", "description": desc})
            logging.warning(f"Coverage Check (Task E) failed: {desc}")

        # Q1: Status erhoben? (Deterministic)
        answers[0] = all(a.get("umsetzungsstatus") for a in anforderungen)
        if not answers[0]:
            findings.append({"category": "AG", "description": "Nicht für alle Anforderungen wurde ein Umsetzungsstatus erhoben."})

        # Q5: Prüfung < 12 Monate? (Deterministic)
        one_year_ago = datetime.now() - timedelta(days=365)
        
        outdated = []
        for a in anforderungen:
            date_str = a.get("datumLetztePruefung", "1970-01-01")
            try:
                # Try ISO format first
                check_date = datetime.strptime(date_str, "%Y-%m-%d")
            except ValueError:
                try:
                    # Try German format DD.MM.YYYY
                    check_date = datetime.strptime(date_str, "%d.%m.%Y")
                except ValueError:
                    # If both fail, use default old date
                    check_date = datetime.strptime("1970-01-01", "%Y-%m-%d")
                    logging.warning(f"Could not parse date '{date_str}', using default 1970-01-01")
            
            if check_date < one_year_ago:
                outdated.append(a)


        answers[4] = not bool(outdated)
        if outdated:
            findings.append({"category": "AG", "description": f"Die Prüfung von {len(outdated)} Anforderungen liegt mehr als 12 Monate zurück."})
            
        # Correctly load the configuration for targeted questions
        ch3_config = self.prompt_config["stages"]["Chapter-3"]
        targeted_prompt_template = ch3_config["targeted_question"]["prompt"]
        questions_config = ch3_config["questions"]

        # Q2: "entbehrlich" plausibel? (Targeted AI - Task D)
        entbehrlich_items = [a for a in anforderungen if a.get("umsetzungsstatus") == "entbehrlich"]
        risikoanalyse_uris = self.rag_client.get_gcs_uris_for_categories(["Risikoanalyse"])
        if entbehrlich_items:
            for item in entbehrlich_items: # Enrich with control level
                item['level'] = self.control_catalog.get_control_level(item.get('id'))
            
            question = questions_config["entbehrlich"]
            prompt = targeted_prompt_template.format(
                question=question,
                json_data=json.dumps(entbehrlich_items, indent=2, ensure_ascii=False),
            )
            res = await self.ai_client.generate_json_response(
                prompt, self._load_asset_json("assets/schemas/generic_1_question_schema.json"), 
                gcs_uris=risikoanalyse_uris, request_context_log="3.6.1-Q2"
            )
            answers[1], findings = (res['answers'][0], findings + [res['finding']] if res['finding']['category'] != 'OK' else findings)
        else:
            answers[1] = True

        # Q3: MUSS-Anforderungen erfüllt? (Targeted AI)
        level_1_ids = self.control_catalog.get_level_1_control_ids()
        muss_anforderungen = [a for a in anforderungen if a.get("id") in level_1_ids]
        if muss_anforderungen:
            prompt = targeted_prompt_template.format(
                question=questions_config["muss_anforderungen"],
                json_data=json.dumps(muss_anforderungen, indent=2, ensure_ascii=False)
            )
            res = await self.ai_client.generate_json_response(prompt, self._load_asset_json("assets/schemas/generic_1_question_schema.json"), request_context_log="3.6.1-Q3")
            answers[2], findings = (res['answers'][0], findings + [res['finding']] if res['finding']['category'] != 'OK' else findings)
        else:
            answers[2] = True

        # Q4: Nicht/teilweise umgesetzte in A.6? (Targeted AI)
        unmet_items = [a for a in anforderungen if a.get("umsetzungsstatus") in ["Nein", "teilweise"]]
        realisierungsplan_uris = self.rag_client.get_gcs_uris_for_categories(["Realisierungsplan"])
        if unmet_items and realisierungsplan_uris:
            prompt = targeted_prompt_template.format(
                question=questions_config["nicht_umgesetzt"],
                json_data=json.dumps(unmet_items, indent=2, ensure_ascii=False)
            )
            res = await self.ai_client.generate_json_response(
                prompt, self._load_asset_json("assets/schemas/generic_1_question_schema.json"), 
                gcs_uris=realisierungsplan_uris, request_context_log="3.6.1-Q4"
            )
            answers[3], findings = (res['answers'][0], findings + [res['finding']] if res['finding']['category'] != 'OK' else findings)
        else:
            answers[3] = not unmet_items
            if unmet_items and not realisierungsplan_uris:
                findings.append({"category": "AG", "description": "Es gibt nicht umgesetzte Anforderungen, aber der Realisierungsplan (A.6) wurde nicht gefunden, um die Dokumentation zu überprüfen."})

        # Consolidate findings
        final_finding = {"category": "OK", "description": "Alle Prüfungen für den IT-Grundschutz-Check waren erfolgreich."}
        if findings:
            final_finding["category"] = "AS" if any(f['category'] == 'AS' for f in findings) else "AG"
            final_finding["description"] = "Zusammenfassung: " + " | ".join([f['description'] for f in findings])

        return {"detailsZumItGrundschutzCheck": {"answers": answers, "finding": final_finding}}

    def _check_document_coverage(self) -> Dict[str, Any]:
        """Checks if all critical BSI document types are present."""
        REQUIRED_CATEGORIES = {
            "Sicherheitsleitlinie", "Strukturanalyse", "Schutzbedarfsfeststellung",
            "Modellierung", "Grundschutz-Check", "Risikoanalyse", "Realisierungsplan"
        }
        present_categories = set(self._doc_map.keys())
        missing_categories = REQUIRED_CATEGORIES - present_categories

        if not missing_categories:
            return {"category": "OK", "description": "Alle kritischen Dokumententypen sind vorhanden."}
        else:
            desc = f"Kritische Dokumente fehlen: {', '.join(sorted(list(missing_categories)))}. Dies ist eine schwerwiegende Abweichung."
            logging.warning(f"Document coverage check failed. Missing: {missing_categories}")
            return {"category": "AS", "description": desc}

    def _build_execution_plan_from_template(self) -> List[Dict[str, Any]]:
        """Parses master_report_template.json to build a dynamic list of tasks."""
        plan = []
        template = self._load_asset_json(self.TEMPLATE_PATH)
        ch3_template = template.get("bsiAuditReport", {}).get("dokumentenpruefung", {})
        
        for subchapter_name, subchapter_data in ch3_template.items():
             if not isinstance(subchapter_data, dict): continue
             task = self._create_task_from_section(subchapter_name, subchapter_data)
             if task: plan.append(task)
             for section_key, section_data in subchapter_data.items():
                if isinstance(section_data, dict):
                    task = self._create_task_from_section(section_key, section_data)
                    if task: plan.append(task)
        return plan

    def _create_task_from_section(self, key: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """Creates a single task dictionary for the execution plan."""
        task_config = self.prompt_config["stages"]["Chapter-3"].get(key)
        if not task_config: return None

        task = {"key": key, "type": task_config.get("type", "ai_driven")}
        if task["type"] == "custom_logic": return task

        task["schema_path"] = task_config["schema_path"]
        task["source_categories"] = task_config.get("source_categories")

        if task["type"] == "ai_driven" or key == 'modellierungsdetails':
            generic_prompt = self.prompt_config["stages"]["Chapter-3"]["generic_question"]["prompt"]
            # For modellierungsdetails, the prompt is custom in the config
            task['prompt'] = task_config.get('prompt', generic_prompt)
            questions = [item["questionText"] for item in data.get("content", []) if item.get("type") == "question"]
            task["questions_formatted"] = "\n".join(f"{i+1}. {q}" for i, q in enumerate(questions))
        elif task["type"] == "summary":
            task["prompt"] = self.prompt_config["stages"]["Chapter-3"]["generic_summary"]["prompt"]
            task["summary_topic"] = data.get("title", key)
        return task

    async def _process_ai_subchapter(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """Generates content for a single AI-driven subchapter."""
        key, schema_path = task["key"], task["schema_path"]
        
        prompt_format_args = {"questions": task.get("questions_formatted", "")}

        # Task C: Inject ground truth context for modellierungsdetails
        if key == 'modellierungsdetails':
            ground_truth_map = await self._get_ground_truth_map()
            zielobjekte_list = ground_truth_map.get('zielobjekte', [])
            prompt_format_args['zielobjekte_json'] = json.dumps(zielobjekte_list, indent=2, ensure_ascii=False)
        
        prompt = task["prompt"].format(**prompt_format_args)
        uris = self.rag_client.get_gcs_uris_for_categories(task.get("source_categories"))
        
        if not uris and task.get("source_categories") is not None:
             return {key: {"error": f"No source documents for categories: {task.get('source_categories')}"}}
        try:
            data = await self.ai_client.generate_json_response(prompt, self._load_asset_json(schema_path), uris, f"Chapter-3: {key}")
            if key == "aktualitaetDerReferenzdokumente":
                coverage_finding = self._check_document_coverage()
                if coverage_finding['category'] != 'OK': data['finding'] = coverage_finding
            return {key: data}
        except Exception as e:
            logging.error(f"Failed to generate for {key}: {e}", exc_info=True)
            return {key: {"error": str(e)}}

    async def _process_summary_subchapter(self, task: Dict[str, Any], previous_findings: str) -> Dict[str, Any]:
        """Generates a summary/verdict for a subchapter."""
        key = task["key"]
        prompt = task["prompt"].format(summary_topic=task["summary_topic"], previous_findings=previous_findings)
        try:
            return {key: await self.ai_client.generate_json_response(prompt, self._load_asset_json(task["schema_path"]), request_context_log=f"Chapter-3 Summary: {key}")}
        except Exception as e:
            return {key: {"error": str(e)}}

    def _get_findings_from_results(self, results_list: List[Dict]) -> str:
        """Extracts and formats findings from a list of results for summary prompts."""
        findings = []
        for res_dict in results_list:
            if not res_dict: continue
            result_data = list(res_dict.values())[0]
            if isinstance(result_data, dict) and isinstance(result_data.get('finding'), dict):
                finding = result_data['finding']
                if finding.get('category') != "OK":
                    findings.append(f"- [{finding.get('category')}]: {finding.get('description')}")
        return "\n".join(findings) if findings else "No specific findings were generated."

    async def run(self, force_overwrite: bool = False) -> dict:
        """Executes the dynamically generated plan for Chapter 3."""
        logging.info(f"Executing dynamically generated plan for stage: {self.STAGE_NAME}")
        
        aggregated_results, processed_results = {}, []
        custom_tasks = [t for t in self.execution_plan if t and t.get("type") == "custom_logic"]
        ai_tasks = [t for t in self.execution_plan if t and (t.get("type") == "ai_driven" or t.get("key") == "modellierungsdetails")]
        summary_tasks = [t for t in self.execution_plan if t and t.get("type") == "summary"]

        for task in custom_tasks:
            key = task['key']
            logging.info(f"--- Processing custom logic task: {key} ---")
            if key == 'detailsZumItGrundschutzCheck':
                result = await self._process_details_zum_it_grundschutz_check()
                processed_results.append(result)
                aggregated_results.update(result)
        
        if ai_tasks:
            ai_coroutines = [self._process_ai_subchapter(task) for task in ai_tasks]
            ai_results = await asyncio.gather(*ai_coroutines)
            processed_results.extend(ai_results)
            for res in ai_results: aggregated_results.update(res)

        if summary_tasks:
            findings_text = self._get_findings_from_results(processed_results)
            summary_coroutines = [self._process_summary_subchapter(task, findings_text) for task in summary_tasks]
            for res in await asyncio.gather(*summary_coroutines): aggregated_results.update(res)

        logging.info(f"Successfully aggregated results for all of stage {self.STAGE_NAME}")
        return aggregated_results
==== bsi-audit-automator/src/audit/stages/stage_4_pruefplan.py ====
# src/audit/stages/stage_4_pruefplan.py
import logging
import json
import asyncio
from typing import Dict, Any
from google.cloud.exceptions import NotFound

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient

class Chapter4Runner:
    """
    Handles generating the audit plan for Chapter 4 "Erstellung eines Prüfplans".
    It uses the ground-truth system map (generated by Chapter 3) to create a realistic
    and accurate audit plan.
    """
    STAGE_NAME = "Chapter-4"
    PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"
    GROUND_TRUTH_MAP_PATH = "output/results/intermediate/system_structure_map.json"


    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self.prompt_config = self._load_asset_json(self.PROMPT_CONFIG_PATH)
        self.subchapter_definitions = self._load_subchapter_definitions()
        self.ground_truth_map = None
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _load_ground_truth_map(self) -> None:
        """Loads the ground truth system structure map from GCS."""
        try:
            self.ground_truth_map = self.gcs_client.read_json(self.GROUND_TRUTH_MAP_PATH)
            logging.info(f"Successfully loaded ground truth map for Chapter 4 planning.")
        except NotFound:
            logging.warning(f"Ground truth map not found at '{self.GROUND_TRUTH_MAP_PATH}'. Audit plan will be less accurate.")
            self.ground_truth_map = {} # Ensure it's not None
        except Exception as e:
            logging.error(f"Failed to load or parse ground truth map: {e}", exc_info=True)
            self.ground_truth_map = {}

    def _load_subchapter_definitions(self) -> Dict[str, Any]:
        """
        Loads definitions for all Chapter 4 subchapters from the central prompt config.
        The logic for which Baustein selection to run is now based on the AUDIT_TYPE.
        """
        logging.info(f"Loading Chapter 4 definitions for audit type: {self.config.audit_type}")
        definitions = {}
        ch4_config = self.prompt_config["stages"]["Chapter-4"]

        # AI-driven Baustein selection, conditional on the specific audit type
        if self.config.audit_type == "Zertifizierungsaudit":
            logging.info("Loading definitions for 'Zertifizierungsaudit'.")
            definitions["auswahlBausteineErstRezertifizierung"] = ch4_config["auswahlBausteineErstRezertifizierung"]
        elif self.config.audit_type == "1. Überwachungsaudit":
            logging.info("Loading definitions for '1. Überwachungsaudit'.")
            definitions["auswahlBausteine1Ueberwachungsaudit"] = ch4_config["auswahlBausteine1Ueberwachungsaudit"]
        elif self.config.audit_type == "2. Überwachungsaudit":
            logging.info("Loading definitions for '2. Überwachungsaudit'.")
            definitions["auswahlBausteine2Ueberwachungsaudit"] = ch4_config["auswahlBausteine2Ueberwachungsaudit"]
        else:
            logging.warning(f"Unknown audit type '{self.config.audit_type}'. No Baustein selection definitions loaded.")
            
        # Common parts for all audit types
        definitions["auswahlStandorte"] = {
            "key": "4.1.4",
            "type": "deterministic",
            "table": {
                "rows": [{"Standort": "Hauptstandort", "Erst- bzw. Rezertifizierung": "Ja", "1. Überwachungsaudit": "Ja", "2. Überwachungsaudit": "Ja", "Begründung für die Auswahl": "Zentraler Standort mit kritischer Infrastruktur."}]
            }
        }
        definitions["auswahlMassnahmenAusRisikoanalyse"] = ch4_config["auswahlMassnahmenAusRisikoanalyse"]

        # Mark the type for processing
        for key in definitions:
            if "prompt" in definitions[key]:
                definitions[key]["type"] = "ai_driven"

        return definitions

    async def _process_single_subchapter(self, name: str, definition: dict) -> Dict[str, Any]:
        """Generates planning content for a single subchapter, supporting AI and deterministic modes."""
        logging.info(f"Starting planning for subchapter: {definition.get('key', name)} ({name})")
        
        if definition.get("type") == "deterministic":
            logging.info(f"Processing '{name}' deterministically.")
            return {name: {"table": definition["table"]}}

        # AI-driven
        prompt_template = definition["prompt"]
        # NEW: Inject the ground truth map into the prompt.
        prompt = prompt_template.format(
            ground_truth_map_json=json.dumps(self.ground_truth_map, indent=2, ensure_ascii=False)
        )
        schema = self._load_asset_json(definition["schema_path"])
        
        try:
            generated_data = await self.ai_client.generate_json_response(
                prompt=prompt,
                json_schema=schema,
                request_context_log=f"Chapter-4: {name}"
            )
            logging.info(f"Successfully generated plan for subchapter {definition.get('key', name)}")
            # The AI response is the table content itself, e.g. {"rows": [...]}.
            return {name: generated_data}
        except Exception as e:
            logging.error(f"Failed to generate plan for subchapter {definition.get('key', name)}: {e}", exc_info=True)
            return {name: {"rows": []}} # Return empty structure on failure

    async def run(self, force_overwrite: bool = False) -> dict:
        """
        Executes the planning logic for all of Chapter 4 in parallel.
        """
        logging.info(f"Executing stage: {self.STAGE_NAME}")

        # Load the map first, it's a dependency for the prompts
        self._load_ground_truth_map()

        if not self.subchapter_definitions:
            logging.warning(f"No subchapter definitions found. Skipping Chapter 4.")
            return {}

        tasks = [self._process_single_subchapter(name, definition) for name, definition in self.subchapter_definitions.items()]
        results_list = await asyncio.gather(*tasks)

        aggregated_results = {}
        for res_dict in results_list:
            aggregated_results.update(res_dict)
            
        logging.info(f"Successfully aggregated planning results for stage {self.STAGE_NAME}")
        return aggregated_results
==== bsi-audit-automator/src/audit/stages/stage_5_vor_ort_audit.py ====
# file: src/audit/stages/stage_5_vor_ort_audit.py
import logging
import json
from typing import Dict, Any, List, Tuple
from google.cloud.exceptions import NotFound

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient
from src.audit.stages.control_catalog import ControlCatalog
from src.constants import EXTRACTED_CHECK_DATA_PATH, GROUND_TRUTH_MAP_PATH

class Chapter5Runner:
    """
    Handles generating content for Chapter 5 "Vor-Ort-Audit".
    It deterministically prepares the control checklist for the manual audit,
    enriching it with data extracted in prior stages.
    """
    STAGE_NAME = "Chapter-5"

    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self.control_catalog = ControlCatalog()
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    def _load_system_structure_map(self) -> Dict[str, Any]:
        """
        Loads the ground-truth system structure map which contains the authoritative
        Baustein-to-Zielobjekt mappings.
        """
        try:
            system_map = self.gcs_client.read_json(GROUND_TRUTH_MAP_PATH)
            logging.info(f"Successfully loaded ground truth map from: {GROUND_TRUTH_MAP_PATH}")
            return system_map
        except NotFound:
            logging.error(f"FATAL: Ground truth map '{GROUND_TRUTH_MAP_PATH}' not found. Cannot generate Chapter 5 checklist. Please run the 'Grundschutz-Check-Extraction' stage first.")
            raise
        except Exception as e:
            logging.error(f"Failed to load or parse ground truth map: {e}", exc_info=True)
            raise

    def _load_extracted_check_data(self) -> Dict[Tuple[str, str], Dict[str, Any]]:
        """
        Loads the refined Grundschutz-Check data and creates a lookup map
        keyed by a composite tuple of (requirement_id, zielobjekt_kuerzel)
        for efficient, context-aware access.
        """
        try:
            data = self.gcs_client.read_json(EXTRACTED_CHECK_DATA_PATH)
            anforderungen_list = data.get("anforderungen", [])
            # The key is a tuple of the requirement ID and the target object's short ID (Kürzel).
            # This correctly handles the same requirement applied to multiple objects.
            lookup_map = {
                (item['id'], item['zielobjekt_kuerzel']): item 
                for item in anforderungen_list if 'id' in item and 'zielobjekt_kuerzel' in item
            }
            logging.info(f"Successfully loaded and mapped {len(lookup_map)} unique requirement-object pairs for Chapter 5.")
            return lookup_map
        except NotFound:
            logging.warning(f"Refined check data file '{EXTRACTED_CHECK_DATA_PATH}' not found. Checklist will not contain customer explanations. Please run the 'Grundschutz-Check-Extraction' stage first.")
            return {}
        except Exception as e:
            logging.error(f"Failed to load or parse refined check data: {e}", exc_info=True)
            return {}

    def _generate_control_checklist(self, chapter_4_data: Dict[str, Any], system_structure_map: Dict[str, Any], extracted_data_map: Dict[Tuple[str, str], Dict[str, Any]]) -> Dict[str, Any]:
        """
        Deterministically generates the control checklist for subchapter 5.5.2,
        using the specific Zielobjekt selected in the audit plan (Chapter 4)
        to select the exact instance to audit and populate with customer data.
        """
        name = "verifikationDesITGrundschutzChecks"
        logging.info(f"Generating control checklist for {name} based on the specific audit plan...")
        
        # Combine bausteine from all possible sections of chapter 4
        selected_bausteine = []
        baustein_sections = [
            "auswahlBausteineErstRezertifizierung", 
            "auswahlBausteine1Ueberwachungsaudit", 
            "auswahlBausteine2Ueberwachungsaudit"
        ]
        for section in baustein_sections:
            section_data = chapter_4_data.get(section, {})
            if isinstance(section_data, dict):
                 selected_bausteine.extend(section_data.get("rows", []))
        
        if not selected_bausteine:
            logging.warning("No Bausteine found in Chapter 4 results. Checklist for 5.5.2 will be empty.")
            return {name: {"einzelergebnisse": {"bausteinPruefungen": []}}}

        baustein_pruefungen_list = []
        for i, baustein_plan_item in enumerate(selected_bausteine):
            baustein_id_full = baustein_plan_item.get("Baustein", "")
            if not baustein_id_full: continue
            baustein_id = baustein_id_full.split(" ")[0]

            # --- ROBUSTNESS FIX (Task H) ---
            # Directly get the name and Kürzel from the plan. No more fragile name-based lookups.
            zielobjekt_name_from_plan = baustein_plan_item.get("Zielobjekt-Name")
            planned_zielobjekt_kuerzel = baustein_plan_item.get("Zielobjekt-Kürzel")

            if not planned_zielobjekt_kuerzel:
                logging.warning(f"Could not find 'Zielobjekt-Kürzel' in audit plan for Baustein '{baustein_id}'. Specific details for its controls will be missing.")

            controls = self.control_catalog.get_controls_for_baustein_id(baustein_id)
            
            anforderungen_list = []
            for control in controls:
                control_id = control.get("id", "N/A")
                # The lookup key is now robustly created using the Kürzel from the plan.
                lookup_key = (control_id, planned_zielobjekt_kuerzel) if planned_zielobjekt_kuerzel else None
                extracted_details = extracted_data_map.get(lookup_key, {})
                
                customer_explanation = extracted_details.get("umsetzungserlaeuterung", "Keine spezifische Angabe für dieses Zielobjekt im Grundschutz-Check gefunden.")
                bewertung_status_raw = extracted_details.get("umsetzungsstatus", "N/A")

                status_map = {"Ja": "Umgesetzt", "Nein": "Nicht umgesetzt", "teilweise": "Teilweise umgesetzt", "entbehrlich": "Entbehrlich"}
                final_bewertung_status = status_map.get(bewertung_status_raw, bewertung_status_raw)

                anforderungen_list.append({
                    "nummer": control_id,
                    "anforderung": control.get("title", "N/A"),
                    "bewertung": final_bewertung_status,
                    "dokuAntragsteller": customer_explanation,
                    "pruefmethode": { "D": False, "I": False, "C": False, "S": False, "A": False, "B": False },
                    "auditfeststellung": "",
                    "abweichungen": ""
                })
            
            # Create the new subchapter structure
            baustein_pruefungen_list.append({
                "subchapterNumber": f"5.5.2.{i+1}",
                "title": f"Prüfung für Baustein: {baustein_id_full}",
                "baustein": baustein_id_full,
                "bezogenAufZielobjekt": zielobjekt_name_from_plan,
                "auditiertAm": "",
                "auditor": "",
                "befragtWurde": "",
                "anforderungen": anforderungen_list
            })

        logging.info(f"Generated checklist with {len(baustein_pruefungen_list)} specific Baustein/Zielobjekt subchapters for manual audit.")
        return {name: {"einzelergebnisse": {"bausteinPruefungen": baustein_pruefungen_list}}}
        
    def _generate_risikoanalyse_checklist(self, chapter_4_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generates the checklist for risk analysis measures (5.6.2) based on
        the measures selected in Chapter 4.1.5.
        """
        name = "risikoanalyseA5"
        logging.info(f"Deterministically generating checklist for {name} (5.6.2)...")
        
        selected_measures = chapter_4_data.get("auswahlMassnahmenAusRisikoanalyse", {}).get("rows", [])
        
        if not selected_measures:
            logging.warning("No measures from risk analysis found in Chapter 4 results. Checklist for 5.6.2 will be empty.")
            return {name: {"einzelergebnisseDerRisikoanalyse": {"massnahmenPruefungen": []}}}

        massnahmen_pruefungen_list = []
        for measure in selected_measures:
            massnahmen_pruefungen_list.append({
                "massnahme": measure.get("Maßnahme", "N/A"),
                "zielobjekt": measure.get("Zielobjekt", "N/A"),
                "bewertung": "",
                "dokuAntragsteller": "",
                "pruefmethode": { "D": False, "I": False, "C": False, "S": False, "A": False, "B": False },
                "auditfeststellung": "",
                "abweichungen": ""
            })
            
        logging.info(f"Generated checklist with {len(massnahmen_pruefungen_list)} risk analysis measures.")
        return {name: {"einzelergebnisseDerRisikoanalyse": {"massnahmenPruefungen": massnahmen_pruefungen_list}}}

    async def run(self, force_overwrite: bool = False) -> dict:
        """
        Executes the generation logic for Chapter 5.
        """
        logging.info(f"Executing stage: {self.STAGE_NAME}")
        
        # Load all dependencies first
        try:
            ch4_results_path = f"{self.config.output_prefix}results/Chapter-4.json"
            chapter_4_data = self.gcs_client.read_json(ch4_results_path)
            logging.info("Successfully loaded dependency: Chapter 4 results.")
        except Exception as e:
            logging.error(f"Could not load Chapter 4 results, which are required for Chapter 5. Aborting stage. Error: {e}")
            raise

        system_structure_map = self._load_system_structure_map()
        extracted_check_data_map = self._load_extracted_check_data()
        
        checklist_result = self._generate_control_checklist(chapter_4_data, system_structure_map, extracted_check_data_map)
        risiko_result = self._generate_risikoanalyse_checklist(chapter_4_data)
        
        final_result = {**checklist_result, **risiko_result}
        
        logging.info(f"Successfully prepared data for stage {self.STAGE_NAME}")
        return final_result
==== bsi-audit-automator/src/audit/stages/stage_7_anhang.py ====
# src/audit/stages/stage_7_anhang.py
import logging
import json
from typing import Dict, Any

from src.config import AppConfig
from src.clients.gcs_client import GcsClient

class Chapter7Runner:
    """
    Handles generating the appendix for Chapter 7.
    - 7.1 is generated deterministically by listing GCS source files.
    - 7.2 (Deviations) is populated by the ReportGenerator from the central findings file.
    """
    STAGE_NAME = "Chapter-7"

    def __init__(self, config: AppConfig, gcs_client: GcsClient):
        self.config = config
        self.gcs_client = gcs_client
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    async def _generate_referenzdokumente_table(self) -> Dict[str, Any]:
        """Generates the table of reference documents by listing source files in GCS."""
        logging.info("Generating subchapter 7.1 (Referenzdokumente) from GCS file list.")
        try:
            source_files = self.gcs_client.list_files()
            rows = []
            for i, blob in enumerate(source_files):
                rows.append({
                    "Nummer": f"A.{i}",
                    "Kurzbezeichnung": blob.name.split('/')[-1],
                    "Dateiname / Verweis": blob.name,
                    "Version, Datum": blob.updated.strftime("%Y-%m-%d") if blob.updated else "N/A",
                    "Relevante Änderungen": "Initial eingereicht für Audit."
                })
            # The key must match the structure in master_report_template.json
            return {"referenzdokumente": {"table": {"rows": rows}}}
        except Exception as e:
            logging.error(f"Failed to generate Referenzdokumente table: {e}", exc_info=True)
            return {"referenzdokumente": {"table": {"rows": []}}}

    async def run(self, force_overwrite: bool = False) -> dict:
        """Executes the generation logic for Chapter 7."""
        logging.info(f"Executing stage: {self.STAGE_NAME}")
        # Only one task remains for this chapter.
        result = await self._generate_referenzdokumente_table()
        logging.info(f"Successfully generated data for stage {self.STAGE_NAME}")
        return result
==== bsi-audit-automator/src/audit/stages/stage_gs_check_extraction.py ====
# bsi-audit-automator/src/audit/stages/stage_gs_check_extraction.py
import logging
import json
from typing import Dict, Any

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.document_ai_client import DocumentAiClient
from src.clients.ai_client import AiClient
from src.clients.rag_client import RagClient
from src.constants import EXTRACTED_CHECK_DATA_PATH, GROUND_TRUTH_MAP_PATH

from .gs_extraction.ground_truth_mapper import GroundTruthMapper
from .gs_extraction.document_processor import DocumentProcessor
from .gs_extraction.block_grouper import BlockGrouper
from .gs_extraction.ai_refiner import AiRefiner


class GrundschutzCheckExtractionRunner:
    """
    Main orchestrator for the Ground-Truth-Driven Semantic Chunking pipeline.
    Coordinates the four-stage process of extracting structured security requirements
    from the Grundschutz-Check document.
    """
    STAGE_NAME = "Grundschutz-Check-Extraction"

    def __init__(self, config: AppConfig, gcs_client: GcsClient, doc_ai_client: DocumentAiClient, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.gcs_client = gcs_client
        
        # Initialize specialized processors
        self.ground_truth_mapper = GroundTruthMapper(ai_client, rag_client, gcs_client)
        self.document_processor = DocumentProcessor(gcs_client, doc_ai_client, rag_client, config)
        self.block_grouper = BlockGrouper(gcs_client)
        self.ai_refiner = AiRefiner(ai_client, gcs_client)
        
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    async def run(self, force_overwrite: bool = False) -> Dict[str, Any]:
        """Main execution method for the full extraction and refinement pipeline."""
        logging.info(f"Executing stage: {self.STAGE_NAME}")
        
        try:
            # Step 1: Establish Ground Truth system structure
            system_map = await self.ground_truth_mapper.create_system_structure_map(force_overwrite)
            
            # Step 2: Process document with Document AI Layout Parser
            await self.document_processor.execute_layout_parser_workflow(force_overwrite)

            # Step 3: Group layout blocks by Zielobjekt context
            await self.block_grouper.group_layout_blocks_by_zielobjekt(system_map, force_overwrite)
            
            # Step 4: Refine grouped blocks with AI to extract structured requirements
            await self.ai_refiner.refine_grouped_blocks_with_ai(system_map, force_overwrite)

            return {"status": "success", "message": f"Stage {self.STAGE_NAME} completed successfully."}
            
        except Exception as e:
            logging.error(f"Stage {self.STAGE_NAME} failed: {e}", exc_info=True)
            return {"status": "error", "message": f"Stage {self.STAGE_NAME} failed: {str(e)}"}
==== bsi-audit-automator/src/audit/stages/stage_previous_report_scan.py ====
# src/audit/stages/stage_previous_report_scan.py
import logging
import json
import asyncio
from typing import Dict, Any

from src.config import AppConfig
from src.clients.ai_client import AiClient
from src.clients.rag_client import RagClient

class PreviousReportScanner:
    """
    A dedicated stage to scan a previous audit report and extract key data.
    It runs three extraction tasks in parallel for maximum efficiency.
    """
    STAGE_NAME = "Scan-Report"
    PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"

    def __init__(self, config: AppConfig, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.ai_client = ai_client
        self.rag_client = rag_client
        self.prompt_config = self._load_asset_json(self.PROMPT_CONFIG_PATH)
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    async def _run_extraction_task(self, task_name: str, gcs_uri: str) -> Dict[str, Any]:
        """
        Runs a single AI extraction task for a part of the report.
        
        Args:
            task_name: The key from the prompt_config (e.g., 'extract_chapter_1').
            gcs_uri: The GCS URI of the previous audit report.

        Returns:
            A dictionary containing the extracted data for that task.
        """
        logging.info(f"Starting extraction for task: {task_name}")
        try:
            task_config = self.prompt_config["stages"][self.STAGE_NAME][task_name]
            prompt = task_config["prompt"]
            schema = self._load_asset_json(task_config["schema_path"])
            
            response = await self.ai_client.generate_json_response(
                prompt=prompt,
                json_schema=schema,
                gcs_uris=[gcs_uri],
                request_context_log=f"{self.STAGE_NAME}: {task_name}"
            )
            return response
        except Exception as e:
            logging.error(f"Extraction task '{task_name}' failed: {e}", exc_info=True)
            return {task_name: {"error": str(e)}} # Return error structure

    async def run(self, force_overwrite: bool = False) -> dict:
        """
        Executes the logic for scanning the previous audit report.
        """
        logging.info(f"Executing stage: {self.STAGE_NAME}")
        
        # 1. Find the previous audit report document
        report_uris = self.rag_client.get_gcs_uris_for_categories(["Vorheriger-Auditbericht"])
        if not report_uris:
            logging.warning("No document with category 'Vorheriger-Auditbericht' found. Skipping stage.")
            return {"status": "skipped", "reason": "No previous audit report found."}
        
        # Use the first report found if multiple are classified
        report_uri = report_uris[0]
        logging.info(f"Found previous audit report to scan: {report_uri}")

        # 2. Define and run all extraction tasks in parallel
        extraction_tasks = ["extract_chapter_1", "extract_chapter_4", "extract_chapter_7"]
        coroutines = [self._run_extraction_task(task_name, report_uri) for task_name in extraction_tasks]
        
        results_list = await asyncio.gather(*coroutines)

        # 3. Aggregate results into a single dictionary
        final_result = {}
        for result in results_list:
            final_result.update(result)
            
        logging.info(f"Successfully completed all extractions for stage {self.STAGE_NAME}")
        return final_result
==== bsi-audit-automator/src/clients/ai_client.py ====
# src/clients/ai_client.py
import logging
import json
import asyncio
import time
import datetime
from typing import List, Dict, Any, Optional

from google.cloud import aiplatform
from google.api_core import exceptions as api_core_exceptions
from vertexai.generative_models import GenerativeModel, GenerationConfig, Part

from src.config import AppConfig
from src.constants import GROUND_TRUTH_MODEL

MAX_RETRIES = 5
PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"


class AiClient:
    """A client for all Vertex AI model interactions, using the aiplatform SDK."""

    def __init__(self, config: AppConfig):
        self.config = config
        
        with open(PROMPT_CONFIG_PATH, 'r', encoding='utf-8') as f:
            prompt_config = json.load(f)
        
        base_system_message = prompt_config.get("system_message", "")
        if not base_system_message:
            logging.warning("System message is empty. AI calls will not have a predefined persona.")

        # Append the current date to the system prompt
        current_date = datetime.date.today().strftime("%Y-%m-%d")
        self.system_message = f"{base_system_message}\n\nImportant: Today's date is {current_date}."

        # aiplatform.init(project=config.gcp_project_id, location=config.region)
        aiplatform.init(project=config.gcp_project_id, location="global")
        
        # Default model instance
        self.generative_model = GenerativeModel(
            GROUND_TRUTH_MODEL, system_instruction=self.system_message
        )
        
        # Cache for alternative model instances
        self._model_cache = {GROUND_TRUTH_MODEL: self.generative_model}
        
        self.semaphore = asyncio.Semaphore(config.max_concurrent_ai_requests)

        logging.info(f"Vertex AI Client instantiated for project '{config.gcp_project_id}' in region '{config.region}'.")
        logging.info(f"System Message Context includes today's date: {current_date}")

    def _get_model_instance(self, model_name: str) -> GenerativeModel:
        """
        Get or create a GenerativeModel instance for the specified model.
        
        Args:
            model_name: The model name (e.g., 'gemini-2.5-pro', 'gemini-2.5-flash')
            
        Returns:
            GenerativeModel instance for the specified model
        """
        if model_name not in self._model_cache:
            logging.info(f"Creating new model instance for '{model_name}'")
            self._model_cache[model_name] = GenerativeModel(
                model_name, system_instruction=self.system_message
            )
        return self._model_cache[model_name]

    async def generate_json_response(
        self, 
        prompt: str, 
        json_schema: Dict[str, Any], 
        gcs_uris: List[str] = None, 
        request_context_log: str = "Generic AI Request",
        model_override: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Generates a JSON response from the AI model, enforcing a specific schema and
        optionally providing GCS files as context. Implements an async retry loop
        with exponential backoff and connection limiting.

        Args:
            prompt: The text prompt for the model.
            json_schema: The JSON schema to enforce on the model's output.
            gcs_uris: A list of 'gs://...' URIs pointing to PDF files for context.
            request_context_log: A string to identify the request source in logs.
            model_override: Optional model name to use instead of the default.

        Returns:
            The parsed JSON response from the model.
        """
        try:
            schema_for_api = json.loads(json.dumps(json_schema))
            schema_for_api.pop("$schema", None)
        except Exception as e:
            logging.error(f"Failed to process JSON schema before API call: {e}")
            raise ValueError("Invalid JSON schema provided.") from e

        gen_config = GenerationConfig(
            response_mime_type="application/json",
            response_schema=schema_for_api,
            max_output_tokens=65535,

            
            temperature=0.2,
        )

        # Select the appropriate model
        model_to_use = model_override if model_override else GROUND_TRUTH_MODEL
        generative_model = self._get_model_instance(model_to_use)

        # Build the content list. The system message is now handled by the model constructor.
        contents = [prompt]
        if gcs_uris:
            for uri in gcs_uris:
                contents.append(Part.from_uri(uri, mime_type="application/pdf"))
            if self.config.is_test_mode:
                logging.info(f"Attaching {len(gcs_uris)} GCS files to the prompt.")

        async with self.semaphore:
            for attempt in range(MAX_RETRIES):
                try:
                    model_info = f" (using {model_to_use})" if model_override else ""
                    logging.info(f"[{request_context_log}] Attempt {attempt + 1}/{MAX_RETRIES}: Calling Gemini model '{model_to_use}'{model_info}...")
                    response = await generative_model.generate_content_async(
                        contents=contents,
                        generation_config=gen_config,
                    )

                    if not response.candidates:
                        raise ValueError("The model response contained no candidates.")

                    finish_reason = response.candidates[0].finish_reason.name
                    if finish_reason not in ["STOP", "MAX_TOKENS"]:
                        raise ValueError(f"Model finished with non-OK reason: '{finish_reason}'")

                    response_json = json.loads(response.text)
                    logging.info(f"[{request_context_log}] Successfully generated and parsed JSON response on attempt {attempt + 1}.")
                    return response_json

                except (api_core_exceptions.GoogleAPICallError, Exception) as e:
                    wait_time = 2 ** attempt
                    if attempt == MAX_RETRIES - 1:
                        logging.critical(f"[{request_context_log}] AI generation failed after all {MAX_RETRIES} retries.", exc_info=True)
                        raise

                    if isinstance(e, api_core_exceptions.GoogleAPICallError):
                        logging.warning(f"[{request_context_log}] Generation attempt {attempt + 1} failed with Google API Error (Code: {e.code}): {e.message}. Retrying in {wait_time}s...")
                    else:
                        logging.warning(f"[{request_context_log}] Generation attempt {attempt + 1} failed with an exception: {e}. Retrying in {wait_time}s...")

                    await asyncio.sleep(wait_time)

        raise RuntimeError("AI generation failed unexpectedly after exhausting all retries.")
==== bsi-audit-automator/src/clients/document_ai_client.py ====
# src/clients/document_ai_client.py
import logging
import asyncio
import json
from typing import Dict, Any, Optional
from google.cloud import documentai_v1 as documentai
from google.cloud.documentai_v1.types import (
    BatchDocumentsInputConfig,
    BatchProcessRequest,
    DocumentOutputConfig,
    GcsDocument,
    GcsDocuments
)

from google.api_core.client_options import ClientOptions
from google.api_core.exceptions import GoogleAPICallError

from src.config import AppConfig
from src.clients.gcs_client import GcsClient

class DocumentAiClient:
    """A client for handling interactions with Google Cloud Document AI."""

    def __init__(self, config: AppConfig, gcs_client: GcsClient):
        self.config = config
        self.gcs_client = gcs_client

        if not self.config.doc_ai_processor_name:
            raise ValueError("Document AI processor name is not configured.")

        # Parse and validate name
        self.processor_name = self.config.doc_ai_processor_name.strip()  # Trim any extras
        parts = self.processor_name.split('/')
        if len(parts) != 6 or parts[0] != 'projects' or parts[2] != 'locations' or parts[4] != 'processors':
            raise ValueError(f"Invalid processor name format: '{self.processor_name}'. Expected 'projects/{{project}}/locations/{{location}}/processors/{{processor}}'.")

        self.location = parts[3]
        if self.location == 'us':
            opts = ClientOptions(api_endpoint="documentai.googleapis.com")
        else:
            opts = ClientOptions(api_endpoint=f"{self.location}-documentai.googleapis.com")
        self.client = documentai.DocumentProcessorServiceClient(client_options=opts)
        self.semaphore = asyncio.Semaphore(config.max_concurrent_ai_requests)
        logging.info(f"DocumentAI Client initialized for processor '{self.processor_name}' in location '{self.location}'.")
        logging.info(f"DocumentAI concurrent request limit set to: {config.max_concurrent_ai_requests}")

    def _adjust_text_anchors_recursive(self, data: Any, offset: int):
        """
        Recursively finds and adjusts 'startIndex' and 'endIndex' in a Document AI structure
        by adding the given offset. This is critical when merging sharded results.
        """
        if isinstance(data, dict):
            if 'textAnchor' in data and data['textAnchor'].get('textSegments'):
                for segment in data['textAnchor']['textSegments']:
                    if segment.get('startIndex') is not None:
                        segment['startIndex'] = str(int(segment['startIndex']) + offset)
                    if segment.get('endIndex') is not None:
                        segment['endIndex'] = str(int(segment['endIndex']) + offset)
            
            for value in data.values():
                self._adjust_text_anchors_recursive(value, offset)
        elif isinstance(data, list):
            for item in data:
                self._adjust_text_anchors_recursive(item, offset)

    async def process_document_chunk_async(self, gcs_input_uri: str, gcs_output_prefix: str) -> Optional[str]:
        """
        Processes a single document chunk from GCS using batch processing and saves the result.
        This method is now idempotent on a per-chunk basis and uses a semaphore to limit concurrency.

        Args:
            gcs_input_uri: The 'gs://' path to the input PDF document chunk.
            gcs_output_prefix: The GCS prefix where the output JSON should be stored.

        Returns:
            The GCS path to the generated JSON result file, or None on failure.
        """
        input_filename = gcs_input_uri.split('/')[-1]  # e.g., 'chunk_0.pdf'
        chunk_basename = input_filename.replace('.pdf', '')  # e.g., 'chunk_0'
        output_json_filename = f"{chunk_basename}.json"  # e.g., 'chunk_0.json'
        gcs_output_json_path = f"{gcs_output_prefix}{output_json_filename}"
        
        # IDEMPOTENCY: Check if the result for this specific chunk already exists.
        if self.gcs_client.blob_exists(gcs_output_json_path):
            logging.info(f"Result for chunk '{gcs_input_uri}' already exists. Skipping processing.")
            return gcs_output_json_path

        async with self.semaphore:
            gcs_output_uri_for_api = f"gs://{self.config.bucket_name}/{gcs_output_prefix}"
            if not gcs_output_uri_for_api.endswith('/'):
                gcs_output_uri_for_api += '/'  # Ensure trailing slash for directory prefix
            logging.info(f"Starting Document AI batch processing for chunk '{gcs_input_uri}'.")
            
            input_config = GcsDocument(gcs_uri=gcs_input_uri, mime_type="application/pdf")
            batch_input_config = BatchDocumentsInputConfig(gcs_documents=GcsDocuments(documents=[input_config]))
            
            gcs_output_config = DocumentOutputConfig.GcsOutputConfig(gcs_uri=gcs_output_uri_for_api)
            output_config = DocumentOutputConfig(gcs_output_config=gcs_output_config)

            request = BatchProcessRequest(
                name=self.processor_name,
                input_documents=batch_input_config,
                document_output_config=output_config,
            )

            try:
                operation = self.client.batch_process_documents(request=request)
                logging.info(f"Waiting for Document AI operation for '{input_filename}' to complete...")
                await asyncio.to_thread(operation.result)
                logging.info(f"Document AI operation for '{input_filename}' completed.")
                
                # Get precise output folder from metadata (e.g., output/doc_ai_results/{op_id}/0/)
                from google.cloud.documentai_v1 import BatchProcessMetadata
                metadata = BatchProcessMetadata(operation.metadata)
                if not metadata.individual_process_statuses:
                    logging.error(f"No process statuses found for operation {operation.name}")
                    return None
                # Since one input document, take the first status
                output_gcs_destination = metadata.individual_process_statuses[0].output_gcs_destination
                output_folder = output_gcs_destination.replace(f"gs://{self.config.bucket_name}/", "")
                
                # List all JSON shards in this operation's output folder
                output_blobs = self.gcs_client.list_files(prefix=output_folder)
                shard_blobs = [b for b in output_blobs if b.name.endswith('.json') and chunk_basename in b.name.split('/')[-1]]
                
                if not shard_blobs:
                    logging.error(f"No result JSONs found in output path: {output_folder}")
                    return None
                
                # Merge shards if multiple (sort by name for page order)
                merged_data = {"text": "", "documentLayout": {"blocks": []}}
                text_offset = 0
                for blob in sorted(shard_blobs, key=lambda b: b.name):
                    shard_content = json.loads(await asyncio.to_thread(blob.download_as_text))
                    shard_text = shard_content.get("text", "")

                    if "documentLayout" in shard_content and "blocks" in shard_content["documentLayout"]:
                        blocks_to_process = shard_content["documentLayout"]["blocks"]
                        self._adjust_text_anchors_recursive(blocks_to_process, text_offset)
                        merged_data["documentLayout"]["blocks"].extend(blocks_to_process)
                    else:
                        logging.warning(f"Shard {blob.name} missing expected 'documentLayout.blocks'; skipping.")
                    
                    merged_data["text"] += shard_text
                    text_offset += len(shard_text)
                
                if not merged_data["documentLayout"]["blocks"]:
                    logging.error(f"No valid blocks found after merging shards for '{input_filename}'")
                    return None
                
                # Upload merged result to clean path
                merged_json_str = json.dumps(merged_data, ensure_ascii=False)
                await self.gcs_client.upload_from_string_async(merged_json_str, gcs_output_json_path)
                logging.info(f"Saved merged result for chunk to: {gcs_output_json_path}")
                
                # Clean up: Delete the raw shard files and any other blobs in the output folder
                blobs_to_delete = [blob.name for blob in output_blobs]
                if blobs_to_delete:
                    self.gcs_client.bucket.delete_blobs(blobs_to_delete)
                    logging.info(f"Deleted {len(blobs_to_delete)} raw shard files from {output_folder}")
                
                return gcs_output_json_path

            except GoogleAPICallError as e:
                logging.error(f"Document AI processing for chunk '{gcs_input_uri}' failed with API error: {e}", exc_info=True)
                return None
            except Exception as e:
                logging.error(f"An unexpected error occurred during Document AI processing for chunk '{gcs_input_uri}': {e}", exc_info=True)
                return None
==== bsi-audit-automator/src/clients/gcs_client.py ====
# src/clients/gcs_client.py
import logging
import asyncio
from google.cloud import storage
from src.config import AppConfig

class GcsClient:
    """A client for all Google Cloud Storage interactions."""

    def __init__(self, config: AppConfig):
        """
        Initializes the GCS client.

        Args:
            config: The application configuration object.
        """
        self.config = config
        self.storage_client = storage.Client(project=config.gcp_project_id)
        # We derive the bucket name from the config, which should be set by an env var
        # that comes from the terraform output.
        if not config.bucket_name:
            raise ValueError("GCS Bucket name is not configured in the environment.")
        self.bucket = self.storage_client.bucket(config.bucket_name)
        logging.info(f"GCS Client initialized for bucket: gs://{self.bucket.name}")

    def list_files(self, prefix: str = None) -> list[storage.Blob]:
        """
        Lists all files from a given GCS prefix.

        Returns:
            A list of GCS blob objects.
        """
        list_prefix = prefix if prefix is not None else self.config.source_prefix
        logging.info(f"Listing files from prefix: {list_prefix}")
        blobs = self.storage_client.list_blobs(
            self.bucket.name, prefix=list_prefix
        )
        # Filter for common document types, ignore empty "directory" blobs
        files = [blob for blob in blobs if "." in blob.name]
        logging.info(f"Found {len(files)} source files to process.")
        return files

    def download_blob_as_bytes(self, blob: storage.Blob) -> bytes:
        """Downloads a blob from GCS into memory as bytes."""
        logging.debug(f"Downloading blob: {blob.name}")
        return blob.download_as_bytes()

    async def upload_from_string_async(self, content: str, destination_blob_name: str, content_type: str = 'application/json'):
        """Asynchronously uploads a string content to a specified blob in GCS."""
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(
            None, self.upload_from_string, content, destination_blob_name, content_type
        )

    def upload_from_bytes(self, content: bytes, destination_blob_name: str, content_type: str = 'application/pdf'):
        """
        Synchronously uploads bytes content to a specified blob in GCS.

        Args:
            content: The bytes content to upload.
            destination_blob_name: The full path for the object in the bucket.
            content_type: The MIME type of the content.
        """
        logging.info(f"Uploading bytes content to gs://{self.bucket.name}/{destination_blob_name}")
        blob = self.bucket.blob(destination_blob_name)
        blob.upload_from_string(content, content_type=content_type) # upload_from_string can handle bytes

    async def upload_from_bytes_async(self, content: bytes, destination_blob_name: str, content_type: str = 'application/pdf'):
        """Asynchronously uploads bytes content to a specified blob in GCS."""
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(None, self.upload_from_bytes, content, destination_blob_name, content_type)

    def upload_from_string(self, content: str, destination_blob_name: str, content_type: str = 'application/json'):
        """
        Synchronously uploads a string content to a specified blob in GCS.

        Args:
            content: The string content to upload.
            destination_blob_name: The full path for the object in the bucket.
            content_type: The MIME type of the content.
        """
        logging.info(f"Uploading string content to gs://{self.bucket.name}/{destination_blob_name}")
        blob = self.bucket.blob(destination_blob_name)
        blob.upload_from_string(content, content_type=content_type)
        logging.info(f"Upload complete for {destination_blob_name}.")

    async def read_json_async(self, blob_name: str) -> dict:
        """Asynchronously downloads and parses a JSON file from GCS."""
        loop = asyncio.get_running_loop()
        # Use asyncio.to_thread in Python 3.9+ for a cleaner syntax
        return await loop.run_in_executor(None, self.read_json, blob_name)

    def read_json(self, blob_name: str) -> dict:
        """Downloads and parses a JSON file from GCS."""
        import json
        logging.info(f"Attempting to read JSON from: gs://{self.bucket.name}/{blob_name}")
        blob = self.bucket.blob(blob_name)
        content = blob.download_as_text() # This raises NotFound if not present.
        return json.loads(content)

    def read_text_file(self, blob_name: str) -> str:
        """Downloads and returns the content of a text-based file from GCS."""
        logging.info(f"Attempting to read text from: gs://{self.bucket.name}/{blob_name}")
        blob = self.bucket.blob(blob_name)
        return blob.download_as_text()

    def blob_exists(self, blob_name: str) -> bool:
        """Checks if a blob exists in the GCS bucket."""
        logging.debug(f"Checking for existence of blob: gs://{self.bucket.name}/{blob_name}")
        blob = self.bucket.blob(blob_name)
        return blob.exists()

    def copy_blob(self, source_blob_name: str, destination_blob_name: str):
        """Copies a blob within the same bucket."""
        source_blob = self.bucket.blob(source_blob_name)
        self.bucket.copy_blob(source_blob, self.bucket, destination_blob_name)
        logging.info(f"Copied gs://{self.bucket.name}/{source_blob_name} to gs://{self.bucket.name}/{destination_blob_name}")

    async def copy_blob_async(self, source_blob_name: str, destination_blob_name: str):
        """Asynchronously copies a blob within the same bucket."""
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(
            None, self.copy_blob, source_blob_name, destination_blob_name
        )
==== bsi-audit-automator/src/clients/rag_client.py ====
# src/clients/rag_client.py
import logging
import json
import asyncio
from typing import List, Dict, Any, Optional

from google.cloud.exceptions import NotFound

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient
from src.constants import DOCUMENT_CATEGORY_MAP_PATH

DOC_MAP_PATH = DOCUMENT_CATEGORY_MAP_PATH
MAX_FILES_TEST_MODE = 3
PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"


class RagClient:
    """
    Client to find relevant documents for audit tasks. It manages a map of
    document filenames to BSI categories, creating this map on-demand if it
    doesn't exist. This client is the replacement for the Vector Search RAG pipeline.
    Its name is kept for consistency in the project structure.
    """

    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self._document_category_map: Optional[Dict[str, List[str]]] = None
        self._all_source_files: List[str] = []
        self.prompt_config = self._load_asset_json(PROMPT_CONFIG_PATH)

    @classmethod
    async def create(cls, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient, force_remap: bool = False):
        """Asynchronous factory to create and initialize the client."""
        instance = cls(config, gcs_client, ai_client)
        await instance._initialize(force_remap=force_remap)
        return instance

    async def _initialize(self, force_remap: bool = False):
        """Initializes the client by ensuring the document map is ready."""
        logging.info("Initializing Document Finder (RagClient)...")
        self._all_source_files = [blob.name for blob in self.gcs_client.list_files()]
        await self._ensure_document_map_exists(force_remap=force_remap)

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f: return json.load(f)

    async def _create_document_map(self) -> None:
        """
        Uses an AI model to classify source documents into predefined BSI categories
        based on their filenames. Saves the result to a map file in GCS.
        The map stores the full GCS object path for each file.
        Falls back to classifying all documents as 'Sonstiges' on failure.
        """
        logging.info("Starting AI-driven document classification...")
        
        basename_to_fullpath_map = {name.split('/')[-1]: name for name in self._all_source_files}
        filenames = list(basename_to_fullpath_map.keys())

        if not filenames:
            logging.warning("No source files found to classify.")
            self.gcs_client.upload_from_string("{}", DOC_MAP_PATH)
            return

        etl_config = self.prompt_config["stages"]["ETL"]["classify_documents"]
        prompt_template = etl_config["prompt"]
        schema = self._load_asset_json(etl_config["schema_path"])
        
        filenames_json = json.dumps(filenames, indent=2)
        prompt = prompt_template.format(filenames_json=filenames_json)

        try:
            classification_result = await self.ai_client.generate_json_response(
                prompt,
                schema,
                request_context_log="Document Classification"
            )
            
            for item in classification_result.get("document_map", []):
                basename = item.get("filename")
                if basename in basename_to_fullpath_map:
                    item["filename"] = basename_to_fullpath_map[basename]
                else:
                    logging.warning(f"AI returned a filename '{basename}' not found in the source file list. It will be ignored.")

            content_to_upload = json.dumps(classification_result, indent=2, ensure_ascii=False)
            logging.info("Successfully created document map via AI with full file paths.")

        except Exception as e:
            logging.critical(
                f"AI-driven document classification failed: {e}. "
                f"Creating a fallback map with all documents as 'Sonstiges'. "
                "Document selection will be impaired.",
                exc_info=True
            )
            fallback_map = {"document_map": [{"filename": full_path, "category": "Sonstiges"} for full_path in self._all_source_files]}
            content_to_upload = json.dumps(fallback_map, indent=2, ensure_ascii=False)
        
        self.gcs_client.upload_from_string(
            content=content_to_upload,
            destination_blob_name=DOC_MAP_PATH
        )
        logging.info(f"Saved document map to '{DOC_MAP_PATH}'.")

    async def _ensure_document_map_exists(self, force_remap: bool = False) -> None:
        """
        Loads the document classification map from GCS. If it doesn't exist,
        or if `force_remap` is True, it triggers the creation process.
        """
        if force_remap or not self.gcs_client.blob_exists(DOC_MAP_PATH):
            if force_remap:
                logging.info("--force flag is set. Re-creating document classification map.")
            else:
                logging.warning(f"Document map not found at '{DOC_MAP_PATH}'. Triggering creation.")
            await self._create_document_map()
        else:
             logging.info(f"Using existing document map from '{DOC_MAP_PATH}'.")

        try:
            map_data = self.gcs_client.read_json(DOC_MAP_PATH)
        except NotFound:
            logging.critical(f"FATAL: Document map '{DOC_MAP_PATH}' could not be loaded, even after creation attempt. Cannot proceed.")
            raise
        
        category_map = {}
        doc_map_list = map_data.get("document_map", [])
        for item in doc_map_list:
            category = item.get("category")
            filename = item.get("filename")
            if category and filename:
                if category not in category_map:
                    category_map[category] = []
                category_map[category].append(filename)
        
        self._document_category_map = category_map
        logging.info(f"Successfully built document category map with {len(category_map)} categories.")

    def get_gcs_uris_for_categories(self, source_categories: List[str] = None) -> List[str]:
        """
        Finds the GCS URIs for documents belonging to the specified categories.

        Args:
            source_categories: A list of BSI categories (e.g., 'Strukturanalyse').
                               If None, all source document URIs are returned.

        Returns:
            A list of 'gs://...' URIs for the model to use as context.
        """
        if self._document_category_map is None:
            raise RuntimeError("Document map has not been initialized. Call `await RagClient.create()`.")
            
        selected_filenames = set()
        
        if source_categories:
            for category in source_categories:
                filenames = self._document_category_map.get(category, [])
                selected_filenames.update(filenames)
            if not selected_filenames:
                 logging.warning(f"No documents found for categories: {source_categories}. Returning all documents as a fallback.")
                 selected_filenames.update(self._all_source_files)
        else:
            selected_filenames.update(self._all_source_files)

        uris = [f"gs://{self.config.bucket_name}/{fname}" for fname in sorted(list(selected_filenames))]
        
#        if self.config.is_test_mode and len(uris) > MAX_FILES_TEST_MODE:
#            logging.warning(f"TEST MODE: Limiting context files from {len(uris)} to {MAX_FILES_TEST_MODE}.")
#            return uris[:MAX_FILES_TEST_MODE]
            
        return uris
==== bsi-audit-automator/src/clients/rag_client.py.rej ====
--- bsi-audit-automator/src/clients/rag_client.py
+++ bsi-audit-automator/src/clients/rag_client.py
@@ -8,8 +8,9 @@ from google.cloud.exceptions import NotFound
 from src.config import AppConfig
 from src.clients.gcs_client import GcsClient
 from src.clients.ai_client import AiClient
+from src.audit.constants import DOCUMENT_CATEGORY_MAP_PATH
 
-DOC_MAP_PATH = "output/document_map.json"
+DOC_MAP_PATH = DOCUMENT_CATEGORY_MAP_PATH
 MAX_FILES_TEST_MODE = 3
 
 

==== bsi-audit-automator/src/config.py ====
# src/config.py
import os
from dataclasses import dataclass
from typing import Optional
from dotenv import load_dotenv

@dataclass(frozen=True)
class AppConfig:
    """
    Dataclass to hold all application configuration. It's frozen to prevent
    accidental modification after initialization.
    """
    gcp_project_id: str
    source_prefix: str
    output_prefix: str
    audit_type: str
    region: str
    doc_ai_processor_name: str
    max_concurrent_ai_requests: int
    is_test_mode: bool
    bucket_name: Optional[str] = None 

def load_config_from_env() -> AppConfig:
    """
    Loads configuration from environment variables, validates them,
    and returns a frozen AppConfig dataclass.

    Raises:
        ValueError: If a required environment variable is missing.

    Returns:
        AppConfig: The validated application configuration.
    """
    # Load .env file for local development. In a cloud environment, these
    # will be set directly.
    load_dotenv()

    required_vars = [
        "GCP_PROJECT_ID", "SOURCE_PREFIX", "OUTPUT_PREFIX", "AUDIT_TYPE", "REGION", "DOC_AI_PROCESSOR_NAME", "BUCKET_NAME"
    ]
    
    config_values = {}
    for var in required_vars:
        value = os.getenv(var)
        if not value:
            raise ValueError(f"Configuration Error: Missing required environment variable: {var}")
        # Convert to lowercase to match dataclass fields
        config_values[var.lower()] = value

    # Handle special case and boolean variables
    config_values["is_test_mode"] = os.getenv("TEST", "false").lower() == "true"
    
    # Load the new concurrency limit, defaulting to 5 if not set or invalid
    max_reqs_str = os.getenv("MAX_CONCURRENT_AI_REQUESTS", "5")
    config_values["max_concurrent_ai_requests"] = int(max_reqs_str) if max_reqs_str.isdigit() else 5

    return AppConfig(**config_values)

# Create a singleton instance to be imported by other modules.
# The try/except block ensures the application exits gracefully if config is invalid.
try:
    config = load_config_from_env()
except ValueError as e:
    print(e)
    exit(1)
==== bsi-audit-automator/src/constants.py ====
"""
Centralized constants for file paths and output organization.
This ensures consistency across all stages and reduces magic strings.
"""

CHUNK_PROCESSING_MODEL =  "gemini-2.5-flash-lite-preview-06-17"
GROUND_TRUTH_MODEL =  "gemini-2.5-pro"

# Output organization structure:
# output/results/         -> Final stage outputs ready for report generation
# output/temp/           -> Temporary files (PDF chunks, intermediate processing)
# output/intermediary/   -> Idempotent saves with stage-specific subfolders

# =============================================================================
# RESULTS PATHS - Final stage outputs
# =============================================================================
RESULTS_BASE = "output/results"
STAGE_RESULTS_PATH = f"{RESULTS_BASE}/{{stage_name}}.json"  # Format with stage_name
ALL_FINDINGS_PATH = f"{RESULTS_BASE}/all_findings.json"
FINAL_REPORT_PATH = f"{RESULTS_BASE}/final_audit_report.json"

# =============================================================================
# TEMPORARY PATHS - Short-lived processing files
# =============================================================================
TEMP_BASE = "output/temp"
TEMP_PDF_CHUNKS_PREFIX = f"{TEMP_BASE}/pdf_chunks/"
DOC_AI_BATCH_RESULTS_PREFIX = f"{TEMP_BASE}/doc_ai_results/"

# =============================================================================
# INTERMEDIARY PATHS - Idempotent saves organized by stage
# =============================================================================
INTERMEDIARY_BASE = "output/intermediate"

# Grundschutz-Check-Extraction stage paths
GS_EXTRACTION_BASE = f"{INTERMEDIARY_BASE}/gs_extraction"
GROUND_TRUTH_MAP_PATH = f"{GS_EXTRACTION_BASE}/system_structure_map.json"
GROUPED_BLOCKS_PATH = f"{GS_EXTRACTION_BASE}/zielobjekt_grouped_blocks.json"
EXTRACTED_CHECK_DATA_PATH = f"{GS_EXTRACTION_BASE}/extracted_grundschutz_check_merged.json"
INDIVIDUAL_RESULTS_PREFIX = f"{GS_EXTRACTION_BASE}/individual_results/"
FINAL_MERGED_LAYOUT_PATH = f"{GS_EXTRACTION_BASE}/merged_layout_parser_result.json"

# Document AI processing paths
DOC_AI_BASE = f"{INTERMEDIARY_BASE}/doc_ai"
DOC_AI_CHUNK_RESULTS_PREFIX = f"{DOC_AI_BASE}/chunk_results/"

# RAG Client paths  
RAG_BASE = f"{INTERMEDIARY_BASE}/rag"
DOCUMENT_CATEGORY_MAP_PATH = f"{RAG_BASE}/document_category_map.json"

==== bsi-audit-automator/src/logging_setup.py ====
# src/logging_setup.py
import logging
import sys
from src.config import AppConfig

def setup_logging(config: AppConfig):
    """
    Sets up the root logger based on the execution mode from the config.

    Args:
        config: The application configuration object.
    """
    # In test mode, we want detailed logs at INFO level.
    # In production, we want high-level INFO, with details at DEBUG.
    log_level = logging.INFO if config.is_test_mode else logging.DEBUG
    
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        stream=sys.stdout,
    )

    # In production, set the root logger to INFO to see high-level status,
    # while our application-specific logs can be at the DEBUG level.
    if not config.is_test_mode:
        logging.getLogger().setLevel(logging.INFO)

        # Suppress noisy third-party library logs for cleaner production output
        logging.getLogger("google.auth").setLevel(logging.WARNING)
        logging.getLogger("google.api_core").setLevel(logging.WARNING)
        logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)
        logging.info("Production logging enabled. Set root to INFO, app logs to DEBUG, and suppressed noisy libs.")
    else:
        logging.info("Test mode logging enabled. All INFO logs will be visible.")
==== bsi-audit-automator/src/main.py ====
# src/main.py
import argparse
import logging
import asyncio

from .config import config
from .logging_setup import setup_logging
from .clients.gcs_client import GcsClient
from .clients.rag_client import RagClient
from .clients.ai_client import AiClient
from .audit.controller import AuditController
from .audit.report_generator import ReportGenerator

async def main_async():
    """
    Asynchronous main function to handle all pipeline operations.
    """
    setup_logging(config)

    parser = argparse.ArgumentParser(
        description="BSI Grundschutz Audit Automation Pipeline."
    )
    group = parser.add_mutually_exclusive_group(required=True)
    
    group.add_argument(
        '--scan-previous-report',
        action='store_true',
        help='Run the stage to scan a previous audit report.'
    )
    group.add_argument(
        '--run-gs-check-extraction',
        action='store_true',
        help='Run only the Grundschutz-Check data extraction and mapping stage.'
    )
    group.add_argument(
        '--run-stage',
        type=str,
        help='Run a single audit stage (e.g., --run-stage Chapter-1).'
    )
    group.add_argument(
        '--run-all-stages',
        action='store_true',
        help='Run all audit generation stages sequentially.'
    )
    group.add_argument(
        '--generate-report',
        action='store_true',
        help='Assemble the final report from completed stage stubs.'
    )

    parser.add_argument(
        '--force',
        action='store_true',
        help='Force re-running of completed stages and re-classification of source documents.'
    )

    args = parser.parse_args()

    gcs_client = GcsClient(config)
    ai_client = AiClient(config)

    if args.generate_report:
        logging.info("Starting final report assembly...")
        generator = ReportGenerator(config, gcs_client)
        await generator.assemble_report()
        return
        
    # For all other tasks, we need the RagClient (Document Finder)
    logging.info("Initializing Document Finder Client...")
    try:
        rag_client = await RagClient.create(config, gcs_client, ai_client, force_remap=args.force)
    except Exception as e:
        logging.critical(f"Failed to initialize the Document Finder client. This can happen if no source documents are present. Error: {e}", exc_info=True)
        exit(1)

    controller = AuditController(config, gcs_client, ai_client, rag_client)

    if args.scan_previous_report:
        await controller.run_single_stage("Scan-Report", force_overwrite=args.force)
    elif args.run_gs_check_extraction:
        await controller.run_single_stage("Grundschutz-Check-Extraction", force_overwrite=args.force)
    elif args.run_stage:
        await controller.run_single_stage(args.run_stage, force_overwrite=args.force)
    elif args.run_all_stages:
        await controller.run_all_stages(force_overwrite=args.force)


def main():
    """
    Main entry point for the BSI Audit Automator.
    Parses command-line arguments and runs the appropriate async task.
    """
    try:
        asyncio.run(main_async())
        logging.info("Pipeline step completed successfully.")
    except Exception as e:
        logging.critical(f"A critical error occurred in the pipeline: {e}", exc_info=True)
        exit(1)

if __name__ == "__main__":
    main()

