==== bsi-audit-automator/src/clients/rag_client.py.orig ====
# src/clients/rag_client.py
import logging
import json
from typing import List, Dict, Any

from google.cloud import aiplatform
from google.cloud.aiplatform.matching_engine import MatchingEngineIndexEndpoint
from google.cloud.exceptions import NotFound

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient

DOC_MAP_PATH = "output/document_map.json"
# **NEW**: Constants for dynamic context filtering
SIMILARITY_THRESHOLD = 0.7  # Lower is stricter. Cosine distance of 0.7 is a reasonable starting point.
NEIGHBOR_POOL_SIZE = 10     # Fetch a larger pool of candidates to filter from.


class RagClient:
    """Client for Retrieval-Augmented Generation using Vertex AI Vector Search."""

    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client

        aiplatform.init(
            project=config.gcp_project_id,
            location=config.vertex_ai_region
        )

        if config.index_endpoint_public_domain:
            logging.info(f"Connecting to PUBLIC Vector Search Endpoint: {config.index_endpoint_public_domain}")
            self.index_endpoint = MatchingEngineIndexEndpoint.from_public_endpoint(
                project_id=config.gcp_project_id,
                region=config.vertex_ai_region,
                public_endpoint_domain_name=config.index_endpoint_public_domain
            )
        else:
            logging.info(f"Connecting to PRIVATE Vector Search Endpoint: {config.index_endpoint_id}")
            self.index_endpoint = MatchingEngineIndexEndpoint(
                index_endpoint_name=self.config.index_endpoint_id,
            )

        self._chunk_lookup_map = self._load_chunk_lookup_map()
        self._document_category_map = self._load_document_category_map()

    def _load_document_category_map(self) -> Dict[str, str]:
        """
        Loads the document classification map from GCS. The map provides a
        lookup from document category to a list of filenames.
        """
        logging.info(f"Loading document category map from '{DOC_MAP_PATH}'...")
        category_map = {}
        try:
            map_data = self.gcs_client.read_json(DOC_MAP_PATH)
            doc_map_list = map_data.get("document_map", [])
            for item in doc_map_list:
                category = item.get("category")
                filename = item.get("filename")
                if category and filename:
                    if category not in category_map:
                        category_map[category] = []
                    category_map[category].append(filename)
            
            logging.info(f"Successfully built document category map with {len(category_map)} categories.")
            return category_map
        except NotFound:
            logging.error(f"CRITICAL: Document map file not found at '{DOC_MAP_PATH}'. The ETL process must be run first.")
            raise
        except Exception as e:
            logging.error(f"Failed to build document category map: {e}", exc_info=True)
            return {}

    def _load_chunk_lookup_map(self) -> Dict[str, Dict[str, str]]:
        """
        Downloads all embedding batch files from GCS and creates a mapping from
        chunk ID to its text content and source document for fast lookups.
        """
        lookup_map: Dict[str, Dict[str, str]] = {}
        logging.info("Building chunk ID to text lookup map from all batch files...")

        try:
            embedding_blobs = self.gcs_client.list_files(prefix="vector_index_data/")

            for blob in embedding_blobs:
                if "placeholder.json" in blob.name:
                    continue

                jsonl_content = self.gcs_client.read_text_file(blob.name)
                
                for line in jsonl_content.strip().split('\n'):
                    try:
                        data = json.loads(line)
                        chunk_id = data.get("id")
                        chunk_text = data.get("text_content")
                        source_doc = data.get("source_document")
                        if chunk_id and chunk_text and source_doc:
                            lookup_map[chunk_id] = {
                                "text_content": chunk_text,
                                "source_document": source_doc
                            }
                    except json.JSONDecodeError:
                        logging.warning(f"Skipping invalid JSON line in {blob.name}: '{line}'")
                        continue
            
            logging.info(f"Successfully built lookup map with {len(lookup_map)} entries.")
            return lookup_map
        except Exception as e:
            logging.error(f"Failed to build chunk lookup map from batch files: {e}", exc_info=True)
            return {}

    def get_context_for_query(self, query: str, source_categories: List[str] = None) -> str:
        """
        Finds relevant document chunks for a query, dynamically filtering them by
        similarity score for the highest quality context.
        """
        if self.config.is_test_mode:
            logging.info(f"RAG_CLIENT_TEST_MODE: Sending query to vector DB: '{query}'")

        context_str = ""
        try:
            success, embeddings = self.ai_client.get_embeddings([query])
            if not success or not embeddings:
                logging.error("Failed to generate embedding for the RAG query.")
                return "Error: Could not generate embedding for query."
            
            query_vector = embeddings[0]

            filters = None
            if source_categories and self._document_category_map:
                allow_list_filenames = []
                for category in source_categories:
                    filenames = self._document_category_map.get(category, [])
                    allow_list_filenames.extend(filenames)
                
                if allow_list_filenames:
                    logging.info(f"Applying search filter for categories: {source_categories} ({len(allow_list_filenames)} files)")
                    filters = aiplatform.IndexDatapoint.Restriction(
                        namespace="source_document", allow_list=allow_list_filenames
                    )
                else:
                    logging.warning(f"No documents found for categories: {source_categories}. Searching all documents.")

            response = self.index_endpoint.find_neighbors(
                deployed_index_id="bsi_deployed_index_kunde_x",
                queries=[query_vector],
                num_neighbors=NEIGHBOR_POOL_SIZE,
                filter=[filters] if filters else []
            )

            if response and response[0]:
                all_neighbors = response[0]
                
                # **NEW**: Filter the retrieved neighbors by their distance score.
                # A lower distance means higher similarity.
                quality_neighbors = [n for n in all_neighbors if n.distance <= SIMILARITY_THRESHOLD]
                
                logging.info(f"Filtering {len(all_neighbors)} neighbors down to {len(quality_neighbors)} by similarity score (<= {SIMILARITY_THRESHOLD}).")

                if not quality_neighbors:
                    logging.warning("No neighbors met the similarity threshold.")
                    return "No highly relevant context found in the documents."

                for neighbor in quality_neighbors:
                    chunk_id = neighbor.id
                    context_info = self._chunk_lookup_map.get(chunk_id)
                    if context_info:
                        context_str += f"-- CONTEXT FROM DOCUMENT: {context_info['source_document']} (Similarity: {1-neighbor.distance:.2%}) --\n"
                        context_str += f"{context_info['text_content']}\n\n"
                    else:
                        logging.warning(f"Could not find text for chunk ID: {chunk_id}")
                
                return context_str
            else:
                logging.warning("Vector DB query returned no neighbors.")
                return "No relevant context found in the documents."
                
        except Exception as e:
            logging.error(f"Error querying Vector DB: {e}", exc_info=True)
            return "Error retrieving context from Vector DB."
==== project_context.txt ====

==== scripts/loc_counter.py ====
#!/usr/bin/env python3
import subprocess
import os
from collections import defaultdict


def analyze_loc():
    """
    Analyzes the lines of code (LoC) for all files in the git repository,
    categorized by file extension.
    """
    print("Analyzing Lines of Code (LoC) for tracked git files...")

    try:
        # Get a list of all files tracked by git
        result = subprocess.run(
            ['git', 'ls-files'],
            capture_output=True,
            text=True,
            check=True,
            encoding='utf-8'
        )
        all_files = result.stdout.strip().split('\n')
    except (subprocess.CalledProcessError, FileNotFoundError) as e:
        print(f"Error: Could not execute 'git ls-files'. Is git installed and are you in a git repository? Details: {e}")
        return

    # This specific large JSON file is excluded from the count as requested.
    # We use endswith to make the script runnable from any subdirectory of the repo.
    excluded_file_suffix = 'assets/json/BSI_GS_OSCAL_current_2023_benutzerdefinierte.json'

    loc_by_extension = defaultdict(int)
    total_loc = 0

    for file_path in all_files:
        if file_path.endswith(excluded_file_suffix):
            print(f"--> Excluding file: {file_path}")
            continue

        try:
            # Use 'ignore' to handle potential binary files gracefully
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                line_count = sum(1 for _ in f)

            _, extension = os.path.splitext(file_path)
            category = extension if extension else '.no_extension'

            loc_by_extension[category] += line_count
            total_loc += line_count
        except FileNotFoundError:
            # This can happen if a file was deleted but the deletion isn't committed yet.
            print(f"--> Warning: File not found, skipping: {file_path}")

    # --- Print the report ---
    print("\n" + "="*50)
    print("      Lines of Code (LoC) by File Extension")
    print("="*50)
    sorted_extensions = sorted(loc_by_extension.items(), key=lambda item: item[1], reverse=True)
    for extension, count in sorted_extensions:
        print(f"{extension:<15} | {count:>10,} lines")
    print("-"*50)
    print(f"{'Total':<15} | {total_loc:>10,} lines")
    print("="*50)

if __name__ == "__main__":
    analyze_loc()
==== .gitignore ====
# Terraform
#
# Ignore the .terraform directory, which contains downloaded provider plugins
# and modules. This is generated by `terraform init` and should not be versioned.
.terraform/

# Ignore local Terraform state files, which can contain sensitive information.
# Use a remote backend for state management in a team environment.
*.tfstate
*.tfstate.*.backup

# Ignore generated plans, which can also contain sensitive information.
*.tfplan

# Ignore local variable definitions files as they may contain sensitive data.
# It's a best practice to use a .tfvars.example file to document required variables.
*.tfvars
*.tfvars.json
*.auto.tfvars
*.auto.tfvars.json

# Ignore temporary and local override files.
override.tf
override.tf.json
*_override.tf
*_override.tf.json

# Ignore crash logs and local CLI configuration files.
crash.log
crash.*.log
.terraformrc
terraform.rc

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[codz]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py.cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock
#poetry.toml

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
#   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
#pdm.lock
#pdm.toml
.pdm-python
.pdm-build/

# pixi
#   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
#pixi.lock
#   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
#   in the .venv directory. It is recommended not to include this directory in version control.
.pixi

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.envrc
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer, 
#  you could uncomment the following to ignore the entire vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

# Marimo
marimo/_static/
marimo/_lsp/
__marimo__/

==== AI_Data_Processing_Strategy.md ====
### **AI Data Processing Strategy (Revised)**

**Guiding Principle:** Accuracy and Auditability through Focused, Evidence-Based Generation.

This strategy uses a three-phase process centered around a Vertex AI Vector Search index and Retrieval-Augmented Generation (RAG). The process is driven by the structure of our `master_report_template.json`, ensuring that every AI task is small, focused, and directly tied to a specific section of the final report.

---

### **Phase 0: Idempotent Document Ingestion and Indexing**

**Objective:** To process customer source documents and populate a semantic search index, ensuring resilience and preventing rework. This phase is executed by the `EtlProcessor`.

1.  **List & Filter Documents:** The processor lists all source documents from the configured GCS prefix. For each document, it first checks for a corresponding `.success` or `.failed` status marker in the `etl_status/` GCS prefix. If either marker exists, the document is skipped.
2.  **Per-Document Processing Loop:** For each new document:
    a.  **Extract & Chunk:** Text is extracted (using PyMuPDF for PDFs) and broken into semantically meaningful chunks using a `RecursiveCharacterTextSplitter`. Each chunk is prefixed with metadata (e.g., source page number) for traceability.
    b.  **Generate Embeddings:** Chunks are sent to the `gemini-embedding-001` model to be converted into 3072-dimension vectors. This process is batched and includes robust retry logic.
    c.  **Format & Upload Individual JSONL:** The chunks, their embeddings, and metadata (`id`, `source_document`, `text_content`) are formatted into a single JSONL file (one JSON object per line), specific to the source document. This file is uploaded to the `vector_index_data/` GCS directory.
    d.  **Mark as Completed:** Upon successful upload, an empty `.success` file is created in the status directory. If any step fails permanently after retries, a `.failed` file containing the error is created instead.
3.  **Automatic Index Ingestion:** The Vertex AI Vector Search Index is configured via Terraform to monitor the `vector_index_data/` directory and automatically ingests new JSONL files as they arrive.

---

### **Phase 1: Staged, Finding-Oriented Generation**

**Objective:** To systematically populate the audit report by iterating through each required subchapter, using RAG to generate content and a structured finding. This is orchestrated by the `AuditController`.

1.  **Iterate Report Stages:** The `AuditController` orchestrates the process, running a "stage" for each chapter (e.g., Chapter 1, 3, 4, 5, 7). It uses lazy-loading to instantiate the required `Runner` class for each stage only when needed.
2.  **Formulate Targeted Query:** For each automated subchapter (e.g., 1.2, 3.1, 3.2), the stage runner defines a specific, semantically rich search query tailored to the questions of that section.
3.  **Retrieve Relevant Context:** The query is sent to the `RagClient`. The client embeds the query and searches the Vertex AI Vector Index for the most similar document chunks, returning their full text content as evidence.
4.  **Construct AI Prompt:** A highly specific prompt is assembled, containing:
    *   **The Role:** "You are a BSI security auditor..."
    *   **The Task:** The specific questions to answer for the subchapter.
    *   **The Context:** The full text of the relevant document chunks retrieved in the previous step.
    *   **The Schema Stub:** A JSON schema defining the required output, which **mandates a structured `finding` object** with a `category` ('AG', 'AS', 'E', 'OK') and a `description`.
5.  **Generate and Validate:** The prompt is sent to the `gemini-2.5-pro` model via the `AiClient`. The returned JSON is validated against the schema.
6.  **Centralized Finding Collection:** The `AuditController` inspects the result from every stage. If a generated `finding` object's category is not 'OK', it is appended to a master list of findings held in memory by the controller.
7.  **Save Intermediate Result:** The validated JSON for the entire chapter is saved to GCS (e.g., `output/results/chapter_3.json`). This ensures the entire process is resumable.

---

### **Phase 2: Final Report Assembly**

**Objective:** To deterministically merge all validated JSON stubs and the collected findings into the final audit report. This is handled by the `ReportGenerator`.

1.  **Save All Findings:** After all stages are run, the `AuditController` saves the master list of findings to `output/results/all_findings.json`.
2.  **Load Master Template:** The `ReportGenerator` script loads the `master_report_template.json`.
3.  **Populate Stage Content:** It reads all the individual stage result files (`chapter_1.json`, etc.) from GCS and populates the corresponding sections of the report. This includes filling in text blocks, answers to questions, and deterministically generated tables (like the control checklist in Chapter 5).
4.  **Populate Findings Tables:** The `ReportGenerator` then reads `output/results/all_findings.json` and iterates through the list, populating the three tables in Chapter 7.2 (`geringfuegigeAbweichungen`, `schwerwiegendeAbweichungen`, `empfehlungen`) based on the `category` of each finding.
5.  **Render Final Output:** The final, fully populated JSON object is saved as `final_audit_report.json`, ready for review in the `report_editor.html` tool.
==== RAG_Howto.md ====
# Implementing a High-Precision RAG with Vertex AI: A Step-by-Step Guide

### **1. Introduction: From Broad Searches to Precision Retrieval**

The core value of our Vector Database (VDB) is its ability to function as a semantic search engine for the customer's documentation. The initial RAG approach was to search this entire database for every query. While functional, this can lead to imprecise results, as the AI receives a mix of highly relevant and only vaguely related evidence, forcing it to guess.

To achieve superior accuracy and auditability, we evolve this strategy. Instead of searching *everything*, we will implement **scoped searching**. For each audit question, we will first determine the *type* of document that should contain the answer (e.g., "Netzplan", "Sicherheitsleitlinie") and instruct our RAG client to **only search within those specific files**.

This is accomplished by adding a new "Document Classification" step to our ETL pipeline, which uses the AI to categorize every source document. This pre-filtering dramatically reduces noise, provides higher-quality context to the AI, and leads to more precise, evidence-based findings.

### **2. The High-Precision RAG Workflow**

The process for a single audit task now follows these more intelligent steps:

1.  **(ETL Phase) Classify Documents:** At the start of the ETL run, an AI-powered process analyzes all source filenames and creates a `document_map.json`, mapping each file to a BSI category (e.g., `{"filename": "HiSolutions_Netzplan_v4.5.pdf", "category": "Netzplan"}`).
2.  **Define a Scoped Query:** The stage runner defines a clear question *and* the document categories it expects the answer to be in (e.g., Question: "Is the network plan current?", Categories: `['Netzplan']`).
3.  **Filter and Search the VDB:** The query is sent to our `RagClient`. The client uses the `document_map.json` to get the specific filenames for the requested categories and tells the Vector Search to restrict its search to only those documents.
4.  **Retrieve Relevant Chunks:** The Vector Search returns the most semantically similar text chunks *from within the filtered document set*.
5.  **Construct a Contextual Prompt:** A rich prompt is built, containing the original question and the high-quality, targeted evidence retrieved.
6.  **Generate a Grounded Response:** This context-rich prompt is sent to the Gemini model, which synthesizes an answer based on the focused evidence.

### **3. Detailed Implementation Steps**

Here is the technical breakdown of how to implement this high-precision workflow.

#### **Prerequisite: AI-Powered Document Classification in ETL**

The foundation of this strategy is the `document_map.json` file. This is generated during the ETL phase by the `EtlProcessor`, which uses a dedicated prompt and schema to classify documents by their filenames. The `RagClient` depends on this file existing.

#### **Step 1: The `RagClient` Loads the Category Map**

The `RagClient`'s first task is to load both the chunk-to-text lookup map and the new document category map into memory.

```python
# src/clients/rag_client.py

import logging
import json
from google.cloud import aiplatform
from google.cloud.exceptions import NotFound
# ... other imports

DOC_MAP_PATH = "output/document_map.json"

class RagClient:
    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient):
        # ... (standard initializations) ...
        self._chunk_lookup_map = self._load_chunk_lookup_map()
        # NEW: Load the document category map.
        self._document_category_map = self._load_document_category_map()

    def _load_document_category_map(self) -> Dict[str, str]:
        """
        Loads the document classification map from GCS, creating a lookup from
        a category (e.g., "Netzplan") to a list of its associated filenames.
        """
        logging.info(f"Loading document category map from '{DOC_MAP_PATH}'...")
        category_map = {}
        try:
            map_data = self.gcs_client.read_json(DOC_MAP_PATH)
            doc_map_list = map_data.get("document_map", [])
            for item in doc_map_list:
                category = item.get("category")
                filename = item.get("filename")
                if category and filename:
                    if category not in category_map:
                        category_map[category] = []
                    category_map[category].append(filename)
            
            logging.info(f"Successfully built document category map with {len(category_map)} categories.")
            return category_map
        except NotFound:
            logging.error(f"CRITICAL: Document map file not found. ETL must be run first.")
            raise
        # ... error handling ...
```

#### **Step 2: Execute a Filtered Query**

The core search method, `get_context_for_query`, is updated to accept a list of `source_categories`. It uses this to build a filter for the Vector Search API call.

```python
# In src/clients/rag_client.py, within the RagClient class

    def get_context_for_query(self, query: str, num_neighbors: int = 5, source_categories: List[str] = None) -> str:
        """
        Finds relevant document chunks, optionally filtering the search
        to specific document categories for higher precision.
        """
        try:
            # 1. Embed the text query into a numerical vector first.
            success, embeddings = self.ai_client.get_embeddings([query])
            if not success or not embeddings:
                logging.error("Failed to generate embedding for the RAG query.")
                return "Error: Could not generate embedding for query."
            
            query_vector = embeddings[0]

            # 2. NEW: Build the filter based on the requested categories.
            filters = None
            if source_categories and self._document_category_map:
                # Collect all filenames belonging to the requested categories.
                allow_list_filenames = []
                for category in source_categories:
                    filenames = self._document_category_map.get(category, [])
                    allow_list_filenames.extend(filenames)
                
                if allow_list_filenames:
                    logging.info(f"Applying search filter for categories: {source_categories}")
                    # Create a restriction filter for the vector search.
                    filters = aiplatform.IndexDatapoint.Restriction(
                        namespace="source_document", # Must match the key in the index data
                        allow_list=allow_list_filenames
                    )
            
            # 3. Use the vector and filter to find neighbors.
            response = self.index_endpoint.find_neighbors(
                deployed_index_id="bsi_deployed_index_kunde_x",
                queries=[query_vector],
                num_neighbors=num_neighbors,
                # The find_neighbors call expects a list of restriction objects.
                filter=[filters] if filters else []
            )

            # ... (rest of the logic to retrieve text content remains the same) ...

        except Exception as e:
            logging.error(f"Error querying Vector DB: {e}", exc_info=True)
            return "Error retrieving context from Vector DB."

```

#### **Step 3: Putting It All Together in a Stage Runner**

The stage runners are updated to use this new capability. Their subchapter definitions now include natural language questions for `rag_query` and a list of `source_categories` to search within.

```python
# Example from src/audit/stages/stage_3_dokumentenpruefung.py

class Chapter3Runner:
    def _load_subchapter_definitions(self) -> Dict[str, Any]:
        """Loads definitions including specific RAG queries and source categories."""
        return {
            "bereinigterNetzplan": {
                "key": "3.3.2",
                "prompt_path": "assets/prompts/stage_3_3_2_netzplan.txt",
                "schema_path": "assets/schemas/stage_3_3_2_netzplan_schema.json",
                # The query is now a clear, natural language question.
                "rag_query": "Liegt ein aktueller und vollständiger Netzplan vor und sind alle Komponenten korrekt bezeichnet?",
                # The search is now scoped to only documents classified as "Netzplan".
                "source_categories": ["Netzplan"]
            },
            # ... other subchapter definitions
        }

    async def _process_rag_subchapter(self, name: str, definition: dict) -> Dict[str, Any]:
        """Generates content for a single subchapter using the scoped RAG pipeline."""
        
        # ... (load prompt and schema) ...

        # The call to the RagClient now includes the categories for filtering.
        context_evidence = self.rag_client.get_context_for_query(
            query=definition["rag_query"],
            source_categories=definition.get("source_categories")
        )
        
        prompt = prompt_template.format(context=context_evidence)

        # ... (call AI and return result) ...

==== bsi-audit-automator/.dockerignore ====
# Git / version control
.git
.gitignore

# Docker
Dockerfile
.dockerignore

# Python cache and virtual environment
__pycache__/
*.pyc
.venv
venv/

# Environment variables - should be passed at runtime, not baked into the image
.env

# IDE / Editor specific
.vscode/
.idea/
==== bsi-audit-automator/Dockerfile ====
# Stage 1: Use an official Python runtime as a parent image
# Using a "slim" image to keep the final image size down.
FROM python:3.11-slim-bookworm

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1

# Set the working directory in the container
WORKDIR /app

# Add the non-root user's local bin to the PATH.
# This prevents warnings during pip install.
ENV PATH="/app/.local/bin:${PATH}"

# It's good practice to upgrade pip to the latest version
RUN pip install --upgrade pip

# Create a non-root user and group to run the application
# This avoids the "Running pip as root" warning and is a security best practice.
RUN groupadd -r appgroup && useradd -r -g appgroup -d /app -s /sbin/nologin -c "Docker image user" appuser
RUN chown -R appuser:appgroup /app

# Copy the requirements file into the container
# This is done before copying the rest of the code to leverage Docker's layer caching.
COPY requirements.txt .

# Switch to the non-root user before installing dependencies
USER appuser

# Install any needed packages specified in requirements.txt
# --no-cache-dir reduces image size by not storing the download cache.
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application's source code into the container
COPY . .

# Specify the command to run on container start.
ENTRYPOINT ["python", "-m", "src.main"]

# Set a default command to show the help message if no other command is provided.
CMD ["--help"]
==== bsi-audit-automator/assets/json/BSI_GS_OSCAL_current_2023_benutzerdefinierte.json ====
{
  "catalog": {
    "uuid": "d186e90d-ecbd-4d07-8311-4603deb60225",
    "metadata": {
      "title": "Gesamtkatalog BSI Grundschutz Kompendium 2023",
      "last-modified": "2025-06-27T17:42:20.097143+00:00",
      "version": "1.0.0",
      "oscal-version": "1.1.2"
    },
    "groups": [
      {
        "id": "ISMS",
        "title": "ISMS: Sicherheitsmanagement",
        "class": "main-group",
        "groups": [
          {
            "id": "ISMS.1",
            "title": "Sicherheitsmanagement",
            "class": "baustein",
            "controls": [
              {
                "id": "ISMS.1.A1",
                "title": "Übernahme der Gesamtverantwortung für Informationssicherheit durch die Leitung (B)",
                "class": "Management",
                "props": [
                  {
                    "name": "level",
                    "value": "1",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "phase",
                    "value": "Initiation",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "practice",
                    "value": "GOV",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_c",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_i",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_a",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  }
                ],
                "parts": [
                  {
                    "id": "isms.1.a1-m1",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 1: Partial",
                    "class": "maturity-level-partial",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Institutionsleitung hat die Verantwortung für Informationssicherheit nur mündlich oder informell anerkannt. Die Übernahme der Verantwortung ist nicht dokumentiert und wird nicht aktiv kommuniziert, was zu Unklarheiten bei den Mitarbeitenden führt."
                      },
                      {
                        "name": "guidance",
                        "prose": "In dieser Stufe wird die Verantwortung oft nur reaktiv im Falle eines Sicherheitsvorfalls oder einer externen Prüfung erwähnt. Es gibt keine formalen Dokumente wie eine Ernennungsurkunde für den ISB oder eine offizielle Leitlinie."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer befragt Führungskräfte und stellt fest, ob sie ihre Verantwortung für die Informationssicherheit verstehen und wahrnehmen. Es wird geprüft, ob eine formale Zuweisung der Gesamtverantwortung fehlt."
                      }
                    ]
                  },
                  {
                    "id": "isms.1.a1-m2",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 2: Foundational",
                    "class": "maturity-level-foundational",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Gesamtverantwortung für die Informationssicherheit ist formal der Institutionsleitung zugewiesen, z. B. in einer Geschäftsordnung. Die Leitung nimmt diese Verantwortung jedoch nur passiv wahr und delegiert die Aufgaben vollständig ohne aktive Steuerung oder Kontrolle."
                      },
                      {
                        "name": "guidance",
                        "prose": "Die Zuweisung der Verantwortung ist in einem offiziellen Dokument festgehalten. Die Leitung genehmigt Budgets, greift aber nicht proaktiv in die Gestaltung des Sicherheitsprozesses ein und informiert sich nur unregelmäßig über den Status."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer sichtet Dokumente wie die Geschäftsordnung, um die formale Zuweisung der Verantwortung zu verifizieren. Es wird durch Interviews geprüft, inwieweit die Leitung den Sicherheitsprozess aktiv steuert und kontrolliert."
                      }
                    ]
                  },
                  {
                    "id": "isms.1.a1-m3",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 3: Defined",
                    "class": "maturity-level-defined",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Institutionsleitung MUSS die Gesamtverantwortung für Informationssicherheit in der Institution übernehmen. Dies MUSS für alle Beteiligten deutlich erkennbar sein. Die Institutionsleitung MUSS den Sicherheitsprozess initiieren, steuern und kontrollieren. Die Institutionsleitung MUSS die Zuständigkeiten für Informationssicherheit festlegen und die zuständigen Mitarbeitenden mit den erforderlichen Kompetenzen und Ressourcen ausstatten. Die Institutionsleitung MUSS sich regelmäßig über den Status der Informationssicherheit sowie über mögliche Risiken und Konsequenzen aufgrund fehlender Sicherheitsmaßnahmen informieren lassen."
                      },
                      {
                        "name": "guidance",
                        "prose": "Die Übernahme der Verantwortung wird durch eine von der Leitung unterzeichnete Sicherheitsleitlinie dokumentiert. Regelmäßige Meetings (z. B. quartalsweise) zwischen der Leitung und dem ISB werden etabliert, um den Status und die Risiken zu besprechen. Die Ergebnisse dieser Meetings werden protokolliert."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer prüft die unterzeichnete Sicherheitsleitlinie. Es werden Protokolle der Management-Meetings zur Informationssicherheit eingesehen. Interviews mit Mitarbeitenden bestätigen, dass die Rolle der Leitung im Sicherheitsprozess klar und bekannt ist."
                      }
                    ]
                  },
                  {
                    "id": "isms.1.a1-m4",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 4: Enhanced",
                    "class": "maturity-level-enhanced",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Institutionsleitung lebt die Informationssicherheit aktiv vor ('Tone at the Top'). Sie hinterfragt die erhaltenen Statusberichte kritisch, fordert tiefere Analysen bei Abweichungen und verfolgt die Umsetzung von Maßnahmen zur Risikobehandlung konsequent nach. Die Einhaltung der Sicherheitspolicies wird auch auf Leitungsebene demonstriert."
                      },
                      {
                        "name": "guidance",
                        "prose": "Die Leitung nimmt aktiv an wichtigen Sensibilisierungsmaßnahmen teil. Entscheidungen zur Risikobehandlung werden nicht nur dokumentiert, sondern auch mit einer klaren Begründung für die gewählte Option (z. B. Akzeptanz, Minderung) versehen. Ein dediziertes Risikokomitee unter Vorsitz eines Mitglieds der Institutionsleitung wird eingerichtet."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer analysiert die Protokolle des Risikokomitees und die dokumentierten Risikoentscheidungen auf ihre Nachvollziehbarkeit. Es wird geprüft, ob die Leitung bei der Zuweisung von Ressourcen für Sicherheitsmaßnahmen eine klare Priorisierung basierend auf dem Geschäftsrisiko vornimmt."
                      }
                    ]
                  },
                  {
                    "id": "isms.1.a1-m5",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 5: Comprehensive",
                    "class": "maturity-level-comprehensive",
                    "parts": [
                      {
                        "name": "statement",
                        "prose": "Die Informationssicherheitsziele sind vollständig in die übergeordneten Geschäftsziele und die strategische Planung der Institution integriert. Die Leistung im Bereich der Informationssicherheit ist Teil der Zielvereinbarungen und Leistungsbeurteilungen für das obere Management. Die Leitung fördert aktiv eine positive Sicherheitskultur und investiert proaktiv in zukünftige Sicherheitstechnologien."
                      },
                      {
                        "name": "guidance",
                        "prose": "In den jährlichen Geschäftsberichten wird über die Informationssicherheitslage berichtet. Die Zielerreichung wird anhand von Key Performance Indicators (KPIs) gemessen, die in einem Management-Dashboard visualisiert werden. Die Leitung initiiert und finanziert Forschungsprojekte zur Abwehr zukünftiger Bedrohungen."
                      },
                      {
                        "name": "assessment-method",
                        "prose": "Der Prüfer überprüft die strategischen Planungsdokumente und die Zielvereinbarungen des Managements auf die Integration von Sicherheitszielen. Es wird die Existenz und Nutzung eines KPI-Dashboards verifiziert. Die Wirksamkeit der Sicherheitskultur wird durch Mitarbeiterbefragungen und die Analyse von Sicherheitsvorfällen bewertet."
                      }
                    ]
                  }
                ]
              },
              {
                "id": "ISMS.1.A2",
                "title": "Festlegung der Sicherheitsziele und -strategie (B)",
                "class": "Management",
                "props": [
                  {
                    "name": "level",
                    "value": "1",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "phase",
                    "value": "Initiation",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "practice",
                    "value": "GOV",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_c",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_i",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  },
                  {
                    "name": "effective_on_a",
                    "value": "true",
                    "ns": "https://www.bsi.bund.de/ns/grundschutz"
                  }
                ],
                "parts": [
                  {
                    "id": "isms.1.a2-m1",
                    "name": "maturity-level-description",
                    "title": "Maturity Level 1: Partial",

[... File truncated. Only first 200 of 204472 lines included. ...]

==== bsi-audit-automator/assets/prompts/etl_classify_documents.txt ====
You are an expert BSI (German Federal Office for Information Security) audit assistant. Your task is to analyze a list of document filenames and classify each one into a single, most appropriate category from a predefined list. The filenames often contain clues like "A.1", "A.4", "Netzplan", "Sicherheitsleitlinie", etc.

**Predefined Categories:**
- "Sicherheitsleitlinie": The main, high-level security policy document.
- "Organisations-Richtlinie": Other policies, guidelines, or organizational rules.
- "Informationsverbund": Documents describing the scope and boundary of the information network.
- "Netzplan": Network diagrams or topology plans.
- "Strukturanalyse": The core structural analysis document (often A.1).
- "Schutzbedarfsfeststellung": Protection needs assessment document (often A.2).
- "Modellierung": The IT Grundschutz modeling document (often A.3).
- "Grundschutz-Check": The implementation check of controls (often A.4).
- "Risikoanalyse": Risk analysis documents (often A.5).
- "Realisierungsplan": The risk treatment or implementation plan (often A.6).
- "Dienstleister-Liste": Lists of external service providers.
- "Sonstiges": Any other document that does not fit the above categories.

Analyze the following list of filenames and return a structured JSON response mapping each filename to its category.

**Filenames to Classify:**
---
{filenames_json}
---

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/assets/prompts/generic_question_prompt.txt ====
You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.

Rules:
1. Imperative: Based *only* on the following context retrieved from the customer's documentation, answer the following questions!
2. Imperative: If the provided context is insufficient to answer a question, state that clearly in your answer (e.g., "Cannot be determined from the provided documents.").
3. While you try to answer the questions, check if the context contains deviations from BSI Grundschutz.
4. Generate a finding for each deviation.
5. Categorize the finding as 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation).
6. Provide evidence and a reference to the document that caused the finding in finding->descripton.

**Provided Context:**
---
{context}
---

The questions to answer are:
{questions}

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/assets/prompts/generic_summary_prompt.txt ====
You are a BSI security auditor providing a final verdict on the {summary_topic}.
Based on the summary of findings from the previous sections provided below, provide a summary verdict ("Votum").

**Summary of Previous Findings:**
---
{previous_findings}
---

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/assets/prompts/stage_1_2_geltungsbereich.txt ====
You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.

Rules:
1. Imperative: Based *only* on the following context retrieved from the customer's documentation, answer the following questions!
2. Imperative: If the provided context is insufficient to answer a question, state that clearly in your answer (e.g., "Cannot be determined from the provided documents.").
3. While you try to answer the questions, check if the context contains deviations from BSI Grundschutz.
4. Generate a finding for each deviation.
5. Categorize the finding as 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation).
6. Provide evidence and a reference to the document that caused the finding in finding->descripton.

Your description should cover:
- The name of the informational asset network (Informationsverbund).
- The key business processes and applications included.
- The physical locations covered by the audit.
- Any specific systems or infrastructure that are part of the scope.

If the provided context is insufficient or empty, state that the scope could not be fully determined from the documents and must be clarified manually.

**Provided Context:**
---
{context}
---

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/assets/prompts/stage_3_1_aktualitaet.txt ====
You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.

Rules:
1. Imperative: Based *only* on the following context retrieved from the customer's documentation, answer the following questions!
2. Imperative: If the provided context is insufficient to answer a question, state that clearly in your answer (e.g., "Cannot be determined from the provided documents.").
3. While you try to answer the questions, check if the context contains deviations from BSI Grundschutz.
4. Generate a finding for each deviation.
5. Categorize the finding as 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation).
6. Provide evidence and a reference to the document that caused the finding in finding->descripton.


The questions to answer are:
1. Wurden alle Referenzdokumente A.0 gemäß den Vorgaben von A.0.3 Lenkung von Dokumenten überarbeitet?
2. Wurden alle Dateien zu den Referenzdokumenten A.1, A.2, A.3, A.5 und A.6 für das Audit neu erstellt?
3. Wurden alle Maßnahmen im A.4 IT-Grundschutz-Check innerhalb der letzten 12 Monate neu bewertet?
4. Datum der letzten inhaltlichen Änderung im Referenzdokument A.4 IT-Grundschutz-Check.

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.

**Provided Context:**
---
{context}
---

==== bsi-audit-automator/assets/prompts/stage_3_2_sicherheitsleitlinie.txt ====
You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.

Rules:
1. Imperative: Based *only* on the following context retrieved from the customer's documentation, answer the following questions!
2. Imperative: If the provided context is insufficient to answer a question, state that clearly in your answer (e.g., "Cannot be determined from the provided documents.").
3. While you try to answer the questions, check if the context contains deviations from BSI Grundschutz.
4. Generate a finding for each deviation.
5. Categorize the finding as 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation).
6. Provide evidence and a reference to the document that caused the finding in finding->descripton.

**Provided Context:**
---
{context}
---

The questions to answer are:
1. Ist die Leitlinie zur Informationssicherheit (A.0.1) sinnvoll und angemessen für den Antragsteller? Erfüllt die Leitlinie zur Informationssicherheit alle Aspekte gemäß den Anforderungen aus dem Baustein ISMS.1?
2. Decken sich die Sicherheitsziele aus der Leitlinie mit den Sicherheitsanforderungen der restlichen Referenzdokumente (A.0.2 bis A.0.5)?
3. Werden die Sicherheitsrichtlinien (A.0.1 bis A.0.5) durch das Management getragen und wurden sie veröffentlicht?

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/assets/prompts/stage_3_3_1_informationsverbund.txt ====
You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.

Rules:
1. Imperative: Based *only* on the following context retrieved from the customer's documentation, answer the following questions!
2. Imperative: If the provided context is insufficient to answer a question, state that clearly in your answer (e.g., "Cannot be determined from the provided documents.").
3. While you try to answer the questions, check if the context contains deviations from BSI Grundschutz.
4. Generate a finding for each deviation.
5. Categorize the finding as 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation).
6. Provide evidence and a reference to the document that caused the finding in finding->descripton.

**Provided Context:**
---
{context}
---

The questions to answer are:
1. Ist der Informationsverbund eindeutig abgegrenzt?
2. Sind alle infrastrukturellen, organisatorischen, personellen und technischen Komponenten im Informationsverbund enthalten, die zur Aufgabenerfüllung notwendig sind?
3. Sind die Schnittstellen zu allen weiteren Prozessen definiert?

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/assets/prompts/stage_3_3_2_netzplan.txt ====
You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.

Rules:
1. Imperative: Based *only* on the following context retrieved from the customer's documentation, answer the following questions!
2. Imperative: If the provided context is insufficient to answer a question, state that clearly in your answer (e.g., "Cannot be determined from the provided documents.").
3. While you try to answer the questions, check if the context contains deviations from BSI Grundschutz.
4. Generate a finding for each deviation.
5. Categorize the finding as 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation).
6. Provide evidence and a reference to the document that caused the finding in finding->descripton.

**Provided Context:**
---
{context}
---

The questions to answer are:
1. Optional: Liegt ein aktueller und vollständiger bereinigter Netzplan vor?
2. Sind alle Komponenten im Netzplan mit den korrekten Bezeichnern versehen?

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/assets/prompts/stage_3_3_3_geschaeftsprozesse.txt ====
You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.

Rules:
1. Imperative: Based *only* on the following context retrieved from the customer's documentation, answer the following questions!
2. Imperative: If the provided context is insufficient to answer a question, state that clearly in your answer (e.g., "Cannot be determined from the provided documents.").
3. While you try to answer the questions, check if the context contains deviations from BSI Grundschutz.
4. Generate a finding for each deviation.
5. Categorize the finding as 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation).
6. Provide evidence and a reference to the document that caused the finding in finding->descripton.

**Provided Context:**
---
{context}
---

The questions to answer are:
1. Enthält die Liste der Geschäftsprozesse alle benötigten Informationen (eindeutige Bezeichnung, Name, Prozessverantwortlicher, kurze Beschreibung, benötigte Anwendungen)?

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/assets/prompts/stage_3_4_1_schutzbedarfskategorien.txt ====
You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.

Rules:
1. Imperative: Based *only* on the following context retrieved from the customer's documentation, answer the following questions!
2. Imperative: If the provided context is insufficient to answer a question, state that clearly in your answer (e.g., "Cannot be determined from the provided documents.").
3. While you try to answer the questions, check if the context contains deviations from BSI Grundschutz.
4. Generate a finding for each deviation.
5. Categorize the finding as 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation).
6. Provide evidence and a reference to the document that caused the finding in finding->descripton.

**Provided Context:**
---
{context}
---

The questions to answer are:
1. Ist die Definition der Schutzbedarfskategorien plausibel und für den Informationsverbund angemessen?
2. Wurden mehr als drei Schutzbedarfskategorien definiert?

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/assets/prompts/stage_3_5_1_modellierungsdetails.txt ====
You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.

Rules:
1. Imperative: Based *only* on the following context retrieved from the customer's documentation, answer the following questions!
2. Imperative: If the provided context is insufficient to answer a question, state that clearly in your answer (e.g., "Cannot be determined from the provided documents.").
3. While you try to answer the questions, check if the context contains deviations from BSI Grundschutz.
4. Generate a finding for each deviation.
5. Categorize the finding as 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation).
6. Provide evidence and a reference to the document that caused the finding in finding->descripton.

**Provided Context:**
---
{context}
---

The questions to answer are:
1. Ist jeder Baustein des IT-Grundschutz-Kompendiums auf alle relevanten Zielobjekte angewandt?
2. Ist für jeden Baustein des IT-Grundschutz-Kompendiums, der nicht angewandt wurde, eine plausible Begründung vorhanden?
3. Wurden alle Zielobjekte angemessen berücksichtigt, für die keine Bausteine des IT-Grundschutz-Kompendiums vorhanden sind?
4. Gibt es benutzerdefinierte Bausteine?

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/assets/prompts/stage_3_5_2_ergebnis_modellierung.txt ====
You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.

Rules:
1. Imperative: Based *only* on the following context
2. Imperative: If the provided context is insufficient to answer a question, state that clearly in your answer (e.g., "Cannot be determined from the provided documents.").

Based on the summary of findings from the previous modeling sections provided below, provide a summary verdict ("Votum").

**Summary of Previous Findings:**
---
{previous_findings}
---

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/assets/prompts/stage_3_6_1_grundschutz_check.txt ====
You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.

Rules:
1. Imperative: Based *only* on the following context retrieved from the customer's documentation, answer the following questions!
2. Imperative: If the provided context is insufficient to answer a question, state that clearly in your answer (e.g., "Cannot be determined from the provided documents.").
3. While you try to answer the questions, check if the context contains deviations from BSI Grundschutz.
4. Generate a finding for each deviation.
5. Categorize the finding as 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation).
6. Provide evidence and a reference to the document that caused the finding in finding->descripton.

**Provided Context:**
---
{context}
---

The questions to answer are:
1. Wurde zu jeder Anforderung der Umsetzungsstatus erhoben?
2. Wurden alle Anforderungen mit Umsetzungsstatus „entbehrlich“ plausibel begründet?
3. Sind alle MUSS-Teilanforderungen erfüllt?
4. Wurden die nicht oder nur teilweise umgesetzten Anforderungen im Referenzdokument A.6 dokumentiert?
5. Sind alle Anforderungen innerhalb der letzten 12 Monate überprüft worden?

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/assets/prompts/stage_3_9_ergebnis.txt ====
You are a BSI security auditor providing a final verdict on the document review phase.
Based on the summary of findings from the previous document review sections provided below, answer the following question and provide a summary verdict ("Votum").

**Summary of Previous Findings:**
---
{previous_findings}
---

The question to answer is:
1. Ist eine Fortführung des Audits mit der Vor-Ort-Prüfung möglich?

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/assets/prompts/stage_4_1_1_auswahl_bausteine_erst.txt ====
You are a BSI Lead Auditor planning an initial certification audit (Erstzertifizierung). Your plan must strictly adhere to the rules outlined in the official BSI "Auditierungsschema".

**Official Rules for Baustein Selection (from Auditierungsschema, Chapter 4.3):**
1.  **Minimum Count:** You MUST audit at least 6 Bausteine.
2.  **Mandatory Baustein:** The Baustein 'ISMS.1 Sicherheitsmanagement' MUST be included in your selection.
3.  **Risk-Oriented Selection:** You must select the other Bausteine based on risk, ensuring that you cover a variety of layers (Schichten) like ORP, CON, OPS, NET, INF, and SYS.
4.  **Justification:** You must provide a concise, professional justification (Begründung) for the selection of each Baustein.

**Example of a valid selection:**
- ISMS.1 Sicherheitsmanagement
- ORP.4 Identitäts- und Berechtigungsmanagement
- CON.3 Datensicherungskonzept
- OPS.1.1.1 Allgemeiner IT-Betrieb
- NET.3.2 Firewall
- INF.2 Rechenzentrum sowie Serverraum
- SYS.1.1 Allgemeiner Server

Based on these rules, generate a plausible and compliant audit plan. Your response MUST be a single JSON object that strictly adheres to the provided JSON schema, containing a list of rows for the planning table.
==== bsi-audit-automator/assets/prompts/stage_4_1_2_auswahl_bausteine_ueberwachung.txt ====
You are a BSI Lead Auditor planning a surveillance audit (Überwachungsaudit) for a customer. Your plan must strictly adhere to the rules outlined in the official BSI "Auditierungsschema".

**Official Rules for Baustein Selection (from Auditierungsschema, Chapter 4.4):**
1.  **Mandatory Baustein:** The Baustein 'ISMS.1 Sicherheitsmanagement' MUST be audited in every surveillance audit.
2.  **Minimum Count:** You MUST audit at least two other Bausteine in addition to ISMS.1.
3.  **Risk-Oriented Selection:** You must select the other Bausteine based on significant changes, previous deviations, or risk. Ensure you cover a variety of layers (Schichten) like ORP, CON, OPS, NET, INF, and SYS.
4.  **Justification:** You must provide a concise, professional justification (Begründung) for the selection of each Baustein.

**Example of a valid selection:**
- ISMS.1 Sicherheitsmanagement
- ORP.2 Organisation
- SYS.2.1 Allgemeiner Client

Based on these rules, generate a plausible and compliant audit plan. Your response MUST be a single JSON object that strictly adheres to the provided JSON schema, containing a list of rows for the planning table.
==== bsi-audit-automator/assets/prompts/stage_7_2_abweichungen.txt ====
You are an auditor summarizing findings for the appendix of a BSI audit report.
Given the following raw list of deviations and negative findings extracted from the audit, your task is to create a structured table of these items.

For each item, perform the following actions:
1.  Assign a unique ID in the format "AG-X" (for 'Abweichung, Geringfügig') or "AS-X" (for 'Abweichung, Schwerwiegend') where X is a sequential number. Use your judgment based on the severity implied by the text.
2.  Write a concise summary of the issue in the "Abweichung / Empfehlung" field.
3.  Propose a reasonable deadline ("Behebungsfrist / Nachweis") of 30 or 60 days from today.
4.  Set the initial "Status der Behebung" to "Offen".

**Raw Findings from Audit:**
{raw_deviations}

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/envs.sh ====
#!/bin/bash
#
# DYNAMIC Environment variable setup for local BSI Audit Automator development.
#
# This script dynamically fetches configuration from your Terraform state,
# ensuring your local environment matches the cloud deployment.
#
# It also defines a helper function `bsi-auditor` to simplify running the app.
#
# PREREQUISITES:
#   - You must have run 'terraform apply' in the ./terraform directory.
#   - You must have the 'terraform' CLI installed and in your PATH.
#
# USAGE:
#   Run this command from the project root (the 'bsi-audit-automator' directory):
#      source ./envs.sh
#
#   Then, you can run the application like this:
#      bsi-auditor --run-etl
#      bsi-auditor --run-stage Chapter-1
#
set -e # Exit on error

TERRAFORM_DIR="../terraform"

if [ ! -d "$TERRAFORM_DIR" ]; then
    echo "❌ Error: Terraform directory not found at '$TERRAFORM_DIR'. Please run this script from the project root."
    return 1
fi
if ! command -v terraform &> /dev/null; then
    echo "❌ Error: 'terraform' command not found. Please install Terraform."
    return 1
fi

echo "🔹 Fetching infrastructure details from Terraform..."

# --- Dynamic Values from Terraform ---
export GCP_PROJECT_ID="$(terraform -chdir=${TERRAFORM_DIR} output -raw project_id)"
export VERTEX_AI_REGION="$(terraform -chdir=${TERRAFORM_DIR} output -raw region)"
export BUCKET_NAME="$(terraform -chdir=${TERRAFORM_DIR} output -raw vector_index_data_gcs_path | cut -d'/' -f3)"
export INDEX_ENDPOINT_ID_FULL="$(terraform -chdir=${TERRAFORM_DIR} output -raw vertex_ai_index_endpoint_id)"
export GCP_PROJECT_NUMBER="$(echo "${INDEX_ENDPOINT_ID_FULL}" | cut -d'/' -f2)"
export INDEX_ENDPOINT_ID="$(basename "${INDEX_ENDPOINT_ID_FULL}")"
# NEW: Fetch the public domain if it exists, otherwise set to empty string.
export INDEX_ENDPOINT_PUBLIC_DOMAIN="$(terraform -chdir=${TERRAFORM_DIR} output -raw public_endpoint_domain_name 2>/dev/null || echo '')"


# --- Static Values for Local Development ---
# These prefixes now reflect the simpler GCS layout.
export SOURCE_PREFIX="source_documents/"
export OUTPUT_PREFIX="output/"
export ETL_STATUS_PREFIX="output/etl_status/"

# Manually set the audit type and test mode for your local run
export AUDIT_TYPE="Zertifizierungsaudit"
export TEST="true"
export MAX_CONCURRENT_AI_REQUESTS=5 # New: Tunable concurrency limit

# --- NEW: Helper function for correct execution ---
# This alias ensures we always run the application as a module,
# which correctly resolves the relative imports in src/main.py.
bsi-auditor() {
    python -m src.main "$@"
}


set +e
echo "✅ Environment variables configured successfully'."
echo "   - GCP_PROJECT_ID: ${GCP_PROJECT_ID}"
echo "   - BUCKET_NAME:    ${BUCKET_NAME}"
if [ -n "$INDEX_ENDPOINT_PUBLIC_DOMAIN" ]; then
    echo "   - PUBLIC_ENDPOINT: ${INDEX_ENDPOINT_PUBLIC_DOMAIN}"
fi
echo "   - TEST mode:      ${TEST}"
echo ""
echo "👉 A new command 'bsi-auditor' is now available in your shell."
echo "   Run the app with: bsi-auditor --run-stage Chapter-1"
==== bsi-audit-automator/prompts/generic_question_prompt.txt ====
You are a very experienced BSI security auditor with an in depth knowledge of BSI Standards 200-1, 200-2 and 200-3 analyzing the correctness of the customer's reference documents for an audit.

Rules:
1. Imperative: Based *only* on the following context retrieved from the customer's documentation, answer the following questions!
2. Imperative: If the provided context is insufficient to answer a question, state that clearly in your answer (e.g., "Cannot be determined from the provided documents.").
3. While you try to answer the questions, check if the context contains deviations from BSI Grundschutz.
4. Generate a finding for each deviation.
5. Categorize the finding as 'AG' (Minor Deviation), 'AS' (Major Deviation), 'E' (Recommendation).
6. Provide evidence and a reference to the document that caused the finding in finding->descripton.

**Provided Context:**
---
{context}
---

The questions to answer are:
{questions}

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/prompts/generic_summary_prompt.txt ====
You are a BSI security auditor providing a final verdict on the {summary_topic}.
Based on the summary of findings from the previous sections provided below, provide a summary verdict ("Votum").

**Summary of Previous Findings:**
---
{previous_findings}
---

Your response MUST be a single JSON object that strictly adheres to the provided JSON schema.
==== bsi-audit-automator/requirements.txt ====
# GCP and Vertex AI
google-cloud-storage
google-cloud-aiplatform

# For local development, to load .env files
python-dotenv

# For data processing and validation
PyMuPDF # A lightweight, fast library for PDF processing with no system dependencies
langchain # For document chunking logic
langchain-community # Required for UnstructuredFileLoader
jsonschema # For validating AI model outputs
==== bsi-audit-automator/scripts/refresh_audit_data.sh ====
#!/bin/bash
set -euo pipefail

# ===================================================================
# SCRIPT TO REFRESH AUDIT DATA AND CLEAR THE INDEX
# ===================================================================
#
# WHAT IT DOES:
# This script provides a "fast refresh" for an audit by clearing out
# all existing data from GCS and triggering an update on the Vertex
# AI Index to remove the old embeddings. This is a DATA-LAYER
# operation and does NOT destroy the underlying cloud infrastructure.
#
# It performs the following actions:
# 1. Archives the old vector embeddings by moving them.
# 2. Deletes all previous outputs and status markers.
# 3. Triggers a manual update of the Vertex AI Index, which causes it
#    to re-scan the (now empty) source folder and remove all entries.
#
# WHEN TO USE IT:
# Use this script when you receive new source documents for an
# existing audit and want to start the ETL and analysis process
# from a clean slate without a full `terraform apply`.
#
# PREREQUISITES:
#   - Must be run from the project root ('bsi-audit-automator/').
#   - 'gcloud', 'gsutil', and 'terraform' CLIs must be installed.
#

echo "✅ This script will perform a 'fast refresh' of the audit data."
echo "   It will archive old embeddings and clear the Vertex AI Index."

# --- Cleanup handler: ensures the temporary metadata file is deleted on exit ---
cleanup() {
  rm -f index_metadata.yaml
  echo "🔹 Temporary metadata file cleaned up."
}
trap cleanup EXIT

# --- Configuration & Validation ---
TERRAFORM_DIR="../terraform"
METADATA_FILE="index_metadata.yaml"

if [ ! -d "$TERRAFORM_DIR" ]; then
    echo "❌ Error: Terraform directory not found at '$TERRAFORM_DIR'. Please run this from the project root."
    exit 1
fi

echo "🔹 Fetching infrastructure details from Terraform state..."

# --- Dynamic Values from Terraform ---
PROJECT_ID="$(terraform -chdir=${TERRAFORM_DIR} output -raw project_id)"
REGION="$(terraform -chdir=${TERRAFORM_DIR} output -raw region)"
BUCKET_NAME="$(terraform -chdir=${TERRAFORM_DIR} output -raw vector_index_data_gcs_path | cut -d'/' -f3)"
INDEX_ID="$(terraform -chdir=${TERRAFORM_DIR} output -raw vertex_ai_index_id | xargs basename)"

CONTENTS_DELTA_URI="gs://${BUCKET_NAME}/vector_index_data/"
ARCHIVE_TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
ARCHIVE_PATH="gs://${BUCKET_NAME}/vector_index_data_archive/${ARCHIVE_TIMESTAMP}/"

# --- User Confirmation ---
echo "-----------------------------------------------------"
echo "The following data-layer actions will be performed:"
echo "  1. MOVE all embedding files from:"
echo "     ${CONTENTS_DELTA_URI}"
echo "     TO (archive):"
echo "     ${ARCHIVE_PATH}"
echo "  2. DELETE all previous results and status markers from:"
echo "     gs://${BUCKET_NAME}/output/"
echo "  3. TRIGGER an update on Index '${INDEX_ID}' to remove all entries."
echo "-----------------------------------------------------"
read -p "Are you sure you want to refresh the audit data? (y/n) " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Aborted by user."
    exit 1
fi

# --- Execute GCS Data Archival and Cleanup ---
echo "📦 Archiving old embedding data..."
# Use || true to prevent script failure if the source directory is empty
gsutil -m mv "${CONTENTS_DELTA_URI}*" "${ARCHIVE_PATH}" || true

echo "🗑️  Deleting old output files..."
gsutil -m rm -r "gs://${BUCKET_NAME}/output/*" || true
echo "✅ GCS data archival and cleanup complete."

# --- Trigger Index Update ---
echo "🔹 Generating temporary metadata file for index update..."
cat <<EOF > ${METADATA_FILE}
contentsDeltaUri: "${CONTENTS_DELTA_URI}"
config:
  dimensions: 3072
  approximateNeighborsCount: 150
  algorithmConfig:
    treeAhConfig:
      leafNodeEmbeddingCount: 500
EOF

echo "🚀 Sending update command to Vertex AI Index '${INDEX_ID}'..."
gcloud ai indexes update "${INDEX_ID}" \
  --metadata-file="./${METADATA_FILE}" \
  --project="${PROJECT_ID}" \
  --region="${REGION}"

echo ""
echo "✅ Data refresh process initiated."
echo "   The index will now update and remove the old embeddings."
echo "   You can monitor its 'Dense vector count' in the GCP Console."
echo "   Next Steps:"
echo "   1. Upload the NEW set of source documents to gs://${BUCKET_NAME}/source_documents/"
echo "   2. Run the pipeline starting with the ETL job: ./scripts/execute-audit-job.sh"
==== bsi-audit-automator/src/audit/controller.py ====
# src/audit/controller.py
import logging
import json
import asyncio
from typing import List, Dict, Any
from google.cloud.exceptions import NotFound

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient
from src.clients.rag_client import RagClient
from src.audit.stages.stage_1_general import Chapter1Runner
from src.audit.stages.stage_3_dokumentenpruefung import Chapter3Runner
from src.audit.stages.stage_4_pruefplan import Chapter4Runner
from src.audit.stages.stage_5_vor_ort_audit import Chapter5Runner
from src.audit.stages.stage_7_anhang import Chapter7Runner

class AuditController:
    """Orchestrates the entire staged audit process with lazy initialization of runners."""

    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self.rag_client = rag_client
        self.all_findings: List[Dict[str, Any]] = []

        self.stage_runner_classes = {
            "Chapter-1": Chapter1Runner,
            "Chapter-3": Chapter3Runner,
            "Chapter-4": Chapter4Runner,
            "Chapter-5": Chapter5Runner,
            "Chapter-7": Chapter7Runner,
        }
        self.runner_dependencies = {
            "Chapter-1": (self.config, self.ai_client, self.rag_client),
            "Chapter-3": (self.config, self.ai_client, self.rag_client),
            "Chapter-4": (self.config, self.ai_client),
            "Chapter-5": (self.config, self.gcs_client, self.ai_client),
            "Chapter-7": (self.config, self.gcs_client), # AI Client no longer needed here
        }
        logging.info("Audit Controller initialized with lazy stage loading and findings collector.")

    def _extract_and_store_findings(self, stage_name: str, result_data: Dict[str, Any]) -> None:
        """
        Parses stage results, finds structured `finding` objects, and appends
        any deviations or recommendations to the central findings list.

        Args:
            stage_name: The name of the stage that produced the result (e.g., 'Chapter-3').
            result_data: The JSON-like dictionary returned by the stage runner.
        """
        if not result_data:
            return

        for subchapter_key, subchapter_data in result_data.items():
            if isinstance(subchapter_data, dict) and 'finding' in subchapter_data:
                finding = subchapter_data['finding']
                if finding and finding.get('category') != 'OK':
                    self.all_findings.append({
                        "id": f"{finding['category']}-{len(self.all_findings) + 1}",
                        "category": finding['category'],
                        "description": finding['description'],
                        "source_chapter": stage_name.replace('Chapter-', '') + f" ({subchapter_key})"
                    })
                    logging.info(f"Collected finding from {stage_name}/{subchapter_key}: {finding['category']}")

    def _save_all_findings(self) -> None:
        """
        Saves the centrally collected list of all non-'OK' findings to a 
        dedicated JSON file in GCS for final report assembly.
        """
        if not self.all_findings:
            logging.info("No findings were collected during the audit. Skipping save.")
            return

        findings_path = f"{self.config.output_prefix}results/all_findings.json"
        self.gcs_client.upload_from_string(
            content=json.dumps(self.all_findings, indent=2, ensure_ascii=False),
            destination_blob_name=findings_path
        )
        logging.info(f"Successfully saved {len(self.all_findings)} findings to {findings_path}")

    async def run_all_stages(self, force_overwrite: bool = False) -> None:
        """
        Runs all defined audit stages in sequence, collecting findings after each
        stage. It respects resumability by skipping already completed stages.

        Args:
            force_overwrite: If True, all stages will be re-run even if results exist.
        """
        logging.info("Starting to run all audit stages.")
        for stage_name in self.stage_runner_classes.keys():
            await self.run_single_stage(stage_name, force_overwrite=force_overwrite)
        
        self._save_all_findings()
        logging.info("All audit stages completed.")

    async def run_single_stage(self, stage_name: str, force_overwrite: bool = False) -> Dict[str, Any]:
        """
        Runs a single, specified audit stage and collects findings from its result.

        Args:
            stage_name: The name of the stage to run (e.g., 'Chapter-1').
            force_overwrite: If True, the stage will run even if a result file
                             already exists. If False, it will skip.

        Returns:
            The result data dictionary from the completed stage.
        """
        if stage_name not in self.stage_runner_classes:
            logging.error(f"Unknown stage '{stage_name}'. Available: {list(self.stage_runner_classes.keys())}")
            raise ValueError(f"Unknown stage: {stage_name}")

        stage_output_path = f"{self.config.output_prefix}results/{stage_name}.json"
        
        if not force_overwrite:
            try:
                existing_data = self.gcs_client.read_json(stage_output_path)
                logging.info(f"Stage '{stage_name}' already completed. Skipping generation.")
                self._extract_and_store_findings(stage_name, existing_data)
                return existing_data
            except NotFound:
                logging.info(f"No results for stage '{stage_name}' found. Generating...")
            except Exception as e:
                logging.warning(f"Could not read existing state for stage '{stage_name}': {e}. Proceeding.")
        else:
            logging.info(f"Force overwrite enabled for stage '{stage_name}'. Running generation.")

        runner_class = self.stage_runner_classes[stage_name]
        dependencies = self.runner_dependencies[stage_name]
        stage_runner = runner_class(*dependencies)
        logging.info(f"Initialized runner for stage: {stage_name}")

        try:
            result_data = await stage_runner.run()
            self.gcs_client.upload_from_string(
                content=json.dumps(result_data, indent=2, ensure_ascii=False),
                destination_blob_name=stage_output_path
            )
            logging.info(f"Successfully saved results for stage '{stage_name}'.")
            self._extract_and_store_findings(stage_name, result_data)
            return result_data
        except Exception as e:
            logging.error(f"Stage '{stage_name}' failed: {e}", exc_info=True)
            raise
==== bsi-audit-automator/src/audit/report_generator.py ====
# src/audit/report_generator.py
import logging
import json
from google.cloud.exceptions import NotFound
from typing import Dict, Any

from src.config import AppConfig
from src.clients.gcs_client import GcsClient

class ReportGenerator:
    """Assembles the final audit report from individual stage stubs."""
    LOCAL_MASTER_TEMPLATE_PATH = "assets/json/master_report_template.json"
    STAGES_TO_AGGREGATE = ["Chapter-1", "Chapter-3", "Chapter-4", "Chapter-5", "Chapter-7"]

    def __init__(self, config: AppConfig, gcs_client: GcsClient):
        self.config = config
        self.gcs_client = gcs_client
        self.gcs_report_path = "report/master_report_template.json"
        logging.info("Report Generator initialized.")

    def _initialize_report_on_gcs(self) -> dict:
        """
        Loads the report template. It first tries to load from a working copy on GCS,
        falling back to the local `master_report_template.json` if it doesn't exist.
        """
        try:
            report = self.gcs_client.read_json(self.gcs_report_path)
            logging.info(f"Loaded existing report template from GCS: {self.gcs_report_path}")
            return report
        except NotFound:
            logging.info("No report template found on GCS. Initializing from local asset.")
            with open(self.LOCAL_MASTER_TEMPLATE_PATH, 'r', encoding='utf-8') as f:
                report = json.load(f)
            
            # Pre-populate with basic info
            report['bsiAuditReport']['titlePage']['auditedInstitution'] = "Audited Institution"
            audittyp_section = report.get('bsiAuditReport', {}).get('allgemeines', {}).get('audittyp', {})
            if audittyp_section:
                audittyp_section['content'] = self.config.audit_type
            
            self.gcs_client.upload_from_string(
                content=json.dumps(report, indent=2, ensure_ascii=False),
                destination_blob_name=self.gcs_report_path
            )
            logging.info(f"Saved initial report template to GCS: {self.gcs_report_path}")
            return report

    def _populate_chapter_1(self, report: dict, stage_data: dict) -> None:
        """Populates the 'Allgemeines' (Chapter 1) of the report defensively."""
        target_chapter = report.get('bsiAuditReport', {}).get('allgemeines')
        if not target_chapter:
            logging.error("Report template is missing 'bsiAuditReport.allgemeines' structure. Cannot populate Chapter 1.")
            return

        # Populate Informationsverbund (1.4) and Geltungsbereich (1.2) from the same AI result
        geltungsbereich_data = stage_data.get('geltungsbereichDerZertifizierung', {}) # This is the key used in stage_1_runner
        target_section_gelt = target_chapter.get('geltungsbereichDerZertifizierung')
        target_section_info = target_chapter.get('informationsverbund')

        # Populate Geltungsbereich with the main text
        if target_section_gelt and isinstance(geltungsbereich_data, dict):
            # Main description text goes into Geltungsbereich
            final_text = geltungsbereich_data.get('description', '')
            if isinstance(geltungsbereich_data.get('finding'), dict):
                finding = geltungsbereich_data['finding']
                if finding.get('category') != 'OK':
                    final_text += f"\n\nFeststellung: [{finding.get('category')}] {finding.get('description')}"
            
            # Defensively ensure the structure exists before writing to it
            if 'content' not in target_section_gelt or not isinstance(target_section_gelt.get('content'), list):
                target_section_gelt['content'] = []
            if not target_section_gelt['content']:
                target_section_gelt['content'].append({"type": "prose", "text": ""})
            target_section_gelt['content'][0]['text'] = final_text
        else:
            logging.warning("Could not populate 'geltungsbereichDerZertifizierung' due to missing key in stage data or report template.")

        # Populate Informationsverbund with its specific fields from the same AI result
        if target_section_info and isinstance(geltungsbereich_data, dict) and isinstance(target_section_info.get('content'), list):
            target_section_info['content'][0]['text'] = geltungsbereich_data.get('kurzbezeichnung', '')
            target_section_info['content'][1]['text'] = geltungsbereich_data.get('kurzbeschreibung', '')
        else:
             logging.warning("Could not populate 'informationsverbund' due to missing key in stage data or report template.")

        # Populate Audittyp (1.3)
        # The key in stage_1_general is 'audittyp' and it holds a simple string.
        target_section_typ = target_chapter.get('audittyp')
        if target_section_typ and 'content' in target_section_typ:
            target_section_typ['content'] = stage_data.get('audittyp', {}).get('content', self.config.audit_type)
        else:
            logging.warning("Could not populate 'audittyp' due to missing key in stage data or report template.")


    def _populate_chapter_7_findings(self, report: dict) -> None:
        """Populates the findings tables in Chapter 7.2 from the central findings file."""
        logging.info("Populating Chapter 7.2 with collected findings...")
        findings_path = f"{self.config.output_prefix}results/all_findings.json"
        try:
            all_findings = self.gcs_client.read_json(findings_path)
        except NotFound:
            logging.warning("Central findings file not found. Chapter 7.2 will be empty.")
            return

        findings_section = report.get('bsiAuditReport', {}).get('anhang', {}).get('abweichungenUndEmpfehlungen')
        if not findings_section:
            logging.warning("Report template is missing '...anhang.abweichungenUndEmpfehlungen'. Skipping findings population.")
            return

        ag_table = findings_section.get('geringfuegigeAbweichungen', {}).get('table', {}).get('rows')
        as_table = findings_section.get('schwerwiegendeAbweichungen', {}).get('table', {}).get('rows')
        e_table = findings_section.get('empfehlungen', {}).get('table', {}).get('rows')

        if not all(isinstance(table, list) for table in [ag_table, as_table, e_table]):
            logging.warning("One or more findings tables are missing the 'rows' list in the report template. Skipping population.")
            return

        ag_table.clear(); as_table.clear(); e_table.clear()

        for finding in all_findings:
            category = finding.get('category')
            base_row_data = {
                "Nr.": finding.get('id', f"{category}-?"),
                "Quelle (Kapitel)": finding.get('source_chapter', 'N/A'),
                "Behebungsfrist": "30 Tage nach Audit",  # Placeholder
                "Status": "Offen"  # Default
            }

            if category == 'AG':
                row_data = base_row_data.copy()
                row_data["Beschreibung der Abweichung"] = finding.get('description', 'N/A')
                ag_table.append(row_data)
            elif category == 'AS':
                row_data = base_row_data.copy()
                row_data["Beschreibung der Abweichung"] = finding.get('description', 'N/A')
                as_table.append(row_data)
            elif category == 'E':
                row_data = base_row_data.copy()
                row_data["Beschreibung der Empfehlung"] = finding.get('description', 'N/A')
                # Recommendations don't have a strict deadline or status in the same way
                row_data["Behebungsfrist"] = "N/A"
                row_data["Status"] = "Zur Umsetzung empfohlen"
                e_table.append(row_data)
        
        logging.info(f"Populated Chapter 7.2 with {len(all_findings)} total findings.")

    def assemble_report(self) -> None:
        """
        Main method to assemble the final report. It loads all stage results
        and collected findings, populates a master template, and saves the final
        output to GCS.
        """
        report = self._initialize_report_on_gcs()

        for stage_name in self.STAGES_TO_AGGREGATE:
            stage_output_path = f"{self.config.output_prefix}results/{stage_name}.json"
            try:
                stage_data = self.gcs_client.read_json(stage_output_path)
                self._populate_report(report, stage_name, stage_data)
            except NotFound:
                logging.warning(f"Result for stage '{stage_name}' not found. Skipping population.")
                continue
        
        self._populate_chapter_7_findings(report)

        final_report_path = f"{self.config.output_prefix}final_audit_report.json"
        self.gcs_client.upload_from_string(
            content=json.dumps(report, indent=2, ensure_ascii=False),
            destination_blob_name=final_report_path
        )
        logging.info(f"Final report assembled and saved to: gs://{self.config.bucket_name}/{final_report_path}")
        
        self.gcs_client.upload_from_string(
            content=json.dumps(report, indent=2, ensure_ascii=False),
            destination_blob_name=self.gcs_report_path
        )
        logging.info(f"Updated master report state on GCS: gs://{self.config.bucket_name}/{self.gcs_report_path}")

    def _populate_chapter_3(self, report: dict, stage_data: dict) -> None:
        """Populates Chapter 3 (Dokumentenprüfung) content into the report."""
        chapter_3_target = report.get('bsiAuditReport', {}).get('dokumentenpruefung')
        if not chapter_3_target:
            logging.error("Report template is missing 'bsiAuditReport.dokumentenpruefung' structure. Cannot populate Chapter 3.")
            return

        # Mapping from stage_data keys to their location in the report template
        key_to_path_map = {
            "aktualitaetDerReferenzdokumente": ["aktualitaetDerReferenzdokumente"],
            "sicherheitsleitlinieUndRichtlinienInA0": ["sicherheitsleitlinieUndRichtlinienInA0"],
            # Strukturanalyse A.1
            "definitionDesInformationsverbundes": ["strukturanalyseA1", "definitionDesInformationsverbundes"],
            "bereinigterNetzplan": ["strukturanalyseA1", "bereinigterNetzplan"],
            "listeDerGeschaeftsprozesse": ["strukturanalyseA1", "listeDerGeschaeftsprozesse"],
            "listeDerAnwendungen": ["strukturanalyseA1", "listeDerAnwendungen"],
            "listeDerItSysteme": ["strukturanalyseA1", "listeDerItSysteme"],
            "listeDerRaeumeGebaeudeStandorte": ["strukturanalyseA1", "listeDerRaeumeGebaeudeStandorte"],
            "listeDerKommunikationsverbindungen": ["strukturanalyseA1", "listeDerKommunikationsverbindungen"],
            "stichprobenDokuStrukturanalyse": ["strukturanalyseA1", "stichprobenDokuStrukturanalyse"],
            "listeDerDienstleister": ["strukturanalyseA1", "listeDerDienstleister"],
            "ergebnisDerStrukturanalyse": ["strukturanalyseA1", "ergebnisDerStrukturanalyse"],
            # Schutzbedarfsfeststellung A.2
            "definitionDerSchutzbedarfskategorien": ["schutzbedarfsfeststellungA2", "definitionDerSchutzbedarfskategorien"],
            "schutzbedarfGeschaeftsprozesse": ["schutzbedarfsfeststellungA2", "schutzbedarfGeschaeftsprozesse"],
            "schutzbedarfAnwendungen": ["schutzbedarfsfeststellungA2", "schutzbedarfAnwendungen"],
            "schutzbedarfItSysteme": ["schutzbedarfsfeststellungA2", "schutzbedarfItSysteme"],
            "schutzbedarfRaeume": ["schutzbedarfsfeststellungA2", "schutzbedarfRaeume"],
            "schutzbedarfKommunikationsverbindungen": ["schutzbedarfsfeststellungA2", "schutzbedarfKommunikationsverbindungen"],
            "stichprobenDokuSchutzbedarf": ["schutzbedarfsfeststellungA2", "stichprobenDokuSchutzbedarf"],
            "ergebnisDerSchutzbedarfsfeststellung": ["schutzbedarfsfeststellungA2", "ergebnisDerSchutzbedarfsfeststellung"],
            # Modellierung A.3
            "modellierungsdetails": ["modellierungDesInformationsverbundesA3", "modellierungsdetails"],
            "ergebnisDerModellierung": ["modellierungDesInformationsverbundesA3", "ergebnisDerModellierung"],
            # IT-Grundschutz-Check A.4
            "detailsZumItGrundschutzCheck": ["itGrundschutzCheckA4", "detailsZumItGrundschutzCheck"],
            "benutzerdefinierteBausteine": ["itGrundschutzCheckA4", "benutzerdefinierteBausteine"],
            "ergebnisItGrundschutzCheck": ["itGrundschutzCheckA4", "ergebnisItGrundschutzCheck"],
            # Risikoanalyse A.5 & Realisierungsplan A.6
            "risikoanalyseA5": ["risikoanalyseA5", "risikoanalyse"],
            "ergebnisRisikoanalyse": ["risikoanalyseA5", "ergebnisRisikoanalyse"],
            "realisierungsplanA6": ["realisierungsplanA6", "realisierungsplan"],
            "ergebnisRealisierungsplan": ["realisierungsplanA6", "ergebnisRealisierungsplan"],
            # Final result
            "ergebnisDerDokumentenpruefung": ["ergebnisDerDokumentenpruefung"],
        }

        for subchapter_key, result in stage_data.items():
            if not isinstance(result, dict): continue
            
            path_keys = key_to_path_map.get(subchapter_key)
            if not path_keys:
                logging.warning(f"No path mapping found for stage_data key '{subchapter_key}'. Skipping population.")
                continue

            target_section = chapter_3_target
            try:
                for key in path_keys:
                    target_section = target_section.get(key, {})
            except (AttributeError, TypeError):
                 logging.warning(f"Could not traverse path {path_keys} for '{subchapter_key}' in report template."); continue

            if not target_section:
                logging.warning(f"Could not find target section for '{subchapter_key}' in report template."); continue
            
            # Populate finding text
            if 'finding' in result and isinstance(result.get('finding'), dict):
                finding = result['finding']
                finding_text = f"[{finding.get('category')}] {finding.get('description')}"
                for item in target_section.get("content", []):
                    if item.get("type") == "finding":
                        item["findingText"] = finding_text
                        break
            
            # Populate answers for questions or text for prose
            if "answers" in result:
                answers = result.get("answers", [])
                answer_idx = 0
                for item in target_section.get("content", []):
                    if item.get("type") == "question":
                        if answer_idx < len(answers):
                            item["answer"] = answers[answer_idx]
                            answer_idx += 1
                        else:
                            logging.warning(f"Not enough answers in result for questions in '{subchapter_key}'")
                            break
            elif "votum" in result:
                for item in target_section.get("content", []):
                    if item.get("type") == "prose":
                        item["text"] = result.get("votum", "")
                        break
            
            # NEW: Populate table data
            if "table" in result and isinstance(result.get("table"), dict):
                target_table_section = target_section.get("table")
                if isinstance(target_table_section, dict):
                    target_table_section['rows'] = result['table'].get('rows', [])
                    logging.info(f"Populated table for '{subchapter_key}'.")
                else:
                    logging.warning(f"Could not populate table for '{subchapter_key}' as target section is not a dict.")


    def _populate_chapter_4(self, report: dict, stage_data: dict) -> None:
        """Populates Chapter 4 (Prüfplan) content into the report."""
        chapter_4_target = report.get('bsiAuditReport', {}).get('erstellungEinesPruefplans', {}).get('auditplanung')
        if not chapter_4_target:
            logging.error("Report template is missing '...erstellungEinesPruefplans.auditplanung' structure. Cannot populate Chapter 4.")
            return
            
        ch4_plan_key = next(iter(stage_data)) if stage_data else None
        if not ch4_plan_key: return

        result = stage_data.get(ch4_plan_key, {})
        target_key_map = {"auswahlBausteineUeberwachung": "auswahlBausteineErstRezertifizierung"}
        target_key = target_key_map.get(ch4_plan_key, ch4_plan_key)

        target_section = chapter_4_target.get(target_key)
        if isinstance(target_section, dict):
            target_section['rows'] = result.get('rows', [])
        else:
            logging.warning(f"Could not find or invalid target section for '{ch4_plan_key}' (mapped to '{target_key}') in Chapter 4.")

    def _populate_chapter_5(self, report: dict, stage_data: dict) -> None:
        """Populates Chapter 5 (Vor-Ort-Audit) content into the report."""
        chapter_5_target = report.get('bsiAuditReport', {}).get('vorOrtAudit')
        if not chapter_5_target:
            logging.error("Report template is missing 'bsiAuditReport.vorOrtAudit' structure. Cannot populate Chapter 5.")
            return

        for subchapter_key, result in stage_data.items():
            if not isinstance(result, dict): continue

            if subchapter_key == "verifikationDesITGrundschutzChecks":
                target_section_wrapper = chapter_5_target.get(subchapter_key, {})
                target_section = target_section_wrapper.get("einzelergebnisse")
                if isinstance(target_section, dict):
                    target_section["bausteinPruefungen"] = result.get("bausteinPruefungen", [])
                else:
                    logging.warning(f"Could not find target structure 'einzelergebnisse' for '{subchapter_key}'")
            
            elif subchapter_key == "einzelergebnisseDerRisikoanalyse":
                target_section_wrapper = chapter_5_target.get("risikoanalyseA5", {})
                target_section = target_section_wrapper.get(subchapter_key)
                if isinstance(target_section, dict):
                    target_section["massnahmenPruefungen"] = result.get("massnahmenPruefungen", [])
                else: logging.warning(f"Could not find target structure for '{subchapter_key}'")

    def _populate_chapter_7(self, report: dict, stage_data: dict) -> None:
        """Populates Chapter 7 (Anhang) content into the report."""
        anhang_target = report.get('bsiAuditReport', {}).get('anhang')
        if not anhang_target:
            logging.error("Report template is missing 'bsiAuditReport.anhang' structure. Cannot populate Chapter 7.")
            return

        # Populate reference documents table
        ref_docs_data = stage_data.get('referenzdokumente', {})
        target_section = anhang_target.get('referenzdokumente')
        if isinstance(target_section, dict) and isinstance(ref_docs_data.get('table'), dict):
            target_table = target_section.get('table')
            if isinstance(target_table, dict):
                target_table['rows'] = ref_docs_data['table'].get('rows', [])
            else:
                 logging.warning("Could not populate 'referenzdokumente' because target 'table' is not a dict.")
        else:
            logging.warning("Could not populate 'referenzdokumente' due to missing or invalid structure.")

    def _populate_report(self, report: dict, stage_name: str, stage_data: dict) -> None:
        """Router function to call the correct population logic for a given stage."""
        logging.info(f"Populating report with data from stage: {stage_name}")
        population_map = {
            "Chapter-1": self._populate_chapter_1,
            "Chapter-3": self._populate_chapter_3,
            "Chapter-4": self._populate_chapter_4,
            "Chapter-5": self._populate_chapter_5,
            "Chapter-7": self._populate_chapter_7,
        }
        
        populate_func = population_map.get(stage_name)
        if populate_func:
            populate_func(report, stage_data)
        else:
            logging.warning(f"No population logic defined for stage: {stage_name}")
==== bsi-audit-automator/src/audit/stages/control_catalog.py ====
# src/audit/stages/control_catalog.py
import logging
import json
from typing import List, Dict, Any

class ControlCatalog:
    """A utility to load and query the BSI Grundschutz OSCAL catalog."""
    
    def __init__(self, catalog_path: str = "assets/json/BSI_GS_OSCAL_current_2023_benutzerdefinierte.json"):
        self.catalog_path = catalog_path
        self._baustein_map = {}
        try:
            self._load_and_parse_catalog()
            logging.info(f"Successfully loaded and parsed BSI Control Catalog from {catalog_path}.")
        except Exception as e:
            logging.error(f"Failed to initialize ControlCatalog: {e}", exc_info=True)
            raise

    def _load_and_parse_catalog(self):
        """Loads the JSON catalog and builds an efficient lookup map."""
        with open(self.catalog_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        catalog = data.get("catalog", {})
        # Layers like 'ISMS', 'ORP', 'INF', etc.
        for layer_group in catalog.get("groups", []):
            # Bausteine within each layer
            for baustein_group in layer_group.get("groups", []):
                baustein_id = baustein_group.get("id")
                if baustein_id:
                    self._baustein_map[baustein_id] = baustein_group.get("controls", [])
    
    def get_controls_for_baustein_id(self, baustein_id: str) -> List[Dict[str, Any]]:
        """
        Retrieves all controls for a given Baustein ID.

        Args:
            baustein_id: The ID of the Baustein (e.g., 'ISMS.1').

        Returns:
            A list of control objects, or an empty list if not found.
        """
        controls = self._baustein_map.get(baustein_id, [])
        if not controls:
            logging.warning(f"No controls found for Baustein ID: {baustein_id}")
        return controls
==== bsi-audit-automator/src/audit/stages/stage_1_general.py ====
# src/audit/stages/stage_1_general.py
import logging
import json
import asyncio
from typing import Dict, Any

from src.config import AppConfig
from src.clients.ai_client import AiClient
from src.clients.rag_client import RagClient

class Chapter1Runner:
    """Handles generating content for Chapter 1, with 1.4 being a manual placeholder."""
    STAGE_NAME = "Chapter-1"

    def __init__(self, config: AppConfig, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.ai_client = ai_client
        self.rag_client = rag_client
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    def _load_asset_text(self, path: str) -> str:
        with open(path, 'r', encoding='utf-8') as f: return f.read()

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f: return json.load(f)

    async def _process_geltungsbereich(self) -> Dict[str, Any]:
        """Handles 1.2 Geltungsbereich and 1.4 Informationsverbund using a filtered RAG query."""
        logging.info("Processing 1.2 Geltungsbereich and 1.4 Informationsverbund...")
        query = "Name, Umfang und Abgrenzung des Informationsverbunds, betroffene Geschäftsprozesse, Standorte und Anwendungen. Kurzbezeichnung und Kurzbeschreibung des Informationsverbunds."
        
        # **FIX**: Apply category filtering to narrow the search to relevant documents.
        context = self.rag_client.get_context_for_query(
            query,
            source_categories=['Informationsverbund', 'Strukturanalyse']
        )
        
        if "No relevant context found" in context:
            logging.warning("No RAG context found for Geltungsbereich. Generating deterministic response.")
            return {
                "kurzbezeichnung": "Nicht ermittelt",
                "kurzbeschreibung": "Nicht ermittelt",
                "description": "Der Geltungsbereich des Informationsverbunds konnte aus den bereitgestellten Dokumenten nicht eindeutig ermittelt werden. Dies muss manuell geklärt und dokumentiert werden.",
                "finding": {
                    "category": "AS",
                    "description": "Die Abgrenzung des Geltungsbereichs ist unklar, da keine Dokumente gefunden wurden, die diesen beschreiben. Dies ist eine schwerwiegende Abweichung, die vor dem Audit geklärt werden muss."
                }
            }
            
        prompt_template = self._load_asset_text("assets/prompts/stage_1_2_geltungsbereich.txt")
        schema = self._load_asset_json("assets/schemas/stage_1_2_geltungsbereich_schema.json")
        
        prompt = prompt_template.format(context=context)
        return await self.ai_client.generate_json_response(prompt, schema)

    async def run(self) -> dict:
        """Executes the generation logic for Chapter 1."""
        logging.info(f"Executing stage: {self.STAGE_NAME}")
        
        geltungsbereich_result = await self._process_geltungsbereich()

        final_result = {
            "verfasser": {
                "name": "Dixie"
            },
            "geltungsbereichDerZertifizierung": geltungsbereich_result,
            "auditierteInstitution": {},
            "grundlageDesAudits": {},
            "audittyp": {
                "content": self.config.audit_type
            },
            "auditplan": {}
        }

        logging.info(f"Successfully generated data for stage {self.STAGE_NAME}")
        return final_result
==== bsi-audit-automator/src/audit/stages/stage_3_dokumentenpruefung.py ====
# src/audit/stages/stage_3_dokumentenpruefung.py
import logging
import json
import asyncio
from typing import Dict, Any, List, Tuple

from src.config import AppConfig
from src.clients.ai_client import AiClient
from src.clients.rag_client import RagClient

class Chapter3Runner:
    """
    Handles generating content for Chapter 3 "Dokumentenprüfung" by dynamically
    parsing the master report template.
    """
    STAGE_NAME = "Chapter-3"
    TEMPLATE_PATH = "assets/json/master_report_template.json"

    # NEW: Static map for RAG metadata not present in the template.
    # This keeps the template clean while providing necessary context.
    _RAG_METADATA_MAP = {
        "aktualitaetDerReferenzdokumente": {"source_categories": ["Grundschutz-Check", "Organisations-Richtlinie"]},
        "sicherheitsleitlinieUndRichtlinienInA0": {"source_categories": ["Sicherheitsleitlinie", "Organisations-Richtlinie"]},
        "definitionDesInformationsverbundes": {"source_categories": ["Informationsverbund", "Strukturanalyse"]},
        "bereinigterNetzplan": {"source_categories": ["Netzplan", "Strukturanalyse"]},
        "listeDerGeschaeftsprozesse": {"source_categories": ["Strukturanalyse"]},
        "listeDerAnwendungen": {"source_categories": ["Strukturanalyse"]},
        "listeDerItSysteme": {"source_categories": ["Strukturanalyse"]},
        "listeDerRaeumeGebaeudeStandorte": {"source_categories": ["Strukturanalyse"]},
        "listeDerKommunikationsverbindungen": {"source_categories": ["Strukturanalyse"]},
        "stichprobenDokuStrukturanalyse": {"rag_query": "Erstelle eine Stichprobendokumentation der Strukturanalyse.", "source_categories": ["Strukturanalyse"]},
        "listeDerDienstleister": {"source_categories": ["Strukturanalyse", "Dienstleister-Liste"]},
        "definitionDerSchutzbedarfskategorien": {"source_categories": ["Schutzbedarfsfeststellung"]},
        "schutzbedarfGeschaeftsprozesse": {"source_categories": ["Schutzbedarfsfeststellung"]},
        "schutzbedarfAnwendungen": {"source_categories": ["Schutzbedarfsfeststellung"]},
        "schutzbedarfItSysteme": {"source_categories": ["Schutzbedarfsfeststellung"]},
        "schutzbedarfRaeume": {"source_categories": ["Schutzbedarfsfeststellung"]},
        "schutzbedarfKommunikationsverbindungen": {"source_categories": ["Schutzbedarfsfeststellung"]},
        "stichprobenDokuSchutzbedarf": {"rag_query": "Führe eine Stichprobenprüfung der Schutzbedarfsfeststellung durch.", "source_categories": ["Strukturanalyse", "Schutzbedarfsfeststellung"]},
        "modellierungsdetails": {"source_categories": ["Modellierung", "Grundschutz-Check"]},
        "detailsZumItGrundschutzCheck": {"source_categories": ["Grundschutz-Check", "Realisierungsplan"]},
        "benutzerdefinierteBausteine": {"source_categories": ["Grundschutz-Check", "Modellierung"]},
        "risikoanalyse": {"source_categories": ["Risikoanalyse"]},
        "realisierungsplan": {"source_categories": ["Realisierungsplan"]},
    }

    def __init__(self, config: AppConfig, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.ai_client = ai_client
        self.rag_client = rag_client
        # NEW: The execution plan is now built dynamically from the template
        self.execution_plan = self._build_execution_plan_from_template()
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME} with dynamic execution plan.")

    def _load_asset_text(self, path: str) -> str:
        with open(path, 'r', encoding='utf-8') as f: return f.read()

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f: return json.load(f)

    def _build_execution_plan_from_template(self) -> List[Dict[str, Any]]:
        """
        Parses the master_report_template.json to build a dynamic list of
        tasks for Chapter 3, establishing the template as the single source of truth.
        """
        plan = []
        template = self._load_asset_json(self.TEMPLATE_PATH)
        ch3_template = template.get("bsiAuditReport", {}).get("dokumentenpruefung", {})

        for subchapter_key, subchapter_data in ch3_template.items():
            if not isinstance(subchapter_data, dict): continue
            
            for section_key, section_data in subchapter_data.items():
                if not isinstance(section_data, dict): continue

                task = self._create_task_from_section(section_key, section_data)
                if task:
                    plan.append(task)
        return plan

    def _create_task_from_section(self, key: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """Creates a single task dictionary for the execution plan."""
        content = data.get("content", [])
        if not content: return None

        task = {"key": key, "type": "rag"} # Default to RAG task

        # Check for summary tasks
        if any("Votum" in item.get("label", "") for item in content if item.get("type") == "prose"):
            task["type"] = "summary"
            task["prompt_path"] = "assets/prompts/generic_summary_prompt.txt"
            task["schema_path"] = "assets/schemas/generic_summary_schema.json"
            task["summary_topic"] = data.get("title", key)
            return task

        # Process RAG tasks
        questions = [item["questionText"] for item in content if item.get("type") == "question"]
        if not questions: return None
            
        task["questions"] = "\n".join(f"{i+1}. {q}" for i, q in enumerate(questions))
        task["prompt_path"] = "assets/prompts/generic_question_prompt.txt"
        
        # Determine schema based on number of questions
        num_questions = len(questions)
        if num_questions == 1: task["schema_path"] = "assets/schemas/generic_1_question_schema.json"
        elif num_questions == 2: task["schema_path"] = "assets/schemas/generic_2_question_schema.json"
        elif num_questions == 4: task["schema_path"] = "assets/schemas/stage_3_7_risikoanalyse_schema.json" # Special case
        elif num_questions == 5: task["schema_path"] = "assets/schemas/stage_3_6_1_grundschutz_check_schema.json" # Special case
        else: task["schema_path"] = "assets/schemas/stage_3_2_sicherheitsleitlinie_schema.json" # Fallback for 3 questions
        
        # Add RAG metadata
        metadata = self._RAG_METADATA_MAP.get(key, {})
        # **BUGFIX**: RAG query is now all questions combined for better context.
        task["rag_query"] = metadata.get("rag_query", " ".join(questions))
        task["source_categories"] = metadata.get("source_categories")

        # Handle special table-generating prompts
        if "stichproben" in key.lower():
            task["prompt_path"] = f"assets/prompts/stage_3_{data['subchapterNumber'].replace('.', '_')}_{key}.txt"
            task["schema_path"] = f"assets/schemas/stage_3_{data['subchapterNumber'].replace('.', '_')}_{key}_schema.json"

        return task

    async def _process_rag_subchapter(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """Generates content for a single RAG-based subchapter."""
        key = task["key"]
        logging.info(f"Starting RAG generation for subchapter: {key}")
        
        prompt_template_str = self._load_asset_text(task["prompt_path"])
        schema = self._load_asset_json(task["schema_path"])

        context_evidence = self.rag_client.get_context_for_query(
            query=task["rag_query"],
            source_categories=task.get("source_categories")
        )
        
        format_args = {"context": context_evidence}
        if "questions" in task:
            format_args["questions"] = task["questions"]
        prompt = prompt_template_str.format(**format_args)

        try:
            generated_data = await self.ai_client.generate_json_response(prompt, schema)
            return {key: generated_data}
        except Exception as e:
            logging.error(f"Failed to generate data for subchapter {key}: {e}", exc_info=True)
            return {key: {"error": str(e)}}

    async def _process_summary_subchapter(self, task: Dict[str, Any], previous_findings: str) -> Dict[str, Any]:
        """Generates a summary/verdict for a subchapter."""
        key = task["key"]
        logging.info(f"Starting summary generation for subchapter: {key}")

        prompt_template_str = self._load_asset_text(task["prompt_path"])
        schema = self._load_asset_json(task["schema_path"])
        
        prompt = prompt_template_str.format(
            summary_topic=task["summary_topic"],
            previous_findings=previous_findings
        )

        try:
            generated_data = await self.ai_client.generate_json_response(prompt, schema)
            return {key: generated_data}
        except Exception as e:
            logging.error(f"Failed to generate summary for subchapter {key}: {e}", exc_info=True)
            return {key: {"error": str(e)}}

    def _get_findings_from_results(self, results_list: List[Dict]) -> str:
        """Extracts and formats findings from a list of results for summary prompts."""
        findings_for_summary = []
        for res_dict in results_list:
            if not res_dict: continue
            key = list(res_dict.keys())[0]
            result_data = res_dict.get(key)
            
            if isinstance(result_data, dict) and isinstance(result_data.get('finding'), dict):
                finding = result_data['finding']
                category = finding.get('category')
                description = finding.get('description')
                if category and category != "OK":
                    findings_for_summary.append(f"- Finding from {key} [{category}]: {description}")
        
        return "\n".join(findings_for_summary) if findings_for_summary else "No specific findings or deviations were generated."

    async def run(self) -> dict:
        """
        Executes the dynamically generated plan for Chapter 3.
        """
        logging.info(f"Executing dynamically generated plan for stage: {self.STAGE_NAME}")
        
        aggregated_results = {}
        all_findings_text = ""
        
        rag_tasks = [task for task in self.execution_plan if task.get("type") == "rag"]
        summary_tasks = [task for task in self.execution_plan if task.get("type") == "summary"]

        if rag_tasks:
            logging.info(f"--- Processing {len(rag_tasks)} RAG subchapters ---")
            rag_coroutines = [self._process_rag_subchapter(task) for task in rag_tasks]
            rag_results_list = await asyncio.gather(*rag_coroutines)
            
            for res in rag_results_list:
                aggregated_results.update(res)
            
            all_findings_text = self._get_findings_from_results(rag_results_list)

        if summary_tasks:
            logging.info(f"--- Processing {len(summary_tasks)} summary subchapters ---")
            summary_coroutines = [self._process_summary_subchapter(task, all_findings_text) for task in summary_tasks]
            summary_results_list = await asyncio.gather(*summary_coroutines)

            for res in summary_results_list:
                aggregated_results.update(res)

        logging.info(f"Successfully aggregated results for all of stage {self.STAGE_NAME}")
        return aggregated_results
==== bsi-audit-automator/src/audit/stages/stage_4_pruefplan.py ====
# src/audit/stages/stage_4_pruefplan.py
import logging
import json
import asyncio
from typing import Dict, Any

from src.config import AppConfig
from src.clients.ai_client import AiClient

class Chapter4Runner:
    """
    Handles generating the audit plan for Chapter 4 "Erstellung eines Prüfplans".
    It processes each subchapter as a separate, parallel planning task.
    """
    STAGE_NAME = "Chapter-4"

    def __init__(self, config: AppConfig, ai_client: AiClient):
        self.config = config
        self.ai_client = ai_client
        # Definitions are now loaded dynamically based on audit type
        self.subchapter_definitions = self._load_subchapter_definitions()
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    def _load_asset_text(self, path: str) -> str:
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _load_subchapter_definitions(self) -> Dict[str, Any]:
        """
        Loads definitions for subchapters based on the configured audit type.
        This implements conditional logic for different audit types.
        """
        logging.info(f"Loading Chapter 4 definitions for audit type: {self.config.audit_type}")
        
        if self.config.audit_type == "Zertifizierungsaudit":
            return {
                "auswahlBausteineErstRezertifizierung": {
                    "key": "4.1.1",
                    "prompt_path": "assets/prompts/stage_4_1_1_auswahl_bausteine_erst.txt",
                    "schema_path": "assets/schemas/stage_4_1_1_auswahl_bausteine_erst_schema.json"
                }
            }
        elif self.config.audit_type == "Überwachungsaudit":
            return {
                "auswahlBausteineUeberwachung": {
                    "key": "4.1.2",
                    "prompt_path": "assets/prompts/stage_4_1_2_auswahl_bausteine_ueberwachung.txt",
                    "schema_path": "assets/schemas/stage_4_1_2_auswahl_bausteine_ueberwachung_schema.json"
                }
            }
        else:
            logging.warning(f"Unknown audit type '{self.config.audit_type}'. No Chapter 4 definitions loaded.")
            return {}

    async def _process_single_subchapter(self, name: str, definition: dict) -> Dict[str, Any]:
        """Generates planning content for a single subchapter."""
        logging.info(f"Starting planning for subchapter: {definition['key']} ({name})")
        
        prompt_template = self._load_asset_text(definition["prompt_path"])
        schema = self._load_asset_json(definition["schema_path"])
        
        # BUGFIX: Removed format call for customer_id, as it's no longer a config parameter.
        # The prompts have been updated to not require it.
        prompt = prompt_template

        try:
            generated_data = await self.ai_client.generate_json_response(prompt, schema)
            logging.info(f"Successfully generated plan for subchapter {definition['key']}")
            return {name: generated_data}
        except Exception as e:
            logging.error(f"Failed to generate plan for subchapter {definition['key']}: {e}", exc_info=True)
            return {name: None}

    async def run(self) -> dict:
        """
        Executes the planning logic for all of Chapter 4 in parallel.
        
        Returns:
            A dictionary aggregating the results of all subchapter plans.
        """
        logging.info(f"Executing stage: {self.STAGE_NAME}")

        if not self.subchapter_definitions:
            logging.warning(f"No subchapter definitions found for audit type '{self.config.audit_type}'. Skipping Chapter 4.")
            return {}

        tasks = []
        for name, definition in self.subchapter_definitions.items():
            tasks.append(self._process_single_subchapter(name, definition))
        
        results_list = await asyncio.gather(*tasks)

        aggregated_results = {}
        for res_dict in results_list:
            aggregated_results.update(res_dict)
            
        logging.info(f"Successfully aggregated planning results for stage {self.STAGE_NAME}")
        return aggregated_results
==== bsi-audit-automator/src/audit/stages/stage_5_vor_ort_audit.py ====
# src/audit/stages/stage_5_vor_ort_audit.py
import logging
import json
from typing import Dict, Any

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient
from src.audit.stages.control_catalog import ControlCatalog

class Chapter5Runner:
    """
    Handles generating content for Chapter 5 "Vor-Ort-Audit".
    It deterministically prepares the control checklist for the manual audit.
    """
    STAGE_NAME = "Chapter-5"

    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self.control_catalog = ControlCatalog()
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    def _generate_control_checklist(self, chapter_4_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Deterministically generates the control checklist for subchapter 5.5.2.
        This list is for the human auditor to use during the on-site audit.
        """
        name = "verifikationDesITGrundschutzChecks"
        logging.info(f"Deterministically generating control checklist for {name} (5.5.2)...")

        # 1. Get selected Bausteine from Chapter 4 results, handling different audit types.
        ch4_plan_key = next(iter(chapter_4_data)) if chapter_4_data else None
        selected_bausteine = chapter_4_data.get(ch4_plan_key, {}).get("rows", []) if ch4_plan_key else []
        
        if not selected_bausteine:
            logging.warning("No Bausteine found in Chapter 4 results. Checklist for 5.5.2 will be empty.")
            return {name: {"bausteinPruefungen": []}}

        # 2. For each Baustein, get its controls and build the structured checklist.
        baustein_pruefungen_list = []
        for baustein in selected_bausteine:
            baustein_id_full = baustein.get("Baustein", "")
            if not baustein_id_full:
                continue

            baustein_id = baustein_id_full.split(" ")[0]
            controls = self.control_catalog.get_controls_for_baustein_id(baustein_id)

            anforderungen_list = []
            for control in controls:
                anforderungen_list.append({
                    "nummer": control.get("id", "N/A"),
                    "anforderung": control.get("title", "N/A"),
                    "bewertung": "",  # Placeholder for the auditor's manual input
                    "auditfeststellung": "",  # Placeholder for the auditor's manual input
                    "abweichungen": ""  # Placeholder for the auditor's manual input
                })
            
            baustein_pruefungen_list.append({
                "baustein": baustein_id_full,
                "zielobjekt": baustein.get("Zielobjekt", ""),
                "anforderungen": anforderungen_list
            })

        logging.info(f"Successfully generated checklist with {len(baustein_pruefungen_list)} Bausteine for manual audit.")
        return {name: {"bausteinPruefungen": baustein_pruefungen_list}}
        
    def _generate_risikoanalyse_checklist(self) -> Dict[str, Any]:
        """
        Generates a placeholder checklist for the risk analysis measures (5.6.2).
        The actual measures would need to be extracted from A.5/A.6 in a future feature.
        """
        name = "einzelergebnisseDerRisikoanalyse"
        logging.info(f"Generating placeholder checklist for {name} (5.6.2)...")
        # For now, this returns a placeholder structure.
        # Future implementation would use RAG to read A.5/A.6 and populate this.
        return {
            name: {
                "massnahmenPruefungen": []
            }
        }


    async def run(self) -> dict:
        """
        Executes the generation logic for Chapter 5.
        """
        logging.info(f"Executing stage: {self.STAGE_NAME}")
        
        # This stage depends on the audit plan from Chapter 4.
        try:
            ch4_results_path = f"{self.config.output_prefix}results/Chapter-4.json"
            chapter_4_data = self.gcs_client.read_json(ch4_results_path)
            logging.info("Successfully loaded dependency: Chapter 4 results.")
        except Exception as e:
            logging.error(f"Could not load Chapter 4 results, which are required for Chapter 5. Aborting stage. Error: {e}")
            raise
        
        # Generate the checklist for 5.5.2 (deterministic)
        checklist_result = self._generate_control_checklist(chapter_4_data)
        
        # Generate the placeholder for 5.6.2 (deterministic placeholder)
        risiko_result = self._generate_risikoanalyse_checklist()
        
        # Merge results
        final_result = {**checklist_result, **risiko_result}
            
        logging.info(f"Successfully prepared data for stage {self.STAGE_NAME}")
        return final_result
==== bsi-audit-automator/src/audit/stages/stage_7_anhang.py ====
# src/audit/stages/stage_7_anhang.py
import logging
import json
from typing import Dict, Any

from src.config import AppConfig
from src.clients.gcs_client import GcsClient

class Chapter7Runner:
    """
    Handles generating the appendix for Chapter 7.
    - 7.1 is generated deterministically by listing GCS source files.
    - 7.2 (Deviations) is populated by the ReportGenerator from the central findings file.
    """
    STAGE_NAME = "Chapter-7"

    def __init__(self, config: AppConfig, gcs_client: GcsClient):
        self.config = config
        self.gcs_client = gcs_client
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME}")

    async def _generate_referenzdokumente_table(self) -> Dict[str, Any]:
        """Generates the table of reference documents by listing source files in GCS."""
        logging.info("Generating subchapter 7.1 (Referenzdokumente) from GCS file list.")
        try:
            source_files = self.gcs_client.list_files()
            rows = []
            for i, blob in enumerate(source_files):
                rows.append({
                    "Nr.": f"A.{i}",
                    "Kurzbezeichnung": blob.name.split('/')[-1],
                    "Dateiname / Verweis": blob.name,
                    "Version, Datum": blob.updated.strftime("%Y-%m-%d") if blob.updated else "N/A",
                    "Relevante Änderungen": "Initial eingereicht für Audit."
                })
            # The key must match the structure in master_report_template.json
            return {"referenzdokumente": {"table": {"rows": rows}}}
        except Exception as e:
            logging.error(f"Failed to generate Referenzdokumente table: {e}", exc_info=True)
            return {"referenzdokumente": {"table": {"rows": []}}}

    async def run(self) -> dict:
        """Executes the generation logic for Chapter 7."""
        logging.info(f"Executing stage: {self.STAGE_NAME}")
        # Only one task remains for this chapter.
        result = await self._generate_referenzdokumente_table()
        logging.info(f"Successfully generated data for stage {self.STAGE_NAME}")
        return result
==== bsi-audit-automator/src/clients/ai_client.py ====
# src/clients/ai_client.py
import logging
import json
import asyncio
import time
from typing import List, Dict, Any, Tuple

from google.cloud import aiplatform
from vertexai.language_models import TextEmbeddingModel
from google.api_core import exceptions as api_core_exceptions
from vertexai.generative_models import GenerativeModel, GenerationConfig


from src.config import AppConfig

# Constants for the AI client, aligned with the project brief.
GENERATIVE_MODEL_NAME = "gemini-2.5-pro"
EMBEDDING_MODEL_NAME = "gemini-embedding-001"

# Constants for robust generation
MAX_RETRIES = 5


class AiClient:
    """A client for all Vertex AI model interactions, using the aiplatform SDK."""

    def __init__(self, config: AppConfig):
        """Initializes the Vertex AI client and required models."""
        self.config = config

        # Initialize the AI Platform SDK client. The 'location' parameter is the
        # correct way to specify the region for API calls.
        aiplatform.init(
            project=config.gcp_project_id,
            location=config.vertex_ai_region
        )

        # Instantiate specific model clients using the correct classes
        self.generative_model = GenerativeModel(GENERATIVE_MODEL_NAME)
        self.embedding_model = TextEmbeddingModel.from_pretrained(EMBEDDING_MODEL_NAME)

        self.semaphore = asyncio.Semaphore(config.max_concurrent_ai_requests)
        
        logging.info(
            f"Vertex AI Client instantiated for project '{config.gcp_project_id}' in region '{config.vertex_ai_region}'."
        )

    def get_embeddings(self, texts: List[str]) -> Tuple[bool, List[List[float]]]:
        """
        Generates vector embeddings for a list of text chunks. Implements a
        robust retry mechanism.

        Args:
            texts: A list of text strings to embed.

        Returns:
            A tuple containing (success: bool, embeddings: list).
        """
        if not texts:
            logging.warning("get_embeddings called with no texts. Returning empty list.")
            return True, []

        all_embeddings = []
        logging.info(f"Generating embeddings for {len(texts)} chunks...")
        
        # We must iterate and call the API for each text individually with its own retry logic.
        for i, text in enumerate(texts):
            for attempt in range(MAX_RETRIES):
                try:
                    # The model expects a list, even if it's a single item.
                    response = self.embedding_model.get_embeddings([text])
                    all_embeddings.append(response[0].values)

                    # Log progress every 25 chunks at INFO level
                    if (i + 1) % 25 == 0:
                        logging.info(f"Embedding progress: {i + 1}/{len(texts)} chunks complete.")

                    # Keep the client-side rate limit workaround
                    time.sleep(0.1)
                    break  # Success, break the retry loop for this chunk
                except api_core_exceptions.GoogleAPICallError as e:
                    if e.code == 429:  # HTTP status for "Too Many Requests"
                        wait_time = 2 ** attempt
                        logging.warning(f"Embedding for chunk {i+1} hit rate limit. Retrying in {wait_time}s...")
                        time.sleep(wait_time) # Use synchronous sleep
                    else:
                        logging.error(f"Embedding for chunk {i+1} failed with API Error: {e}", exc_info=True)
                        raise # Re-raise other API errors immediately
                
                if attempt == MAX_RETRIES - 1:
                    logging.critical(f"Embedding for chunk {i+1} failed after all retries.")
                    return False, all_embeddings # Return failure status and partial results
        
        logging.info(f"Successfully generated {len(all_embeddings)} embeddings.")
        return True, all_embeddings

    async def generate_json_response(self, prompt: str, json_schema: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generates a JSON response from the AI model, enforcing a specific schema.
        Implements an async retry loop with exponential backoff and connection limiting.
        """
        # --- BUG FIX ---
        # "Launder" the schema by serializing and deserializing it. This ensures that
        # we pass a pure, clean data structure to the Google SDK, avoiding a
        # recurring internal TypeError when it parses complex nested schemas.
        try:
            schema_for_api = json.loads(json.dumps(json_schema))
            schema_for_api.pop("$schema", None)
        except Exception as e:
            logging.error(f"Failed to process JSON schema before API call: {e}")
            raise ValueError("Invalid JSON schema provided.") from e


        gen_config = GenerationConfig(
            response_mime_type="application/json",
            response_schema=schema_for_api,
            max_output_tokens=8192,
            temperature=0.2,
        )

        async with self.semaphore:
            for attempt in range(MAX_RETRIES):
                try:
                    logging.info(f"Attempt {attempt + 1}/{MAX_RETRIES}: Calling Gemini model '{GENERATIVE_MODEL_NAME}'...")

                    response = await self.generative_model.generate_content_async(
                        contents=[prompt],
                        generation_config=gen_config,
                    )

                    if not response.candidates:
                        raise ValueError("The model response contained no candidates.")

                    finish_reason = response.candidates[0].finish_reason.name
                    if finish_reason not in ["STOP", "MAX_TOKENS"]:
                        safety_ratings_str = "N/A"
                        if response.candidates[0].safety_ratings:
                            safety_ratings_str = ", ".join([
                                f"{rating.category.name}: {rating.probability.name}"
                                for rating in response.candidates[0].safety_ratings
                            ])
                        logging.warning(
                            f"Attempt {attempt + 1} finished with non-OK reason: '{finish_reason}'. "
                            f"Safety Ratings: [{safety_ratings_str}]. Retrying..."
                        )
                        await asyncio.sleep(2 ** attempt)
                        continue

                    response_json = json.loads(response.text)
                    logging.info(f"Successfully generated and parsed JSON response on attempt {attempt + 1}.")
                    return response_json

                except api_core_exceptions.GoogleAPICallError as e:
                    if e.code == 429:
                        logging.warning(
                            f"Attempt {attempt + 1} hit rate limit (429). "
                            f"Retrying in {2 ** attempt}s..."
                        )
                    else:
                        logging.error(
                            f"Attempt {attempt + 1} failed with Google API Error (Code: {e.code}). Retrying...",
                            exc_info=self.config.is_test_mode
                        )
                except Exception as e:
                    logging.error(f"Attempt {attempt + 1} failed with a non-API exception. Retrying...", exc_info=True)

                if attempt == MAX_RETRIES - 1:
                    logging.critical("AI generation failed after all retries.", exc_info=True)
                    raise
                
                await asyncio.sleep(2 ** attempt)
        
        raise RuntimeError("AI generation failed after all retries without raising a final exception.")
==== bsi-audit-automator/src/clients/gcs_client.py ====
# src/clients/gcs_client.py
import logging
from google.cloud import storage
from src.config import AppConfig

class GcsClient:
    """A client for all Google Cloud Storage interactions."""

    def __init__(self, config: AppConfig):
        """
        Initializes the GCS client.

        Args:
            config: The application configuration object.
        """
        self.config = config
        self.storage_client = storage.Client(project=config.gcp_project_id)
        # We derive the bucket name from the config, which should be set by an env var
        # that comes from the terraform output.
        if not config.bucket_name:
            raise ValueError("GCS Bucket name is not configured in the environment.")
        self.bucket = self.storage_client.bucket(config.bucket_name)
        logging.info(f"GCS Client initialized for bucket: gs://{self.bucket.name}")

    def list_files(self, prefix: str = None) -> list[storage.Blob]:
        """
        Lists all files from a given GCS prefix.

        Returns:
            A list of GCS blob objects.
        """
        list_prefix = prefix if prefix is not None else self.config.source_prefix
        logging.info(f"Listing files from prefix: {list_prefix}")
        blobs = self.storage_client.list_blobs(
            self.bucket.name, prefix=list_prefix
        )
        # Filter for common document types, ignore empty "directory" blobs
        files = [blob for blob in blobs if "." in blob.name]
        logging.info(f"Found {len(files)} source files to process.")
        return files

    def download_blob_as_bytes(self, blob: storage.Blob) -> bytes:
        """Downloads a blob from GCS into memory as bytes."""
        logging.debug(f"Downloading blob: {blob.name}")
        return blob.download_as_bytes()

    def upload_from_string(self, content: str, destination_blob_name: str, content_type: str = 'application/json'):
        """
        Uploads a string content to a specified blob in GCS.

        Args:
            content: The string content to upload.
            destination_blob_name: The full path for the object in the bucket.
            content_type: The MIME type of the content.
        """
        logging.info(f"Uploading content to gs://{self.bucket.name}/{destination_blob_name}")
        blob = self.bucket.blob(destination_blob_name)
        blob.upload_from_string(content, content_type=content_type)
        logging.info("Upload complete.")

    def read_json(self, blob_name: str) -> dict:
        """Downloads and parses a JSON file from GCS."""
        import json
        logging.info(f"Attempting to read JSON from: gs://{self.bucket.name}/{blob_name}")
        blob = self.bucket.blob(blob_name)
        content = blob.download_as_text() # This raises NotFound if not present.
        return json.loads(content)

    def read_text_file(self, blob_name: str) -> str:
        """Downloads and returns the content of a text-based file from GCS."""
        logging.info(f"Attempting to read text from: gs://{self.bucket.name}/{blob_name}")
        blob = self.bucket.blob(blob_name)
        return blob.download_as_text()

    def blob_exists(self, blob_name: str) -> bool:
        """Checks if a blob exists in the GCS bucket."""
        logging.debug(f"Checking for existence of blob: gs://{self.bucket.name}/{blob_name}")
        blob = self.bucket.blob(blob_name)
        return blob.exists()
==== bsi-audit-automator/src/clients/rag_client.py ====
# src/clients/rag_client.py
import logging
import json
from typing import List, Dict, Any

from google.cloud import aiplatform
from google.cloud.aiplatform.matching_engine import MatchingEngineIndexEndpoint
from google.cloud.exceptions import NotFound

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient

DOC_MAP_PATH = "output/document_map.json"
# **NEW**: Constants for dynamic context filtering
SIMILARITY_THRESHOLD = 0.7  # Lower is stricter. Cosine distance of 0.7 is a reasonable starting point.
NEIGHBOR_POOL_SIZE = 10     # Fetch a larger pool of candidates to filter from.


class RagClient:
    """Client for Retrieval-Augmented Generation using Vertex AI Vector Search."""

    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client

        aiplatform.init(
            project=config.gcp_project_id,
            location=config.vertex_ai_region
        )

        if config.index_endpoint_public_domain:
            logging.info(f"Connecting to PUBLIC Vector Search Endpoint: {config.index_endpoint_public_domain}")
            self.index_endpoint = MatchingEngineIndexEndpoint.from_public_endpoint(
                project_id=config.gcp_project_id,
                region=config.vertex_ai_region,
                public_endpoint_domain_name=config.index_endpoint_public_domain
            )
        else:
            logging.info(f"Connecting to PRIVATE Vector Search Endpoint: {config.index_endpoint_id}")
            self.index_endpoint = MatchingEngineIndexEndpoint(
                index_endpoint_name=self.config.index_endpoint_id,
            )

        self._chunk_lookup_map = self._load_chunk_lookup_map()
        self._document_category_map = self._load_document_category_map()

    def _load_document_category_map(self) -> Dict[str, str]:
        """
        Loads the document classification map from GCS. The map provides a
        lookup from document category to a list of filenames.
        """
        logging.info(f"Loading document category map from '{DOC_MAP_PATH}'...")
        category_map = {}
        try:
            map_data = self.gcs_client.read_json(DOC_MAP_PATH)
            doc_map_list = map_data.get("document_map", [])
            for item in doc_map_list:
                category = item.get("category")
                filename = item.get("filename")
                if category and filename:
                    if category not in category_map:
                        category_map[category] = []
                    category_map[category].append(filename)
            
            logging.info(f"Successfully built document category map with {len(category_map)} categories.")
            return category_map
        except NotFound:
            logging.error(f"CRITICAL: Document map file not found at '{DOC_MAP_PATH}'. The ETL process must be run first.")
            raise
        except Exception as e:
            logging.error(f"Failed to build document category map: {e}", exc_info=True)
            return {}

    def _load_chunk_lookup_map(self) -> Dict[str, Dict[str, str]]:
        """
        Downloads all embedding batch files from GCS and creates a mapping from
        chunk ID to its text content and source document for fast lookups.
        """
        lookup_map: Dict[str, Dict[str, str]] = {}
        logging.info("Building chunk ID to text lookup map from all batch files...")

        try:
            embedding_blobs = self.gcs_client.list_files(prefix="vector_index_data/")

            for blob in embedding_blobs:
                if "placeholder.json" in blob.name:
                    continue

                jsonl_content = self.gcs_client.read_text_file(blob.name)
                
                for line in jsonl_content.strip().split('\n'):
                    try:
                        data = json.loads(line)
                        chunk_id = data.get("id")
                        chunk_text = data.get("text_content")
                        source_doc = data.get("source_document")
                        if chunk_id and chunk_text and source_doc:
                            lookup_map[chunk_id] = {
                                "text_content": chunk_text,
                                "source_document": source_doc
                            }
                    except json.JSONDecodeError:
                        logging.warning(f"Skipping invalid JSON line in {blob.name}: '{line}'")
                        continue
            
            logging.info(f"Successfully built lookup map with {len(lookup_map)} entries.")
            return lookup_map
        except Exception as e:
            logging.error(f"Failed to build chunk lookup map from batch files: {e}", exc_info=True)
            return {}

    def get_context_for_query(self, query: str, source_categories: List[str] = None) -> str:
        """
        Finds relevant document chunks for a query, dynamically filtering them by
        similarity score for the highest quality context.
        """
        if self.config.is_test_mode:
            logging.info(f"RAG_CLIENT_TEST_MODE: Sending query to vector DB: '{query}'")

        context_str = ""
        try:
            success, embeddings = self.ai_client.get_embeddings([query])
            if not success or not embeddings:
                logging.error("Failed to generate embedding for the RAG query.")
                return "Error: Could not generate embedding for query."
            
            query_vector = embeddings[0]

            filters = None
            if source_categories and self._document_category_map:
                allow_list_filenames = []
                for category in source_categories:
                    filenames = self._document_category_map.get(category, [])
                    allow_list_filenames.extend(filenames)
                
                if allow_list_filenames:
                    logging.info(f"Applying search filter for categories: {source_categories} ({len(allow_list_filenames)} files)")
                    # FIX: Use the correct dictionary format for restrictions, not the invalid class path.
                    # The API expects a dictionary with 'namespace' and 'allow' keys.
                    filters = {"namespace": "source_document", "allow": allow_list_filenames}

                else:
                    logging.warning(f"No documents found for categories: {source_categories}. Searching all documents.")

            response = self.index_endpoint.find_neighbors(
                deployed_index_id="bsi_deployed_index_kunde_x",
                queries=[query_vector],
                num_neighbors=NEIGHBOR_POOL_SIZE,
                filter=[filters] if filters else []
            )

            if response and response[0]:
                all_neighbors = response[0]
                
                # **NEW**: Filter the retrieved neighbors by their distance score.
                # A lower distance means higher similarity.
                quality_neighbors = [n for n in all_neighbors if n.distance <= SIMILARITY_THRESHOLD]
                
                logging.info(f"Filtering {len(all_neighbors)} neighbors down to {len(quality_neighbors)} by similarity score (<= {SIMILARITY_THRESHOLD}).")

                if not quality_neighbors:
                    logging.warning("No neighbors met the similarity threshold.")
                    return "No highly relevant context found in the documents."

                for neighbor in quality_neighbors:
                    chunk_id = neighbor.id
                    context_info = self._chunk_lookup_map.get(chunk_id)
                    if context_info:
                        context_str += f"-- CONTEXT FROM DOCUMENT: {context_info['source_document']} (Similarity: {1-neighbor.distance:.2%}) --\n"
                        context_str += f"{context_info['text_content']}\n\n"
                    else:
                        logging.warning(f"Could not find text for chunk ID: {chunk_id}")
                
                return context_str
            else:
                logging.warning("Vector DB query returned no neighbors.")
                return "No relevant context found in the documents."
                
        except Exception as e:
            logging.error(f"Error querying Vector DB: {e}", exc_info=True)
            return "Error retrieving context from Vector DB."
==== bsi-audit-automator/src/config.py ====
# src/config.py
import os
from dataclasses import dataclass
from typing import Optional
from dotenv import load_dotenv

@dataclass(frozen=True)
class AppConfig:
    """
    Dataclass to hold all application configuration. It's frozen to prevent
    accidental modification after initialization.
    """
    gcp_project_id: str
    source_prefix: str
    output_prefix: str
    etl_status_prefix: str
    audit_type: str
    vertex_ai_region: str
    index_endpoint_id: str
    max_concurrent_ai_requests: int
    is_test_mode: bool
    # This will be populated by the GCS Client after Terraform runs
    bucket_name: Optional[str] = None 
    # Optional because it's not needed for the ETL step
    project_number: Optional[str] = None
    # Optional: for local dev against a public endpoint
    index_endpoint_public_domain: Optional[str] = None

def load_config_from_env() -> AppConfig:
    """
    Loads configuration from environment variables, validates them,
    and returns a frozen AppConfig dataclass.

    Raises:
        ValueError: If a required environment variable is missing.

    Returns:
        AppConfig: The validated application configuration.
    """
    # Load .env file for local development. In a cloud environment, these
    # will be set directly.
    load_dotenv()

    required_vars = [
        "GCP_PROJECT_ID", "SOURCE_PREFIX", "OUTPUT_PREFIX", "ETL_STATUS_PREFIX",
        "AUDIT_TYPE", "VERTEX_AI_REGION", "INDEX_ENDPOINT_ID"
    ]
    
    config_values = {}
    for var in required_vars:
        value = os.getenv(var)
        if not value:
            raise ValueError(f"Configuration Error: Missing required environment variable: {var}")
        # Convert to lowercase to match dataclass fields
        config_values[var.lower()] = value

    # Handle optional and boolean variables
    config_values["project_number"] = os.getenv("GCP_PROJECT_NUMBER")
    config_values["is_test_mode"] = os.getenv("TEST", "false").lower() == "true"
    config_values["bucket_name"] = os.getenv("BUCKET_NAME")
    config_values["index_endpoint_public_domain"] = os.getenv("INDEX_ENDPOINT_PUBLIC_DOMAIN")
    
    # Load the new concurrency limit, defaulting to 5 if not set or invalid
    max_reqs_str = os.getenv("MAX_CONCURRENT_AI_REQUESTS", "5")
    config_values["max_concurrent_ai_requests"] = int(max_reqs_str) if max_reqs_str.isdigit() else 5

    return AppConfig(**config_values)

# Create a singleton instance to be imported by other modules.
# The try/except block ensures the application exits gracefully if config is invalid.
try:
    config = load_config_from_env()
except ValueError as e:
    print(e)
    exit(1)
==== bsi-audit-automator/src/etl/__init__.py ====
# This file intentionally left blank to mark the 'etl' directory as a Python package.
==== bsi-audit-automator/src/etl/processor.py ====
# src/etl/processor.py
import logging
import json
import uuid
import re
from typing import List, Dict, Any
import fitz  # PyMuPDF

from langchain.text_splitter import RecursiveCharacterTextSplitter

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient

# Constants for ETL processing
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
MAX_FILES_TEST_MODE = 3
VECTOR_INDEX_PREFIX = "vector_index_data/"
DOC_MAP_PATH = "output/document_map.json"

class EtlProcessor:
    """
    Extracts text from source documents, chunks it, generates embeddings,
    and uploads the formatted output for each document as a separate JSON
    file for Vertex AI Vector Search indexing. This process is idempotent,
    tracking completion status in GCS.
    """

    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
        )
        logging.info("ETL Processor initialized.")

    def _load_asset_text(self, path: str) -> str:
        with open(path, 'r', encoding='utf-8') as f: return f.read()

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f: return json.load(f)

    async def _classify_source_documents(self, filenames: List[str]) -> None:
        """
        Uses an AI model to classify source documents into predefined BSI categories
        based on their filenames. Saves the result to a map file in GCS.
        """
        logging.info("Starting AI-driven document classification based on filenames.")

        # Check if the map already exists to avoid reprocessing
        if self.gcs_client.blob_exists(DOC_MAP_PATH):
            logging.info(f"Document map already exists at '{DOC_MAP_PATH}'. Skipping classification.")
            return

        prompt_template = self._load_asset_text("assets/prompts/etl_classify_documents.txt")
        schema = self._load_asset_json("assets/schemas/etl_classify_documents_schema.json")
        
        # Format the list of filenames as a JSON string for the prompt
        filenames_json = json.dumps(filenames, indent=2)
        prompt = prompt_template.format(filenames_json=filenames_json)

        try:
            classification_result = await self.ai_client.generate_json_response(prompt, schema)
            
            # The result is the entire object, we just need to serialize it.
            self.gcs_client.upload_from_string(
                content=json.dumps(classification_result, indent=2, ensure_ascii=False),
                destination_blob_name=DOC_MAP_PATH
            )
            logging.info(f"Successfully created and saved document map to '{DOC_MAP_PATH}'.")
        except Exception as e:
            logging.error(f"Failed to classify source documents: {e}", exc_info=True)
            # We raise here because the map is critical for subsequent stages.
            raise

    def _extract_text_from_pdf(self, pdf_bytes: bytes, source_filename: str) -> str:
        """
        Extracts text content from a PDF file provided as bytes.

        Args:
            pdf_bytes: The byte content of the PDF file.
            source_filename: The original name of the file for logging purposes.

        Returns:
            The extracted text content as a single string.
        """
        text_content = ""
        try:
            with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
                for page_num, page in enumerate(doc):
                    text_content += f"--- Page {page_num + 1} of {source_filename} ---\n"
                    text_content += page.get_text() + "\n\n"
            return text_content
        except Exception as e:
            logging.error(f"Failed to extract text from {source_filename}: {e}", exc_info=True)
            return ""

    def _sanitize_filename(self, filename: str) -> str: 
        """
        Removes special characters from a filename to create a valid GCS object name.

        Args:
            filename: The original filename, which may contain paths or special chars.

        Returns:
            A sanitized string suitable for use as a GCS object name.
        """
        # Get base name after the last '/'
        base_name = filename.split('/')[-1]
        # Replace invalid chars with underscores
        return re.sub(r'[^a-zA-Z0-9_.-]', '_', base_name)

    def _get_status_blob_path_base(self, source_blob_name: str) -> str:
        """
        Constructs the base path for the status marker blob, without an extension.

        Args:
            source_blob_name: The full name of the source document blob.

        Returns:
            The GCS path prefix for the corresponding status marker file.
        """
        sanitized_name = self._sanitize_filename(source_blob_name)
        return f"{self.config.etl_status_prefix}{sanitized_name}"

    def _process_single_document(self, blob: Any) -> None:
        """
        Runs the full ETL pipeline for a single source document from GCS.

        Args:
            blob: The GCS blob object representing the source document.
        """
        logging.info(f"Processing document: {blob.name}")
        status_blob_base = self._get_status_blob_path_base(blob.name)

        # 1. Extract
        file_bytes = self.gcs_client.download_blob_as_bytes(blob)
        if blob.name.lower().endswith(".pdf"):
            document_text = self._extract_text_from_pdf(file_bytes, blob.name)
        else:
            logging.warning(f"Skipping non-PDF file: {blob.name}")
            return
        
        if not document_text:
            logging.warning(f"No text extracted from {blob.name}. Skipping.")
            raise ValueError("No text could be extracted from document.")

        # 2. Chunk
        chunks = self.text_splitter.split_text(document_text)
        if not chunks:
            logging.warning(f"No chunks created for {blob.name}. Skipping.")
            raise ValueError("Document text could not be split into chunks.")
        logging.info(f"Created {len(chunks)} chunks for {blob.name}.")

        # 3. Embed
        success, embeddings = self.ai_client.get_embeddings(chunks)
        if not success or len(embeddings) != len(chunks):
            logging.error(f"Embedding generation failed for {blob.name}. Skipping document.")
            raise RuntimeError("Embedding generation failed.")

        # 4. Format and Upload
        logging.info(f"Formatting data for {blob.name}...")
        jsonl_content = ""
        for i, embedding_vector in enumerate(embeddings):
            record = {
                "id": str(uuid.uuid4()),
                "embedding": embedding_vector,
                "text_content": chunks[i],
                # The 'source_document' field must contain the full blob name for filtering.
                "source_document": blob.name
            }
            jsonl_content += json.dumps(record) + "\n"
        
        output_filename = self._sanitize_filename(blob.name) + ".json"
        output_path = f"{VECTOR_INDEX_PREFIX}{output_filename}"
        
        self.gcs_client.upload_from_string(
            content=jsonl_content,
            destination_blob_name=output_path,
            content_type='application/json'
        )
        # Upon success, create the status marker file.
        self.gcs_client.upload_from_string(
            content="",
            destination_blob_name=f"{status_blob_base}.success",
            content_type='text/plain'
        )

        logging.info(f"Successfully uploaded embedding data for {blob.name} to gs://{self.config.bucket_name}/{output_path}")

    async def run(self) -> None:
        """
        Executes the main ETL pipeline. It first classifies all documents, then
        processes each new document, checking its status to ensure idempotency.
        """
        logging.info("Starting ETL run...")
        source_files = self.gcs_client.list_files()
        
        if not source_files:
            logging.warning("No source files found. ETL run is complete with no output.")
            return
            
        # Extract just the filenames for the classification step
        source_filenames = [blob.name for blob in source_files]
        
        # **NEW**: Run classification step first.
        await self._classify_source_documents(source_filenames)

        if self.config.is_test_mode:
            logging.warning(f"TEST MODE: Processing only the first {MAX_FILES_TEST_MODE} files for embedding.")
            source_files = source_files[:MAX_FILES_TEST_MODE]

        for blob in source_files:
            status_blob_base = self._get_status_blob_path_base(blob.name)
            success_marker = f"{status_blob_base}.success"
            failed_marker = f"{status_blob_base}.failed"
            
            if self.gcs_client.blob_exists(success_marker):
                logging.info(f"'.success' marker found for {blob.name}. Skipping.")
                continue
            
            if self.gcs_client.blob_exists(failed_marker):
                logging.warning(f"'.failed' marker found for {blob.name}. Skipping to prevent repeated errors.")
                continue

            try:
                self._process_single_document(blob)
            except Exception as e:
                logging.error(f"An unexpected error occurred while processing {blob.name}. Creating '.failed' marker. Error: {e}", exc_info=True)
                self.gcs_client.upload_from_string(
                    content=str(e),
                    destination_blob_name=failed_marker,
                    content_type='text/plain'
                )
        
        logging.info("ETL run finished.")
==== bsi-audit-automator/src/logging_setup.py ====
# src/logging_setup.py
import logging
import sys
from src.config import AppConfig

def setup_logging(config: AppConfig):
    """
    Sets up the root logger based on the execution mode from the config.

    Args:
        config: The application configuration object.
    """
    # In test mode, we want detailed logs at INFO level.
    # In production, we want high-level INFO, with details at DEBUG.
    log_level = logging.INFO if config.is_test_mode else logging.DEBUG
    
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        stream=sys.stdout,
    )

    # In production, set the root logger to INFO to see high-level status,
    # while our application-specific logs can be at the DEBUG level.
    if not config.is_test_mode:
        logging.getLogger().setLevel(logging.INFO)

        # Suppress noisy third-party library logs for cleaner production output
        logging.getLogger("google.auth").setLevel(logging.WARNING)
        logging.getLogger("google.api_core").setLevel(logging.WARNING)
        logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)
        logging.info("Production logging enabled. Set root to INFO, app logs to DEBUG, and suppressed noisy libs.")
    else:
        logging.info("Test mode logging enabled. All INFO logs will be visible.")
==== bsi-audit-automator/src/main.py ====
# src/main.py
import argparse
import logging
import asyncio

from .config import config
from .logging_setup import setup_logging
from .clients.gcs_client import GcsClient
from .clients.rag_client import RagClient
from .clients.ai_client import AiClient
from .etl.processor import EtlProcessor
from .audit.controller import AuditController
from .audit.report_generator import ReportGenerator

EMBEDDINGS_PATH_PREFIX = "vector_index_data/"

async def main_async():
    """
    Asynchronous main function to handle all pipeline operations.
    """
    setup_logging(config)

    parser = argparse.ArgumentParser(
        description="BSI Grundschutz Audit Automation Pipeline."
    )
    group = parser.add_mutually_exclusive_group(required=True)
    
    group.add_argument(
        '--run-etl',
        action='store_true',
        help='Run the ETL phase: Chunk, embed, and upload documents to GCS for indexing.'
    )
    group.add_argument(
        '--run-stage',
        type=str,
        help='Run a single audit stage (e.g., --run-stage Chapter-1).'
    )
    group.add_argument(
        '--run-all-stages',
        action='store_true',
        help='Run all audit generation stages sequentially.'
    )
    group.add_argument(
        '--generate-report',
        action='store_true',
        help='Assemble the final report from completed stage stubs.'
    )

    parser.add_argument(
        '--force',
        action='store_true',
        help='Force re-running of stages that have already completed. Only applies to --run-all-stages.'
    )

    args = parser.parse_args()

    gcs_client = GcsClient(config)
    ai_client = AiClient(config)

    if args.run_etl:
        logging.info("Starting ETL phase...")
        etl_processor = EtlProcessor(config, gcs_client, ai_client)
        # **FIX**: The async run method must be awaited.
        await etl_processor.run()

    elif args.generate_report:
        logging.info("Starting final report assembly...")
        generator = ReportGenerator(config, gcs_client)
        generator.assemble_report()

    else:  # These are the async audit tasks
        rag_dependent_tasks = args.run_stage or args.run_all_stages
        if rag_dependent_tasks:
            logging.info(f"Checking for required ETL output files in: {EMBEDDINGS_PATH_PREFIX}")
            embedding_files = [b for b in gcs_client.list_files(prefix=EMBEDDINGS_PATH_PREFIX) if "placeholder.json" not in b.name]
            
            if not embedding_files:
                logging.critical(
                    f"\n\n--- PREREQUISITE MISSING ---\n"
                    f"No embedding files found in '{EMBEDDINGS_PATH_PREFIX}' in bucket '{config.bucket_name}'.\n"
                    f"You must run the ETL process first to generate embeddings for the source documents.\n"
                    f"Please run: python -m src.main --run-etl\n"
                )
                exit(1)

        rag_client = RagClient(config, gcs_client, ai_client)
        controller = AuditController(config, gcs_client, ai_client, rag_client)

        if args.run_stage:
            await controller.run_single_stage(args.run_stage, force_overwrite=True)
        elif args.run_all_stages:
            await controller.run_all_stages(force_overwrite=args.force)

def main():
    """
    Main entry point for the BSI Audit Automator.
    Parses command-line arguments and runs the appropriate async task.
    """
    try:
        asyncio.run(main_async())
        logging.info("Pipeline step completed successfully.")
    except Exception as e:
        logging.critical(f"A critical error occurred in the pipeline: {e}", exc_info=True)
        exit(1)

if __name__ == "__main__":
    main()
==== code_architecture.md ====
### **Code Architecture (Revised)**

The application is designed to be **modular, scalable, and auditable**, separating concerns into distinct clients, processors, and controllers. This architecture promotes clarity and maintainability.

```
bsi-audit-automator/
│
├── src/
│   ├── clients/
│   │   ├── gcs_client.py       # Handles all Google Cloud Storage interactions.
│   │   ├── rag_client.py       # Handles Vector Search (RAG) queries and context retrieval.
│   │   └── ai_client.py        # Handles all Vertex AI Gemini API interactions (generation & embedding).
│   │
│   ├── etl/
│   │   └── processor.py        # The idempotent ETL processor for the RAG pipeline.
│   │
│   ├── audit/
│   │   ├── controller.py       # Orchestrates all audit stages, manages state, and collects findings.
│   │   ├── report_generator.py # Assembles the final report from stage outputs and collected findings.
│   │   └── stages/             # Contains the specific business logic for each audit stage/chapter.
│   │       ├── control_catalog.py # Helper class to load and parse BSI controls from the OSCAL JSON file.
│   │       ├── stage_1_general.py
│   │       ├── stage_3_dokumentenpruefung.py
│   │       ├── stage_4_pruefplan.py
│   │       ├── stage_5_vor_ort_audit.py
│   │       └── stage_7_anhang.py
│   │
│   ├── config.py               # Loads, validates, and provides application configuration from env variables.
│   ├── logging_setup.py        # Configures application-wide logging.
│   └── main.py                 # Main entry point with CLI argument parsing.
│
├── assets/                     # External, non-code assets for the AI.
│   ├── json/                   # JSON Schemas, BSI OSCAL catalog, and the master report template.
│   └── prompts/                # .txt prompt templates with placeholders for context.
│
└── ... (Other project files: Dockerfile, requirements.txt, scripts/, terraform/, etc.)
```

**Module Descriptions:**

*   **`src/main.py`**: The application's entry point. It parses command-line arguments (`--run-etl`, `--run-all-stages`, etc.) to determine which part of the pipeline to execute. It also performs a critical pre-flight check to ensure ETL has been run before any RAG-dependent audit stage.
*   **`src/etl/processor.py`**: Contains the logic for the **Extract, Transform, Load (ETL)** process. It finds source documents in GCS, extracts text, chunks it, generates vector embeddings via `ai_client`, and uploads the data as per-document JSONL files to GCS. It is idempotent, using `.success`/`.failed` markers to track processed files and ensure resilience.
*   **`src/clients/`**: This directory contains thin clients responsible for communicating with external GCP services, encapsulating all API-specific logic.
    *   **`gcs_client.py`**: Handles all Google Cloud Storage I/O, including listing files, reading/writing text and JSON, and checking for blob existence.
    *   **`ai_client.py`**: A robust wrapper for the Gemini API. It handles model configuration (`gemini-2.5-pro`, `gemini-embedding-001`), asynchronous parallel requests limited by a semaphore, automatic retries with exponential backoff, and schema-enforced JSON generation.
    *   **`rag_client.py`**: Connects to the Vertex AI Vector Search endpoint. Its primary method, `get_context_for_query`, encapsulates the RAG pattern: it takes a text query, uses the `ai_client` to embed it, queries the vector index for similar document chunks, and retrieves the full text for those chunks to be used as prompt context.
*   **`src/audit/controller.py`**: The main orchestrator of the audit. It defines the sequence of stages to run, manages resumability by checking for existing results in GCS, and acts as the **central collector for all audit findings**. It inspects the results of each stage for structured `finding` objects and appends them to a master list, which is saved at the end of the run.
*   **`src/audit/stages/`**: Each module in this directory contains the business logic for a specific chapter of the audit.
    *   `control_catalog.py`: A helper utility that loads the `BSI_GS_OSCAL...json` file and provides a simple method to retrieve all controls for a given Baustein ID. It is used for deterministic checklist generation.
    *   The `stage_X_...py` modules define the RAG queries, select the appropriate prompts and schemas from `/assets`, call the `ai_client` for generation, and return the structured result. Some stages are deterministic (like Chapter 5's checklist generation) or conditional (like Chapter 4's plan generation based on `AUDIT_TYPE`).
*   **`src/audit/report_generator.py`**: This module is responsible for the final, deterministic assembly of the report. It populates the `master_report_template.json` by merging in the JSON results from each stage file and, critically, by reading the `all_findings.json` file to populate the categorized tables in Chapter 7.2.
```

==== create_dirs.sh ====
#!/bin/bash
# A script to create the BSI Audit Automator project structure.

echo "Creating project structure for bsi-audit-automator..."

# Create the root directory and navigate into it
mkdir -p bsi-audit-automator
cd bsi-audit-automator

# Create top-level files and directories
touch main.py requirements.txt
mkdir -p src tests assets

# Create src subdirectories
mkdir -p src/clients src/audit src/audit/stages

# Create files in src
touch src/__init__.py
touch src/config.py
touch src/logging_setup.py

# Create files in src/clients
touch src/clients/__init__.py
touch src/clients/gcs_client.py
touch src/clients/ai_client.py

# Create files in src/audit and src/audit/stages
touch src/audit/__init__.py
touch src/audit/controller.py
touch src/audit/report_generator.py
touch src/audit/stages/__init__.py
touch src/audit/stages/stage_1_general.py
touch src/audit/stages/stage_3_document_review.py

# Create assets subdirectories and placeholder files
mkdir -p assets/prompts assets/schemas
touch assets/prompts/initial_extraction.txt
touch assets/prompts/stage_3_1_actuality.txt
touch assets/schemas/initial_extraction_schema.json
touch assets/schemas/stage_3_1_actuality_schema.json

# Create placeholder test file
touch tests/test_placeholder.py

echo "Project structure created successfully in the 'bsi-audit-automator' directory."
==== patch.diff ====
--- a/bsi-audit-automator/src/clients/rag_client.py
+++ b/bsi-audit-automator/src/clients/rag_client.py
@@ -136,9 +136,10 @@
                 
                 if allow_list_filenames:
                     logging.info(f"Applying search filter for categories: {source_categories} ({len(allow_list_filenames)} files)")
-                    filters = aiplatform.IndexDatapoint.Restriction(
-                        namespace="source_document", allow_list=allow_list_filenames
-                    )
+                    # FIX: Use the correct dictionary format for restrictions, not the invalid class path.
+                    # The API expects a dictionary with 'namespace' and 'allow' keys.
+                    filters = {"namespace": "source_document", "allow": allow_list_filenames}
+
                 else:
                     logging.warning(f"No documents found for categories: {source_categories}. Searching all documents.")
 
==== project_outline.md ====
### **Project Brief: BSI Grundschutz Audit Automation with Vertex AI (Revised)**

This document outlines the requirements and development protocol for a Python-based application designed to automate BSI Grundschutz audits using the Vertex AI Gemini API.

**1. Project Overview & Objective**

The primary goal is to develop a cloud-native application that performs a security audit based on the German BSI Grundschutz framework. The application will run as a batch job on Google Cloud Platform (GCP), processing customer-provided documents against BSI standards and generating a structured audit report.

The core of the application is a **Retrieval-Augmented Generation (RAG)** pipeline. Source documents are first indexed into a Vertex AI Vector Search database. Then, for each section of the audit, the application retrieves the most relevant document excerpts to provide as context to the Gemini model, ensuring accurate, evidence-based findings.

The audit process and resulting report must be based on two key documents:
*   The relevant **BSI Grundschutz Standards** (BSI 200-1, BSI 200-2 and BSI 200-3 and the BSI Grundschutz Kompendium 2023 with its Auditierungsschema and Zertifizierungsschem).
*   The **`bsi-audit-automator/assets/json/master_report_template.json`** file, which serves as the structural and content template for the final audit report.

**2. Core Functional Requirements**

*   **Idempotent ETL Process:** A resilient ETL job processes source documents, creates vector embeddings, and uploads them for indexing. It uses status markers (`.success`, `.failed`) in GCS to prevent reprocessing and ensure robustness.
*   **Staged Audit Process:** The audit is conducted in discrete stages corresponding to the report chapters. The application supports running all stages or single stages. Some sections (e.g., 1.4, 5.1) are intentionally placeholders for manual auditor input.
*   **State Management & Resumability:** The application saves the results of each stage to Google Cloud Storage (GCS). For full audit runs (`--run-all-stages`), it checks for and skips previously completed stages. For single-stage runs, it overwrites existing results by default.
*   **Centralized Finding Collection:** The application systematically collects all structured findings (deviations and recommendations with categories 'AG', 'AS', 'E') from all stages into a central `all_findings.json` file.
*   **Audit Type Configuration:** The application is configurable for "Überwachungsaudit" or "Zertifizierungsaudit" via an environment variable, which drives different logic in the audit planning stage (Chapter 4).
*   **Deterministic and AI-Driven Logic:** The pipeline intelligently combines AI-driven analysis (e.g., Chapter 3 document review) with deterministic, rule-based logic (e.g., Chapter 5 control checklist generation from the BSI catalog).
*   **Comprehensive Reporting:** The final output is a comprehensive JSON audit report, populated from individual stage results and the central findings file, ready for review and finalization in the `report_editor.html` tool.

**3. Gemini Model and API Interaction**
*   **Model Configuration (Imperative):**
    *   **Generative Model:** `gemini-2.5-pro`
    *   **Embedding Model:** `gemini-embedding-001` (for `3072` dimension vectors, as configured in Terraform).
*   **Robustness:** All API calls use an asynchronous, parallel-limited (`Semaphore`), and robust error-handling wrapper with an exponential backoff retry loop.

**4. AI Collaboration & Development Protocol**

**4.1. Communication Protocol**
*   **Commit Message Format:** Start every response with a summary formatted as follows:
    `Case: (A brief summary of my request, word as if cases was speaking)`
    `---`
    `Dixie: (A brief summary of your solution and key details)`
*   **How to test this change:** CLI or similar to test.
*   **Explain Your Reasoning:** Briefly explain the "why" behind your code and architectural decisions.
*   **Track Changes:** For minor code changes (under 20 lines), present them in a `diff` format. For larger changes, provide the full file content.
*   **No Silent Changes:** Never alter code or logic without explicitly stating the change. **Only return files that have been changed.**
*   **Statefulness and Conciseness:** You are expected to be stateful. Review the existing project context, especially `tasks_for_improvement_after_MVP-1.md`, before formulating a response. If your analysis reveals no new bugs, required refactorings, or actionable improvements beyond what is already listed, you MUST respond concisely with the message: `Dixie: I have analyzed the current project state and have no new code changes or tasks to recommend at this time.` Do not re-list or re-phrase existing TODOs.

**4.2. Code Quality and Development Standards**
*   **Modularity and Single Responsibility:** The existing architecture (clients, processors, controllers, stages) must be maintained. Each module and class should have a single, well-defined purpose. For example, `GcsClient` should only ever contain GCS-related logic.
*   **Clarity and Naming:** Code should be as self-documenting as possible. Use descriptive, unambiguous names for variables, functions, and classes (e.g., `_process_single_document` is clearer than `_process`).
*   **Docstrings and Comments:**
    *   All public modules, classes, and functions MUST have Python docstrings explaining their purpose, arguments (`Args:`), and return values (`Returns:`).
    *   Use inline comments not to explain *what* the code is doing (which should be obvious from the code itself), but to explain *why* a particular implementation choice was made, especially for complex logic, business rules, or workarounds.
*   **Strict Type Hinting:** All new functions and methods MUST include full type hints for arguments and return values. This is essential for static analysis, readability, and preventing runtime errors.
*   **Consistent Formatting:** Code MUST adhere to the PEP 8 style guide. It is recommended to use an automated formatter like `black` to ensure consistency across the project.
*   **Robust Error Handling:** Avoid broad `except Exception:` clauses. Catch specific, anticipated exceptions and log them with informative messages. The robust retry logic in `AiClient` serves as a model for handling external API calls.

**4.3. Architecture: "Schema-Stub" Generation**
This is a critical architectural pattern for ensuring reliability.
*   **JSON-Based Communication:** All data sent to and received from the Gemini model must be in JSON format. This must be configured in the `generation_config`.
*   **Schema-Driven Prompts:** For each model interaction, generate a JSON schema "stub" that defines the expected output format. This schema should be included in the prompt.
*   **Minimal Data Subsets:** Pre-filter and structure data sent to the model into a minimal, easy-to-understand JSON object.
*   **Schema as a Quality Gate:** Use the generated schemas to validate the model's JSON output. All validation errors must be caught and logged.
*   **Deterministic Assembly:** The Python script is responsible for the final, deterministic assembly of the complete report from the validated JSON stubs generated by the model.

*   **Critical Schema Constraint: No Tuple Validation for Arrays.**
    *   **Problem:** The Vertex AI SDK does **not** support "tuple validation" for arrays, where the `"items"` keyword is an array of different schemas (e.g., `"items": [ { "type": "boolean" }, { "type": "string" } ]`). This format will cause a `TypeError` deep within the SDK's internal parsing logic.
    *   **Solution:** To define an array that can contain elements of different types, you **MUST** use a single schema object for `"items"` that contains an `"anyOf"` block listing the possible types. Use `minItems` and `maxItems` to enforce a fixed array length if required.
    *   **Correct Implementation Example:**
        ```json
        "answers": {
          "type": "array",
          "items": {
            "anyOf": [
              { "type": "boolean" },
              { "type": "string", "format": "date" }
            ]
          },
          "minItems": 4,
          "maxItems": 4
        }
        ```


**4.5. Code and Asset Management**
*   **Externalized Assets:** All prompts must be stored in external `.txt` files and all JSON schemas in external `.json` files. This separation of logic and assets is mandatory.
*   **Customer Data:** The customer documents are located in one directory the following GCS URI: [GCS_DATA_URI]

**4.6. Testing and Logging**
*   **Test Mode (`TEST="true"`):**
    *   Limit processing to a small number of source files (e.g., the first 3).
    *   Within each file, limit the data sent for generation (e.g., 10% of discovered items).
*   **Conditional Logging:**
    *   The root logger level should be `INFO`.
    *   **In Test Mode:** Log detailed, step-by-step messages at the `INFO` level.
    *   **In Production Mode (`TEST="false"`):** Log verbose messages at the `DEBUG` level. Log only high-level status updates at the `INFO` level.
    *   **Suppress Library Noise:** In production, set the logging level for third-party libraries like `google.auth` and `urllib3` to `WARNING` to maintain clean logs.


**4.7. Environment and Configuration**

| Variable | Required? | Source | Description |
| :--- | :---: | :--- | :--- |
| `GCP_PROJECT_ID` | Yes | Terraform | The Google Cloud Project ID. |
| `BUCKET_NAME` | Yes | Terraform | The GCS bucket for all I/O operations. |
| `AUDIT_TYPE` | Yes | User Input | Specifies the audit type (e.g., "Zertifizierungsaudit"). |
| `SOURCE_PREFIX` | Yes | Script | GCS prefix for source documents (e.g., `source_documents/`). |
| `OUTPUT_PREFIX` | Yes | Script | GCS prefix for generated files (e.g., `output/`). |
| `ETL_STATUS_PREFIX`| Yes | Script | GCS prefix for ETL status markers (e.g., `output/etl_status/`). |
| `INDEX_ENDPOINT_ID`| Yes | Terraform | The numeric ID of the deployed Vertex AI Index Endpoint. |
| `TEST` | No | User Input | Set to `"true"` to enable test mode. Defaults to `false`. |
==== readme.md ====
# BSI Grundschutz Audit Automator

This project automates BSI Grundschutz security audits by transforming customer documentation into a structured report using a cloud-native, multi-stage pipeline on Google Cloud. It leverages a Retrieval-Augmented Generation (RAG) pattern with the Vertex AI Gemini API to ensure audit findings are contextually relevant, evidence-based, and accurate.

## End-to-End Workflow

### Managing Audit Data: Refresh vs. Reset
Before starting, choose the correct method to prepare your environment.

#### **Option 1: Fast Refresh (Recommended for Data Updates)**
Use this when you get new source files for an audit and want to start over without touching the cloud infrastructure.

1.  **Run the Refresh Script:** This moves old embedding data to an archive and clears the index. It's much faster than a full reset.
    ```bash
    bash ./scripts/refresh_audit_data.sh
    ```
2.  **Confirm the Action:** Type `y` to proceed.
3.  **Wait for Index Update:** The script triggers an index update. In the GCP Console, wait for the "Dense vector count" of your index to drop to 0. This can take 5-10 minutes.
4.  **Proceed to the Standard Workflow below.**

#### **Option 2: Full Reset (For Infrastructure Changes)**
Use this only if you need to start from a "scorched-earth" state, for example, for a new customer or if you've changed the Terraform configuration for the index itself.

1.  **Run the Reset Script:** This wipes all GCS data AND marks the index resource in Terraform for recreation.
    ```bash
    bash ./scripts/reset_audit.sh
    ```
2.  **Recreate the Index:** Follow the script's instructions to run `terraform apply`.
    ```bash
    cd ../terraform
    terraform apply -auto-approve
    cd ..
    ```
3.  **Proceed to the Standard Workflow below.**

### Standard Workflow

1.  **Infrastructure Deployment:** If this is the very first run, use Terraform in the `terraform/` directory to create the GCS Bucket, VPC Network, and all necessary Vertex AI and IAM resources.
2.  **Upload Customer Documents:** Upload the customer's documentation (PDFs) to the `source_documents/` path in the GCS bucket.
3.  **Deploy the Job Container:** Build and deploy the application container to Cloud Run Jobs.
    ```bash
    # Run from the project root
    bash ./scripts/deploy-audit-job.sh
    ```
4.  **Execute the ETL Job:** Run the ETL task first using the interactive execution script. This is a **mandatory prerequisite** for all other stages. It processes the source documents and populates the Vector Search Index.
    ```bash
    # Select "Run ETL (Embedding)" from the menu
    bash ./scripts/execute-audit-job.sh
    ```
5.  **Wait for Indexing:** After the ETL job uploads the new embedding files, the Vertex AI Index automatically ingests them. This process can take 5-20 minutes. You can monitor the "Dense vector count" on the Matching Engine page in the GCP Console to see when it's ready.
6.  **Execute Audit Stages:** Once the ETL is complete and the index is populated, run the audit stages.
    ```bash
    # To run all stages sequentially, select "Run All Audit Stages"
    bash ./scripts/execute-audit-job.sh
    ```
7.  **Generate the Final Report:** After the stages are complete, this task assembles the final report from all generated components.
    ```bash
    # Select "Generate Final Report" from the menu
    bash ./scripts/execute-audit-job.sh
    ```
8.  **Manual Review and Finalization:** Open the generated `final_audit_report.json` from the `output/` GCS prefix in the `report_editor.html` tool to perform the final manual review and make any necessary adjustments.

## The Audit Stages Explained

*   **Phase 0: ETL (Embedding)** (`etl/processor.py`): The mandatory first step. It begins by classifying all source documents into BSI-specific categories. It then extracts text, chunks it, generates vector embeddings, and uploads the data for the Vector Index to ingest. It's idempotent and robust, using `.success` and `.failed` markers in GCS to skip already processed or failed files.

*   **Chapter 1: General Information** (`audit/stages/stage_1_general.py`): Generates introductory content.
    *   **1.2 (Scope):** Uses RAG to generate a description of the audit scope and a structured finding on its quality.
    *   **1.4 (Audit Team):** Intentionally left as a placeholder for manual input in the report editor.

*   **Chapter 3: Document Review** (`audit/stages/stage_3_dokumentenpruefung.py`): Performs a deep analysis of core documents. Each subchapter (3.1, 3.2, etc.) runs as a parallel, high-precision RAG task, filtering its search to only the relevant document categories. It generates answers to specific questions and a structured finding. A final summary (3.9) is generated based on the findings from the preceding subchapters.

*   **Chapter 4: Audit Plan Creation** (`audit/stages/stage_4_pruefplan.py`): Generates the audit plan. This stage is **conditional** on the `AUDIT_TYPE` environment variable, using different prompts and rules for a "Zertifizierungsaudit" vs. a "Überwachungsaudit".

*   **Chapter 5: On-Site Audit Preparation** (`audit/stages/stage_5_vor_ort_audit.py`): Prepares materials for the human auditor.
    *   **5.1 (ISMS Effectiveness):** Intentionally left as a placeholder for manual input.
    *   **5.5.2 (Control Verification):** This task is **deterministic**. It reads the audit plan from Chapter 4's output, looks up all required controls from the BSI OSCAL catalog (`control_catalog.py`), and generates a structured checklist for the auditor to use on-site. It does **not** use AI.

*   **Chapter 7: Appendix** (`audit/stages/stage_7_anhang.py`): Generates content for the report's appendix.
    *   **7.1 (Reference Documents):** A **deterministic** task that lists all files found in the source GCS folder.
    .
==== report_editor.html ====
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BSI Audit Report Editor</title>
    <style>
        :root {
            --primary-color: #003359;
            --secondary-color: #0073e6;
            --background-color: #f4f7fa;
            --text-color: #333;
            --border-color: #d1d9e6;
            --header-bg: #fff;
            --card-bg: #fff;
            --button-bg: #005cbf;
            --button-hover-bg: #004a99;
            --danger-color: #d93025;
            --danger-hover-color: #a52714;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            background-color: var(--background-color);
            color: var(--text-color);
            margin: 0;
            line-height: 1.6;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        header {
            background-color: var(--header-bg);
            padding: 20px;
            border-bottom: 1px solid var(--border-color);
            display: flex;
            justify-content: space-between;
            align-items: center;
            position: sticky;
            top: 0;
            z-index: 1000;
        }
        header h1 {
            margin: 0;
            color: var(--primary-color);
            font-size: 1.8em;
        }
        #controls { display: flex; align-items: center; gap: 15px; }
        #controls button, #controls input[type="file"]::file-selector-button {
            background-color: var(--button-bg);
            color: white;
            border: none;
            padding: 10px 15px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 1em;
            transition: background-color 0.2s;
        }
        #controls button:hover, #controls input[type="file"]::file-selector-button:hover { background-color: var(--button-hover-bg); }
        #controls button:disabled { background-color: #999; cursor: not-allowed; }
        
        #chapter-filter-buttons { margin-top: 10px; padding-bottom: 10px; display: flex; flex-wrap: wrap; gap: 10px; }
        #chapter-filter-buttons button { background-color: var(--secondary-color); }
        #chapter-filter-buttons button:hover { background-color: var(--primary-color); }

        .chapter {
            background-color: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            margin-top: 25px;
            padding: 25px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
        }
        .chapter h2 {
            color: var(--primary-color);
            border-bottom: 2px solid var(--secondary-color);
            padding-bottom: 10px;
            margin-top: 0;
        }
        .subchapter {
            margin-top: 20px;
            padding-left: 20px;
            border-left: 3px solid #eef;
        }
        h3 { color: var(--primary-color); font-weight: 600; }
        h4 { color: #555; }
        p.description { color: #555; font-style: italic; }
        
        .content-item, .prose-item {
            margin-bottom: 15px;
            padding: 10px;
            border: 1px solid #f0f0f0;
            border-radius: 4px;
        }
        label { display: block; font-weight: bold; margin-bottom: 5px; color: #444; }
        input[type="text"], input[type="date"], select, textarea {
            width: 100%;
            padding: 8px;
            border-radius: 4px;
            border: 1px solid var(--border-color);
            box-sizing: border-box;
            font-family: inherit;
            font-size: 1em;
        }
        textarea { min-height: 80px; resize: vertical; }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 15px;
        }
        th, td {
            border: 1px solid var(--border-color);
            padding: 10px;
            text-align: left;
            vertical-align: top;
        }
        th { background-color: #f2f2f2; font-weight: bold; }
        td input, td select { padding: 4px; width: 100%; box-sizing: border-box;}
        .action-cell { width: 80px; text-align: center; }
        .delete-row-btn, .add-row-btn {
            background-color: var(--danger-color);
            color: white;
            border: none;
            padding: 5px 10px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.9em;
        }
        .delete-row-btn:hover { background-color: var(--danger-hover-color); }
        .add-row-btn {
            background-color: var(--button-bg);
            margin-top: 10px;
        }
        .add-row-btn:hover { background-color: var(--button-hover-bg); }

        fieldset.baustein-pruefung {
            border: 1px solid var(--secondary-color);
            border-radius: 5px;
            padding: 15px;
            margin-top: 20px;
        }
        fieldset.baustein-pruefung legend {
            font-weight: bold;
            color: var(--primary-color);
            padding: 0 10px;
        }

        #report-container { padding-top: 20px; }
        #initial-message { text-align: center; font-size: 1.2em; color: #888; margin-top: 50px; }
    </style>
</head>
<body>

    <header>
        <h1>BSI Audit Report Editor</h1>
        <div id="controls">
            <input type="file" id="json-file-input" accept=".json">
            <button id="export-json-button" disabled>Export to JSON</button>
        </div>
    </header>

    <div class="container">
        <div id="chapter-filter-buttons"></div>
        <div id="report-container">
            <p id="initial-message">Load your <code>final_audit_report.json</code> or <code>master_report_template.json</code> to begin.</p>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const fileInput = document.getElementById('json-file-input');
            const exportButton = document.getElementById('export-json-button');
            const reportContainer = document.getElementById('report-container');
            const filterContainer = document.getElementById('chapter-filter-buttons');
            const initialMessage = document.getElementById('initial-message');

            let reportData = null;

            fileInput.addEventListener('change', handleFileSelect);
            exportButton.addEventListener('click', handleExport);
            
            reportContainer.addEventListener('click', (event) => {
                if (event.target.classList.contains('add-row-btn')) {
                    handleAddRow(event.target);
                }
                if (event.target.classList.contains('delete-row-btn')) {
                    handleDeleteRow(event.target);
                }
            });

            function handleFileSelect(event) {
                const file = event.target.files[0];
                if (!file) return;

                const reader = new FileReader();
                reader.onload = (e) => {
                    try {
                        reportData = JSON.parse(e.target.result);
                        initialMessage.style.display = 'none';
                        renderReport(reportData);
                        createFilterButtons(reportData);
                        exportButton.disabled = false;
                    } catch (error) {
                        alert('Error parsing JSON file: ' + error.message);
                    }
                };
                reader.readAsText(file);
            }
            
            function syncUiToData() {
                if (!reportData) return;
                const inputs = reportContainer.querySelectorAll('[data-path]');
                inputs.forEach(input => {
                    if (input.tagName === 'BUTTON') return;
                    const path = input.dataset.path;
                    let value = input.value;
                    if (input.type === 'select-one') { // Adjusted for boolean selects
                        value = value === 'true' ? true : (value === 'false' ? false : null);
                    }
                    setValueByPath(reportData, path, value);
                });
            }

            function createFilterButtons(data) {
                filterContainer.innerHTML = '';
                const chapters = data?.bsiAuditReport || {};
                const allButton = document.createElement('button');
                allButton.textContent = 'Show All';
                allButton.onclick = () => filterView('all');
                filterContainer.appendChild(allButton);

                Object.keys(chapters).forEach(key => {
                    if (typeof chapters[key] === 'object' && chapters[key]?.title) {
                        const button = document.createElement('button');
                        button.textContent = `${chapters[key].chapterNumber || ''} ${chapters[key].title}`;
                        button.onclick = () => filterView(key);
                        filterContainer.appendChild(button);
                    }
                });
            }

            function filterView(chapterKey) {
                const allChapters = reportContainer.querySelectorAll('.chapter');
                allChapters.forEach(chapter => {
                    if (chapterKey === 'all' || chapter.dataset.chapterKey === chapterKey) {
                        chapter.style.display = 'block';
                    } else {
                        chapter.style.display = 'none';
                    }
                });
            }

            function renderReport(data) {
                const currentScroll = window.scrollY;
                reportContainer.innerHTML = ''; 
                const report = data.bsiAuditReport;
                if (!report) return;

                for (const key in report) {
                    const chapterData = report[key];
                    if (typeof chapterData === 'object' && chapterData !== null) {
                         reportContainer.appendChild(renderChapter(key, chapterData, `bsiAuditReport.${key}`));
                    }
                }
                window.scrollTo(0, currentScroll);
            }
            
            function renderChapter(key, chapterData, path) {
                const chapterDiv = document.createElement('div');
                chapterDiv.className = 'chapter';
                chapterDiv.dataset.chapterKey = key;
                
                if(chapterData.title) chapterDiv.innerHTML += `<h2>${chapterData.chapterNumber || ''} ${chapterData.title}</h2>`;
                if(chapterData.description) chapterDiv.innerHTML += `<p class="description">${chapterData.description}</p>`;

                for (const subKey in chapterData) {
                     // Recurse only on keys that are objects but are not the known structural keys
                    if (typeof chapterData[subKey] === 'object' && chapterData[subKey] !== null) {
                         chapterDiv.appendChild(renderSection(subKey, chapterData[subKey], `${path}.${subKey}`));
                    }
                }
                return chapterDiv;
            }

            function renderSection(key, sectionData, path) {
                const sectionDiv = document.createElement('div');
                sectionDiv.className = 'subchapter';

                if (sectionData.title) sectionDiv.innerHTML += `<h3>${sectionData.subchapterNumber || ''} ${sectionData.title}</h3>`;
                if (sectionData.description) sectionDiv.innerHTML += `<p class="description">${sectionData.description}</p>`;
                
                const handledKeys = ['title', 'description', 'subchapterNumber'];

                if (Array.isArray(sectionData.content)) {
                    sectionData.content.forEach((item, index) => {
                        const itemPath = `${path}.content[${index}]`;
                        const itemDiv = document.createElement('div');
                        itemDiv.className = 'content-item';
                        if (item.type === 'question') {
                            itemDiv.appendChild(createSelect(item.questionText, item.answer, `${itemPath}.answer`));
                        } else if (item.type === 'finding') {
                            itemDiv.appendChild(createTextArea(item.label, item.findingText, `${itemPath}.findingText`));
                        } else if (item.type === 'prose') {
                            itemDiv.appendChild(createTextArea(item.label, item.text, `${itemPath}.text`));
                        }
                        sectionDiv.appendChild(itemDiv);
                    });
                    handledKeys.push('content');
                }
                
                if (sectionData.table || (sectionData.headers && Array.isArray(sectionData.rows))) {
                    sectionDiv.appendChild(renderTable(sectionData.table || sectionData, path));
                    handledKeys.push('table', 'headers', 'rows');
                }
                
                // --- SPECIAL CASE RENDERERS FOR CHAPTER 5 ---
                if (Array.isArray(sectionData.bausteinPruefungen)) {
                    sectionDiv.appendChild(renderBausteinPruefung(sectionData.bausteinPruefungen, `${path}.bausteinPruefungen`));
                    handledKeys.push('bausteinPruefungen');
                }
                 if (Array.isArray(sectionData.massnahmenPruefungen)) {
                    sectionDiv.appendChild(renderMassnahmenPruefung(sectionData.massnahmenPruefungen, `${path}.massnahmenPruefungen`));
                    handledKeys.push('massnahmenPruefungen');
                }
                // --- END SPECIAL CASES ---

                // Recursive call for any other nested objects
                for (const subKey in sectionData) {
                    if (handledKeys.includes(subKey)) continue;

                    if (typeof sectionData[subKey] === 'object' && sectionData[subKey] !== null) {
                        sectionDiv.appendChild(renderSection(subKey, sectionData[subKey], `${path}.${subKey}`));
                    }
                }
                return sectionDiv;
            }

            function createInputField(label, value, type, path) {
                const wrapper = document.createElement('div');
                if (label) wrapper.innerHTML = `<label for="${path}">${label}</label>`;
                const input = document.createElement('input');
                input.type = type || 'text';
                input.value = value || '';
                input.id = path;
                input.dataset.path = path;
                wrapper.appendChild(input);
                return wrapper;
            }

            function createSelect(label, value, path) {
                const wrapper = document.createElement('div');
                if (label) wrapper.innerHTML = `<label for="${path}">${label}</label>`;
                const select = document.createElement('select');
                select.innerHTML = `<option value="true" ${value === true ? 'selected' : ''}>Ja</option><option value="false" ${value === false ? 'selected' : ''}>Nein</option><option value="" ${value === null || value === undefined ? 'selected' : ''}>N/A</option>`;
                select.id = path;
                select.dataset.path = path;
                wrapper.appendChild(select);
                return wrapper;
            }

            function createTextArea(label, value, path) {
                const wrapper = document.createElement('div');
                if (label) wrapper.innerHTML = `<label for="${path}">${label}</label>`;
                const textarea = document.createElement('textarea');
                textarea.id = path;
                textarea.dataset.path = path;
                textarea.value = value || '';
                wrapper.appendChild(textarea);
                return wrapper;
            }

            function renderTable(tableData, path) {
                const tableContainer = document.createElement('div');
                const table = document.createElement('table');
                const thead = table.createTHead();
                const tbody = table.createTBody();
                const tfoot = table.createTFoot();

                const headerRow = thead.insertRow();
                tableData.headers.forEach(headerText => {
                    const th = document.createElement('th');
                    th.textContent = headerText;
                    headerRow.appendChild(th);
                });
                if (path.includes("anhang.abweichungen")) {
                    headerRow.insertCell().className = 'action-cell';
                }

                if (tableData.rows) {
                    tableData.rows.forEach((rowData, rowIndex) => {
                        const row = tbody.insertRow();
                        const rowPath = `${path}.rows[${rowIndex}]`;
                        tableData.headers.forEach(header => {
                            const cell = row.insertCell();
                            cell.appendChild(createInputField(null, rowData[header] || '', 'text', `${rowPath}.${header}`));
                        });
                        if (path.includes("anhang.abweichungen")) {
                            const actionCell = row.insertCell();
                            actionCell.className = 'action-cell';
                            const deleteBtn = document.createElement('button');
                            deleteBtn.textContent = 'Delete';
                            deleteBtn.className = 'delete-row-btn';
                            deleteBtn.dataset.path = `${path}.rows`;
                            deleteBtn.dataset.rowIndex = rowIndex;
                            actionCell.appendChild(deleteBtn);
                        }
                    });
                }
                
                if (path.includes("anhang.abweichungen")) {
                    const footerRow = tfoot.insertRow();
                    const addCell = footerRow.insertCell();
                    addCell.colSpan = tableData.headers.length + 1;
                    const addBtn = document.createElement('button');
                    addBtn.textContent = '+ Add Row';
                    addBtn.className = 'add-row-btn';
                    addBtn.dataset.path = `${path}.rows`;
                    addBtn.dataset.headers = JSON.stringify(tableData.headers);
                    addCell.appendChild(addBtn);
                }
                
                tableContainer.appendChild(table);
                return tableContainer;
            }

            // Special renderer for Chapter 5.5.2
            function renderBausteinPruefung(pruefungen, path) {
                const container = document.createElement('div');
                pruefungen.forEach((pruefung, pIndex) => {
                    const pPath = `${path}[${pIndex}]`;
                    const fieldset = document.createElement('fieldset');
                    fieldset.className = 'baustein-pruefung';
                    
                    const legend = document.createElement('legend');
                    legend.textContent = `Prüfung für Baustein: ${pruefung.baustein}`;
                    fieldset.appendChild(legend);

                    fieldset.appendChild(createInputField('Zielobjekt', pruefung.zielobjekt, 'text', `${pPath}.zielobjekt`));
                    
                    const anforderungenTable = document.createElement('table');
                    anforderungenTable.innerHTML = `<thead><tr><th>Nummer</th><th>Anforderung</th><th>Bewertung</th><th>Auditfeststellung</th><th>Abweichungen</th></tr></thead>`;
                    const tbody = anforderungenTable.createTBody();
                    
                    pruefung.anforderungen.forEach((anf, aIndex) => {
                        const aPath = `${pPath}.anforderungen[${aIndex}]`;
                        const row = tbody.insertRow();
                        row.insertCell().appendChild(createInputField(null, anf.nummer, 'text', `${aPath}.nummer`));
                        row.insertCell().appendChild(createInputField(null, anf.anforderung, 'text', `${aPath}.anforderung`));
                        row.insertCell().appendChild(createInputField(null, anf.bewertung, 'text', `${aPath}.bewertung`));
                        row.insertCell().appendChild(createInputField(null, anf.auditfeststellung, 'text', `${aPath}.auditfeststellung`));
                        row.insertCell().appendChild(createInputField(null, anf.abweichungen, 'text', `${aPath}.abweichungen`));
                    });
                    fieldset.appendChild(anforderungenTable);
                    container.appendChild(fieldset);
                });
                return container;
            }
             // Special renderer for Chapter 5.6.2
            function renderMassnahmenPruefung(pruefungen, path) {
                const container = document.createElement('div');
                const table = document.createElement('table');
                table.innerHTML = `<thead><tr><th>Maßnahme</th><th>Zielobjekt</th><th>Bewertung</th><th>Auditfeststellung</th><th>Abweichungen</th></tr></thead>`;
                const tbody = table.createTBody();
                 pruefungen.forEach((massnahme, mIndex) => {
                    const mPath = `${path}[${mIndex}]`;
                    const row = tbody.insertRow();
                    row.insertCell().appendChild(createInputField(null, massnahme.massnahme, 'text', `${mPath}.massnahme`));
                    row.insertCell().appendChild(createInputField(null, massnahme.zielobjekt, 'text', `${mPath}.zielobjekt`));
                    row.insertCell().appendChild(createInputField(null, massnahme.bewertung, 'text', `${mPath}.bewertung`));
                    row.insertCell().appendChild(createInputField(null, massnahme.auditfeststellung, 'text', `${mPath}.auditfeststellung`));
                    row.insertCell().appendChild(createInputField(null, massnahme.abweichungen, 'text', `${mPath}.abweichungen`));
                });
                container.appendChild(table);
                return container;
            }

            function handleAddRow(button) {
                syncUiToData(); // Save current UI state to data model first
                const path = button.dataset.path;
                const headers = JSON.parse(button.dataset.headers);
                const newRow = {};
                headers.forEach(header => { newRow[header] = ''; });

                const tableRows = getValueByPath(reportData, path);
                if (Array.isArray(tableRows)) {
                    tableRows.push(newRow);
                    renderReport(reportData);
                }
            }

            function handleDeleteRow(button) {
                syncUiToData(); // Save current UI state to data model first
                const path = button.dataset.path;
                const rowIndex = parseInt(button.dataset.rowIndex, 10);

                const tableRows = getValueByPath(reportData, path);
                if (Array.isArray(tableRows) && !isNaN(rowIndex)) {
                    tableRows.splice(rowIndex, 1);
                    renderReport(reportData);
                }
            }

            function handleExport() {
                syncUiToData(); // Ensure all latest changes are in the data model
                if (!reportData) {
                    alert("No data loaded to export.");
                    return;
                }
                const updatedData = JSON.parse(JSON.stringify(reportData)); 
                downloadObjectAsJson(updatedData, 'edited_report.json');
            }
            
            function getValueByPath(obj, path) {
                const keys = path.replace(/\[(\w+)\]/g, '.$1').replace(/^bsiAuditReport\./, '').split('.');
                let current = obj.bsiAuditReport;
                for (let i = 0; i < keys.length; i++) {
                    if (current === undefined || current === null) return undefined;
                    current = current[keys[i]];
                }
                return current;
            }

            function setValueByPath(obj, path, value) {
                const keys = path.replace(/\[(\w+)\]/g, '.$1').replace(/^bsiAuditReport\./, '').split('.');
                let current = obj.bsiAuditReport;
                for (let i = 0; i < keys.length - 1; i++) {
                    if(current === undefined || current[keys[i]] === undefined) return; 
                    current = current[keys[i]];
                }
                if(current && typeof current === 'object') {
                    current[keys[keys.length - 1]] = value;
                }
            }

            function downloadObjectAsJson(exportObj, exportName) {
                const dataStr = "data:text/json;charset=utf-8," + encodeURIComponent(JSON.stringify(exportObj, null, 2));
                const downloadAnchorNode = document.createElement('a');
                downloadAnchorNode.setAttribute("href", dataStr);
                downloadAnchorNode.setAttribute("download", exportName);
                document.body.appendChild(downloadAnchorNode);
                downloadAnchorNode.click();
                downloadAnchorNode.remove();
            }
        });
    </script>
</body>
</html>
==== scripts/cleanup_assets.sh ====
#!/bin/bash
#
# This script cleans up the redundant prompt and schema files after
# refactoring Chapter3Runner to use generic, template-driven assets.
# Run this from the project root.
#
set -euo pipefail

echo "🗑️  This script will delete 28 redundant asset files."
read -p "Are you sure you want to proceed? (y/n) " -n 1 -r
echo

if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Aborted by user."
    exit 1
fi

echo "--- Deleting Redundant Prompts ---"
rm -v assets/prompts/stage_3_3_10_ergebnis_struktur.txt || true
rm -v assets/prompts/stage_3_3_4_anwendungen.txt || true
rm -v assets/prompts/stage_3_3_5_itsysteme.txt || true
rm -v assets/prompts/stage_3_3_6_raeume.txt || true
rm -v assets/prompts/stage_3_3_7_kommunikation.txt || true
rm -v assets/prompts/stage_3_3_9_dienstleister.txt || true
rm -v assets/prompts/stage_3_4_2_schutzbedarf_gp.txt || true
rm -v assets/prompts/stage_3_4_3_schutzbedarf_anw.txt || true
rm -v assets/prompts/stage_3_4_4_schutzbedarf_its.txt || true
rm -v assets/prompts/stage_3_4_5_schutzbedarf_raeume.txt || true
rm -v assets/prompts/stage_3_4_6_schutzbedarf_komm.txt || true
rm -v assets/prompts/stage_3_4_8_ergebnis_schutzbedarf.txt || true
rm -v assets/prompts/stage_3_6_2_bausteine_custom.txt || true
rm -v assets/prompts/stage_3_6_3_ergebnis_check.txt || true
rm -v assets/prompts/stage_3_7_1_ergebnis_risikoanalyse.txt || true
rm -v assets/prompts/stage_3_8_1_ergebnis_realisierungsplan.txt || true

echo "--- Deleting Redundant Schemas ---"
rm -v assets/schemas/stage_3_3_10_ergebnis_struktur_schema.json || true
rm -v assets/schemas/stage_3_3_4_anwendungen_schema.json || true
rm -v assets/schemas/stage_3_3_5_itsysteme_schema.json || true
rm -v assets/schemas/stage_3_3_6_raeume_schema.json || true
rm -v assets/schemas/stage_3_3_7_kommunikation_schema.json || true
rm -v assets/schemas/stage_3_3_9_dienstleister_schema.json || true
rm -v assets/schemas/stage_3_4_2_schutzbedarf_gp_schema.json || true
rm -v assets/schemas/stage_3_4_3_schutzbedarf_anw_schema.json || true
rm -v assets/schemas/stage_3_4_4_schutzbedarf_its_schema.json || true
rm -v assets/schemas/stage_3_4_5_schutzbedarf_raeume_schema.json || true
rm -v assets/schemas/stage_3_4_6_schutzbedarf_komm_schema.json || true
rm -v assets/schemas/stage_3_4_8_ergebnis_schutzbedarf_schema.json || true
rm -v assets/schemas/stage_3_6_2_bausteine_custom_schema.json || true
rm -v assets/schemas/stage_3_6_3_ergebnis_check_schema.json || true
rm -v assets/schemas/stage_3_7_1_ergebnis_risikoanalyse_schema.json || true
rm -v assets/schemas/stage_3_8_1_ergebnis_realisierungsplan_schema.json || true

echo "✅ Cleanup complete."
==== scripts/deploy-audit-job.sh ====
#!/bin/bash
set -euo pipefail

# --- Configuration from Terraform ---
# NOTE: This script ASSUMES it is being run from the project root directory.
echo "🔹 Fetching infrastructure details from Terraform..."
TERRAFORM_DIR="../terraform"
JOB_NAME="bsi-audit-automator-job"
REGION="$(terraform -chdir=${TERRAFORM_DIR} output -raw region)"
PROJECT_ID="$(terraform -chdir=${TERRAFORM_DIR} output -raw project_id)"
ARTIFACT_REGISTRY_REPO="$(terraform -chdir=${TERRAFORM_DIR} output -raw artifact_registry_repository_name)"
SERVICE_ACCOUNT="$(terraform -chdir=${TERRAFORM_DIR} output -raw service_account_email)"
SUBNET_NAME="$(terraform -chdir=${TERRAFORM_DIR} output -raw subnet_name)" # Fetch the Subnet name
IMAGE_URI="${REGION}-docker.pkg.dev/${PROJECT_ID}/${ARTIFACT_REGISTRY_REPO}/${JOB_NAME}"

# --- 1. Build and Push Container Image ---
echo "🚀 Building and pushing container image to Artifact Registry..."
echo "Image URI: ${IMAGE_URI}"
# The source '.' is now correctly the project root.
gcloud builds submit . --tag "${IMAGE_URI}" --project "${PROJECT_ID}"

# --- 2. Delete Existing Job (for a clean deployment) ---
echo "🗑️  Checking for and deleting existing job to ensure a clean deployment..."
if gcloud run jobs describe "${JOB_NAME}" --region "${REGION}" --project "${PROJECT_ID}" &> /dev/null; then
  gcloud run jobs delete "${JOB_NAME}" \
    --region "${REGION}" \
    --project "${PROJECT_ID}" \
    --quiet
  echo "✅ Existing job '${JOB_NAME}' deleted."
else
  echo "ℹ️ No existing job found to delete. Proceeding."
fi

# --- 3. Deploy New Cloud Run Job ---
echo "📦 Deploying new Cloud Run Job '${JOB_NAME}'..."
gcloud run jobs deploy "${JOB_NAME}" \
  --image "${IMAGE_URI}" \
  --tasks 1 \
  --max-retries 3 \
  --memory 4Gi \
  --cpu 2 \
  --region "${REGION}" \
  --project "${PROJECT_ID}" \
  --task-timeout "7200" \
  --service-account "${SERVICE_ACCOUNT}" \
  --network "${SUBNET_NAME}" \
  --vpc-egress "all-traffic"

echo "✅ Deployment complete."
==== scripts/execute-audit-job.sh ====
#!/bin/bash
set -euo pipefail

# ===================================================================
# INTERACTIVE SCRIPT TO EXECUTE ANY BSI AUDIT TASK
# ===================================================================
# NOTE: This script ASSUMES it is being run from the project root directory.

# --- Script Usage ---
usage() {
  echo "Usage: $0"
  echo "Interactively selects and executes a BSI audit task for the customer"
  echo "defined in the Terraform configuration. Must be run from the project root."
  exit 1
}

# --- Argument Validation ---
if [[ $# -ne 0 ]]; then
  usage
fi
TEST_MODE="false"
MAX_CONCURRENT_AI_REQUESTS=5

# --- Dynamic Values from Terraform ---
echo "🔹 Fetching infrastructure details from Terraform..."
TERRAFORM_DIR="../terraform"
GCP_PROJECT_ID="$(terraform -chdir=${TERRAFORM_DIR} output -raw project_id)"
VERTEX_AI_REGION="$(terraform -chdir=${TERRAFORM_DIR} output -raw region)"
BUCKET_NAME="$(terraform -chdir=${TERRAFORM_DIR} output -raw vector_index_data_gcs_path | cut -d'/' -f3)"
INDEX_ENDPOINT_ID_FULL="$(terraform -chdir=${TERRAFORM_DIR} output -raw vertex_ai_index_endpoint_id)"
INDEX_PUBLIC_DOMAIN="$(terraform -chdir=${TERRAFORM_DIR} output -raw vertex_ai_index_endpoint_public_domain)"
INDEX_ENDPOINT_ID="$(basename "${INDEX_ENDPOINT_ID_FULL}")"

# --- INTERACTIVE SELECTION: Audit Type ---
echo "🔹 Please select the audit type."
audit_types=("Zertifizierungsaudit" "Überwachungsaudit")
PS3="Select audit type number: "
select AUDIT_TYPE in "${audit_types[@]}"; do
  if [[ -n "$AUDIT_TYPE" ]]; then
    echo "Selected audit type: $AUDIT_TYPE"
    break
  else
    echo "Invalid selection. Try again."
  fi
done

# --- INTERACTIVE SELECTION: Task/Stage ---
echo "🔹 Please select the task to execute."
tasks=("Run ETL (Embedding)" "Run Single Audit Stage" "Run All Audit Stages" "Generate Final Report" "Quit")
PS3="Select task number: "
declare TASK_ARGS # This will hold the arguments for main.py

select task in "${tasks[@]}"; do
  case $task in
    "Run ETL (Embedding)")
      TASK_ARGS="--run-etl"
      break
      ;;
    "Run Single Audit Stage")
      echo "🔹 Please select the stage to run."
      stages=("Chapter-1" "Chapter-3" "Chapter-4" "Chapter-5" "Chapter-7")
      PS3_STAGE="Select stage number: "
      select STAGE_NAME in "${stages[@]}"; do
        if [[ -n "$STAGE_NAME" ]]; then
          TASK_ARGS="--run-stage,${STAGE_NAME}"
          break
        else
          echo "Invalid stage selection. Try again."
        fi
      done
      break
      ;;
    "Run All Audit Stages")
      TASK_ARGS="--run-all-stages"
      break
      ;;
    "Generate Final Report")
      TASK_ARGS="--generate-report"
      break
      ;;
    "Quit")
      echo "Exiting."
      exit 0
      ;;
    *)
      echo "Invalid option $REPLY. Please try again."
      ;;
  esac
done

# --- Final gcloud Execution ---
echo "🚀 Executing task with args: [main.py ${TASK_ARGS}]"

# NOTE: The '--args' flag on 'gcloud run jobs execute' overrides the default
# command arguments of the deployed job, allowing us to run any task.
# The INDEX_ENDPOINT_PUBLIC_DOMAIN is now passed to the job.
gcloud run jobs execute "bsi-audit-automator-job" \
  --region "${VERTEX_AI_REGION}" \
  --project "${GCP_PROJECT_ID}" \
  --update-env-vars="GCP_PROJECT_ID=${GCP_PROJECT_ID},BUCKET_NAME=${BUCKET_NAME},INDEX_ENDPOINT_ID=${INDEX_ENDPOINT_ID},VERTEX_AI_REGION=${VERTEX_AI_REGION},SOURCE_PREFIX=source_documents/,OUTPUT_PREFIX=output/,ETL_STATUS_PREFIX=output/etl_status/,AUDIT_TYPE=${AUDIT_TYPE},TEST=${TEST_MODE},MAX_CONCURRENT_AI_REQUESTS=${MAX_CONCURRENT_AI_REQUESTS},INDEX_ENDPOINT_PUBLIC_DOMAIN=${INDEX_PUBLIC_DOMAIN}" \
  --args="${TASK_ARGS}"

echo "✅ Job execution' finished."

==== scripts/get_code_state_for_ai.sh ====
#!/bin/bash
#
# This script finds all non-binary files in the current directory and its
# subdirectories that are tracked by Git (respecting .gitignore), and
# concatenates them into a single context file.
# Large files (>1000 lines) are truncated to the first 200 lines to keep
# the context file manageable.
# USAGE:
# 1. Place this script in the root of your project directory.
# 2. Make it executable: chmod +x create_context.sh
# 3. Run it: ./create_context.sh
#
# The output will be a file named 'project_context.txt'.

OUTPUT_FILE="project_context.txt"

# Check for required commands
if ! command -v git &> /dev/null; then
    echo "Error: 'git' command not found. This script must be run in a Git repository."
    exit 1
fi

if ! command -v file &> /dev/null; then
    echo "Error: 'file' command not found. This is required to identify text files."
    exit 1
fi

# Clear the output file to start fresh
> "$OUTPUT_FILE"

echo "🔍 Finding text files and generating context..."

# Use 'git ls-files' to get a list of all files tracked by git,
# respecting .gitignore. Pipe this list into a loop.
# --cached: All files tracked in the index.
# --others: All untracked files.
# --exclude-standard: Respects .gitignore, .git/info/exclude, and global gitignore.
git ls-files --cached --others --exclude-standard | while read -r filename; do
    # Check if the file is likely a text file by checking its MIME type.
    # This is more reliable than checking the file extension.
    if [[ "$(file -br --mime-type "$filename")" == text/* ]]; then
        # Append a header with the filename
        echo "==== ${filename} ====" >> "$OUTPUT_FILE"

        # Get line count to determine if we need to truncate
        line_count=$(wc -l < "$filename")

        # If file is too long, truncate it. Otherwise, add it whole.
        if [ "$line_count" -gt 1000 ]; then
            echo "   Adding: $filename (Truncated to 200 lines from $line_count)"
            head -n 200 "$filename" >> "$OUTPUT_FILE"
            echo "" >> "$OUTPUT_FILE"
            echo "[... File truncated. Only first 200 of ${line_count} lines included. ...]" >> "$OUTPUT_FILE"
        else
            echo "   Adding: $filename"
            cat "$filename" >> "$OUTPUT_FILE"
        fi

        # Add a newline for spacing between files
        echo "" >> "$OUTPUT_FILE"
    fi
done

echo ""
echo "✅ Project context successfully generated in: $OUTPUT_FILE"
==== scripts/index_metadata.yaml ====
# index_metadata.yaml
contentsDeltaUri: "gs://bsi-auditor-3-audit-data/vector_index_data/"
config:
  dimensions: 3072
  approximateNeighborsCount: 150
  algorithmConfig:
    treeAhConfig:
      leafNodeEmbeddingCount: 500
==== scripts/redeploy-index.sh ====
#!/bin/bash
set -euo pipefail

# ===================================================================
# Manually triggers a deploy-index operation on the endpoint.
#
# This is useful if the initial deployment during `terraform apply`
# needs to be re-run or modified without destroying the endpoint.
# Note: This does NOT re-ingest data. Data ingestion is automatic
# when files in the GCS directory change.
# ===================================================================

echo "🔹 Fetching infrastructure details from Terraform..."
TERRAFORM_DIR="../terraform"
PROJECT_ID="$(terraform -chdir=${TERRAFORM_DIR} output -raw project_id)"
REGION="$(terraform -chdir=${TERRAFORM_DIR} output -raw region)"
ENDPOINT_ID="$(terraform -chdir=${TERRAFORM_DIR} output -raw vertex_ai_index_endpoint_id | xargs basename)"
INDEX_ID="$(terraform -chdir=${TERRAFORM_DIR} output -raw vertex_ai_index_id | xargs basename)"

# Use a consistent deployed index ID
DEPLOYED_INDEX_ID="bsi_deployed_index_kunde_x"
DISPLAY_NAME="BSI Deployed Index"

echo "🚀 Attempting to deploy index '${INDEX_ID}' to endpoint '${ENDPOINT_ID}'..."

# The gcloud command to deploy the index to the endpoint
gcloud ai index-endpoints deploy-index "${ENDPOINT_ID}" \
  --index="${INDEX_ID}" \
  --deployed-index-id="${DEPLOYED_INDEX_ID}" \
  --display-name="${DISPLAY_NAME}" \
  --project="${PROJECT_ID}" \
  --region="${REGION}"

echo "✅ Index deployment command sent successfully."
echo "   Monitor the deployment status in the Google Cloud Console."
==== scripts/refresh-index.sh ====
# bsi-audit-automator/scripts/refresh-index.sh
#!/bin/bash
set -euo pipefail

# ===================================================================
# SCRIPT TO FORCE-REFRESH THE VERTEX AI VECTOR SEARCH INDEX
# ===================================================================
#
# WHAT IT DOES:
# This script manually triggers an update operation on the Vertex AI
# Index. This forces the index to re-scan its configured GCS
# directory (`vector_index_data/`) and ingest any new or updated
# JSONL embedding files.
#
# WHEN TO USE IT:
# Use this script if you suspect the index's automatic update
# (`contentsDeltaUri`) has failed and the item count in the index
# does not reflect the number of chunks generated by the ETL process.
#
# PREREQUISITES:
#   - Must be run from the project root ('bsi-audit-automator/').
#   - 'gcloud' and 'terraform' CLIs must be installed and authenticated.
#   - `terraform apply` must have been run successfully.
#

echo "🔹 This script will force a manual update of the Vertex AI Index content from GCS."

# --- Cleanup handler: ensures the temporary metadata file is deleted on exit ---
cleanup() {
  rm -f index_metadata.yaml
  echo "🔹 Temporary metadata file cleaned up."
}
trap cleanup EXIT

# --- Configuration & Validation ---
TERRAFORM_DIR="../terraform"
METADATA_FILE="index_metadata.yaml"

if [ ! -d "$TERRAFORM_DIR" ]; then
    echo "❌ Error: Terraform directory not found at '$TERRAFORM_DIR'. Please run this from the project root."
    exit 1
fi
if ! command -v terraform &> /dev/null || ! command -v gcloud &> /dev/null; then
    echo "❌ Error: 'terraform' or 'gcloud' command not found. Please install and configure them."
    exit 1
fi

echo "🔹 Fetching infrastructure details from Terraform state..."

# --- Dynamic Values from Terraform ---
PROJECT_ID="$(terraform -chdir=${TERRAFORM_DIR} output -raw project_id)"
REGION="$(terraform -chdir=${TERRAFORM_DIR} output -raw region)"
BUCKET_NAME="$(terraform -chdir=${TERRAFORM_DIR} output -raw vector_index_data_gcs_path | cut -d'/' -f3)"
# Use `xargs basename` to get just the numeric ID from the full resource path
INDEX_ID="$(terraform -chdir=${TERRAFORM_DIR} output -raw vertex_ai_index_id | xargs basename)"

CONTENTS_DELTA_URI="gs://${BUCKET_NAME}/vector_index_data/"

# --- Generate the required metadata file dynamically ---
echo "🔹 Generating temporary metadata file ('${METADATA_FILE}')..."
cat <<EOF > ${METADATA_FILE}
# This file is dynamically generated by refresh-index.sh
contentsDeltaUri: "${CONTENTS_DELTA_URI}"
config:
  dimensions: 3072
  approximateNeighborsCount: 150
  algorithmConfig:
    treeAhConfig:
      leafNodeEmbeddingCount: 500
EOF

# --- User Confirmation ---
echo "-----------------------------------------------------"
echo "  Project:          ${PROJECT_ID}"
echo "  Region:           ${REGION}"
echo "  Index ID:         ${INDEX_ID}"
echo "  Data Source URI:  ${CONTENTS_DELTA_URI}"
echo "-----------------------------------------------------"

read -p "🚨 Do you want to proceed with updating the index content? (y/n) " -n 1 -r
echo # move to a new line
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Aborted by user."
    exit 1
fi

# --- Execute the gcloud command ---
echo "🚀 Sending update command to Vertex AI Index '${INDEX_ID}'..."
gcloud ai indexes update "${INDEX_ID}" \
  --metadata-file="./${METADATA_FILE}" \
  --project="${PROJECT_ID}" \
  --region="${REGION}"

echo ""
echo "✅ Index update operation started successfully."
echo "   You can monitor its progress in the GCP Console:"
echo "   Vertex AI -> Matching Engine -> Select your Index."
echo "   Wait for the 'Dense vector count' to reflect your data."
==== scripts/reset_audit.sh ====
#!/bin/bash
set -euo pipefail

# ===================================================================
# SCRIPT TO COMPLETELY RESET THE AUDIT ENVIRONMENT
# ===================================================================
#
# WHAT IT DOES:
# This is a DESTRUCTIVE script that prepares the environment for a
# completely new audit. It performs two main actions:
# 1. Deletes all generated data from the GCS bucket, including:
#    - All source documents (`source_documents/`)
#    - All generated embeddings (`vector_index_data/`)
#    - All ETL status markers and results (`output/`)
# 2. Marks the Vertex AI Index resource for recreation in Terraform.
#
# WHEN TO USE IT:
# Run this script BEFORE starting a new audit for a new customer or
# when you have a significantly changed set of source documents and
# want to ensure no old data remains.
#
# PREREQUISITES:
#   - Must be run from the project root ('bsi-audit-automator/').
#   - 'gcloud', 'gsutil', and 'terraform' CLIs must be installed and authenticated.
#   - `terraform apply` must have been run successfully at least once.
#

echo "🚨 WARNING: This is a destructive operation! 🚨"
echo "This script will delete ALL audit data from GCS and mark the"
echo "Vertex AI Index for recreation."

# --- Configuration & Validation ---
TERRAFORM_DIR="../terraform"

if [ ! -d "$TERRAFORM_DIR" ]; then
    echo "❌ Error: Terraform directory not found at '$TERRAFORM_DIR'. Please run this from the project root."
    exit 1
fi

echo "🔹 Fetching infrastructure details from Terraform state..."

# --- Dynamic Values from Terraform ---
BUCKET_NAME="$(terraform -chdir=${TERRAFORM_DIR} output -raw vector_index_data_gcs_path | cut -d'/' -f3)"
INDEX_RESOURCE_NAME="google_vertex_ai_index.bsi_audit_index"

# --- User Confirmation ---
echo "-----------------------------------------------------"
echo "The following actions will be performed:"
echo "  1. DELETE all objects in gs://${BUCKET_NAME}/source_documents/"
echo "  2. DELETE all objects in gs://${BUCKET_NAME}/vector_index_data/"
echo "  3. DELETE all objects in gs://${BUCKET_NAME}/output/"
echo "  4. TAINT the Terraform resource '${INDEX_RESOURCE_NAME}', forcing it to be recreated on next 'apply'."
echo "-----------------------------------------------------"

read -p "Are you absolutely sure you want to proceed? (y/n) " -n 1 -r
echo # move to a new line
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Aborted by user."
    exit 1
fi

# --- Execute GCS Cleanup ---
echo "🗑️  Deleting existing data from GCS bucket..."
# Use gsutil with -m for parallel deletion, which is faster.
# The `|| true` prevents the script from exiting if a folder is already empty.
gsutil -m rm -r "gs://${BUCKET_NAME}/source_documents/*" || true
gsutil -m rm -r "gs://${BUCKET_NAME}/vector_index_data/*" || true
gsutil -m rm -r "gs://${BUCKET_NAME}/output/*" || true
echo "✅ GCS data deleted."

# --- Execute Terraform Taint ---
echo "🎯 Marking Vertex AI Index for recreation..."
terraform -chdir=${TERRAFORM_DIR} taint "${INDEX_RESOURCE_NAME}"
echo "✅ Index resource tainted successfully."

# --- Final Instructions ---
echo ""
echo "✅ Reset complete. The environment is now clean."
echo "   Next Steps:"
echo "   1. Navigate to the terraform directory: cd ../terraform"
echo "   2. Apply the changes to recreate the index: terraform apply -auto-approve"
echo "   3. Navigate back to the project root: cd .."
echo "   4. Upload the NEW set of source documents to gs://${BUCKET_NAME}/source_documents/"
echo "   5. You can now start the audit pipeline with '--run-etl'."
==== tasks_for_improvement_after_MVP-1.md ====
### **Project Improvement Roadmap**

This document tracks completed enhancements and the prioritized backlog of features and technical debt to address.

---

### **Completed Milestones (as of this review)**

*   **[✅] Implemented Full RAG Pipeline:** The `RagClient` is functional and integrated into all relevant stages (Chapters 1 & 3) to provide evidence-based context to prompts.
*   **[✅] Implemented Centralized Finding Collection:** The `AuditController` now systematically collects structured findings from all stages, and the `ReportGenerator` populates them into Chapter 7.2.
*   **[✅] Implemented Conditional Audit Planning (Stage 4):** The `Chapter4Runner` now correctly loads different prompts and schemas based on the `AUDIT_TYPE`, supporting both "Zertifizierungsaudit" and "Überwachungsaudit".
*   **[✅] Refactored Chapter 5 for Manual Audit:** The `Chapter5Runner` no longer simulates the audit for 5.5.2. Instead, it deterministically generates a control checklist for the human auditor using the `ControlCatalog`. Automated generation for 5.1 has been removed.
*   **[✅] Refactored Chapter 1 for Accuracy:** The `Chapter1Runner` now uses a stricter prompt for 1.2 to prevent hallucination and includes finding-generation logic. Processing for 1.4 has been correctly removed.
*   **[✅] Bugfix - Chapter 3 Aggregation:** Fixed the aggregation logic in the `Chapter3Runner` to correctly parse the new structured `finding` object.
*   **[✅] Bugfix - Report Editor Table Functionality:** Fixed the JavaScript logic to allow adding/deleting rows in all tables, including the nested findings tables in Chapter 7.
*   **[✅] Implemented Idempotent & Robust ETL:** The ETL processor in `src/etl/processor.py` now creates `.success` and `.failed` status markers, ensuring resilience and preventing reprocessing of files.
*   **[✅] Critical Bugfix - ReportGenerator Stability:** Systematically refactored `src/audit/report_generator.py` with defensive data population logic, preventing crashes from `KeyError` or `IndexError` when merging stage results into the master template.
*   **[✅] Code Quality - Docstrings & Type Hints:** Added comprehensive docstrings and strict type hints to public methods in core modules (`EtlProcessor`, `AuditController`) to improve maintainability and clarity.
*   **[✅] Bugfix - Intuitive Stage Execution:** Corrected the main control flow to align with user intent. `--run-stage` now *always* overwrites its specific target. `--run-all-stages` now correctly skips completed stages by default for resumability, and can be overridden with `--force`.
*   **[✅] Bugfix - SDK Schema Parsing (COMPLETE):** Resolved a recurring `TypeError` by refactoring all affected schemas (Chapters 1 and 3). The fix ensures the `"items"` keyword for arrays always uses a single schema object (e.g., `{"items": {"type": "..."}}`) instead of the unsupported "tuple validation" format (`{"items": [...]}`), ensuring full compatibility with the Vertex AI SDK's schema parser.
*   **[✅] Corrected Chapter 7.2 Findings Tables (TODO 15):** Restructured the findings tables in `master_report_template.json` to match the BSI standard (`Nr.`, `Beschreibung`, `Quelle`, `Behebungsfrist`, `Status`). The `ReportGenerator` now correctly populates these tables with default deadlines and status, ensuring the final report is compliant.
*   **[✅] Added Missing Report Chapters & Generation Logic (TODO 13 & 14):** Significantly expanded the application's scope to align with the official BSI template.
    *   Updated `master_report_template.json` with structures for all missing chapters and subchapters (e.g., 1.4, 3.3.2, 3.3.3, 3.4, 3.5, 3.6, 5.6.2).
    *   Created a comprehensive set of new prompts and schemas to drive AI generation for these new sections.
    *   Enhanced `Chapter3Runner`, `Chapter5Runner`, and `ReportGenerator` to process and populate this new content.
*   **[✅] Improved RAG Context and Traceability (TODO 12):** Refactored the `RagClient` to include the source document filename in the context provided to the AI. This replaces generic chunk IDs with clear document references (e.g., `CONTEXT FROM DOCUMENT: ...`), improving the traceability and clarity of AI-generated findings.

---

### **New Prioritized TODO List**

*   **[ ] TODO 2: Optimize RagClient Memory Usage.**
    *   **File:** `src/clients/rag_client.py`
    *   **Action:** Currently, the `RagClient` loads the full text of every chunk from every document into memory at startup. This could exhaust the Cloud Run instance's memory on very large audits. Refactor `_load_chunk_lookup_map` to be more memory-efficient. For example, it could map `chunk_id -> source_document_name` only. When a lookup is needed, it would then open only the required source document's embedding file to retrieve the text, drastically reducing the initial memory footprint.

*   **[ ] TODO 3: Implement "Two-Plus-One" Verification.**
    *   **File:** `src/clients/ai_client.py`
    *   **Action:** Refactor the `generate_json_response` function. Instead of making a single AI call, it should make two parallel calls for the initial prompt, then construct a new "synthesis prompt" to have the AI review and merge the two initial results into a final, higher-quality response. This enhances accuracy and reliability.

*   **[ ] TODO (Feature): Enhance Chapter 5 Checklist Generation.**
    *   **Files:** `src/audit/stages/stage_5_vor_ort_audit.py`, `src/audit/stages/control_catalog.py`
    *   **Action:** The current on-site checklist in 5.5.2 is basic. Enhance it by leveraging the detailed maturity level descriptions (M1-M5) in the BSI OSCAL catalog. For each required control, use the AI to generate specific, maturity-level-targeted interview questions and evidence requests for the human auditor.
    *   **Example:** Instead of just "Check ISMS.1.A1", the output would be a list like: `["[M3] Show me the signed security policy.", "[M4] Provide minutes from the last risk committee meeting.", "[M5] How are security KPIs tracked and reported in management dashboards?"]`. This would make the on-site audit significantly more effective.

*   **[ ] TODO 4: Implement Location Audit Planning (Standortauswahl).**
    *   **File:** `src/audit/stages/stage_4_pruefplan.py`
    *   **Action:** Add a new subchapter processor within the `Chapter4Runner` to handle section 4.1.4 ("Auswahl Standorte"). This function will need to use RAG to list available locations and then use the formulas in the BSI `Auditierungsschema` (e.g., `sqrt(n)`) to instruct the AI to select a risk-based sample of sites to audit.

*   **[ ] TODO 5: Automate Check of "Entbehrliche Bausteine" & Justifications.**
    *   **Action:** Develop a logic that generates a list of all controls marked as "Entbehrlich" (dispensable) from the "A.4 Grundschutz-Check" document. For each item, use a focused RAG prompt to ask the AI if the provided justification ("Begründung") is sufficient and plausible according to BSI standards. This also covers checking all justifications in A.4.

*   **[ ] TODO 6: Automate Check of Strukturanalyse vs. Netzplan.**
    *   **Action:** For checks if all Objects mentioned in the Strukturanalyse and the Netzplan are present in the other document, develop a logic that generates a list of all objects in each file and then compares the lists to find discrepancies.

*   **[ ] TODO 7: Automate Check of Modellierung (Baustein Application).**
    *   **Action:** Check if the correct and all required Bausteine are listed for each object in the A.4 Grundschutz-Check, cross-referencing against the BSI modeling requirements.

*   **[ ] TODO 8: Automate Check of Risk Analysis & Additional Controls.**
    *   **Action:** Check that objects identified with a 'high' or 'very high' protection requirement in the Strukturanalyse have corresponding additional security controls defined. Verify that these additional controls are correctly included in the A.4 Grundschutz-Check.

*   **[ ] TODO 9: Automate Check of Risk Treatment for Unimplemented Controls (A.6).**
    *   **Action:** For all controls listed in A.6 (Realisierungsplan) that are not yet fully implemented, verify that they are listed and that the risk treatment plan is well-explained and plausible.

*   [ ] TODO 10: Implement a Comprehensive Test Suite.
    *   **Directory:** `tests/`
    *   **Action:** The project currently lacks automated tests. Create a testing suite using a framework like `pytest`. Add unit tests for individual functions (e.g., in `etl/processor.py`). Add integration tests for client interactions, using mock objects (`unittest.mock`) to simulate GCS and AI API calls. This is also a way to "deterministically check results" by providing fixed inputs and asserting expected outputs from business logic.

*   **[ ] TODO 11: Improve Prompts and Schemas.**
    *   **Directory:** `assets/`
    *   **Action:** Continuously refine the prompts in `assets/prompts/` and schemas in `assets/schemas/` to improve the quality, accuracy, and consistency of the AI-generated results.

==== terraform/.terraform.lock.hcl ====
# This file is maintained automatically by "terraform init".
# Manual edits may be lost in future updates.

provider "registry.terraform.io/hashicorp/google" {
  version     = "6.41.0"
  constraints = ">= 4.50.0"
  hashes = [
    "h1:ecCCOOF2ZDP1vLT/PDVNcL0+1PbtG2auHg8KcSyXI8Y=",
    "zh:0a375c4f37599de566b6dd19d1953f70f6f88f089bb7cf97d1fa1287b2ebf41c",
    "zh:2309a75bf6bd6f97ccae0c8348bb560eb893518786150cb634bc7795898d38f9",
    "zh:66ed27c1b9a2b8ab7910954447f77efb319f49bc546d3611fd00d56776a928d8",
    "zh:7a8624d5b8d8aee0604cb9a5bc0fb450d235983cf540dd19f68535063fa8cf17",
    "zh:883879b1d5d3058d87db13b824c15193e3e2e4995a3b8165ce5835872b955349",
    "zh:89971c2ca25a546febbab693f98480a1c032945e7b10e4ccfc0273c5201978bd",
    "zh:923281ee1f1f5bb827786a198973abfdfb2a86e87d751e37809a19e55e84c160",
    "zh:c0a28bee3e4e8722463e53ca5a090d4df5b6db5fdfb9f622461bc9bc3f27f418",
    "zh:c3ac6015b50c93f12cced716ad92f072a8a1245924b034b4a9bc9261eb31fbde",
    "zh:d70f83f015182021a4593cd4a9a2687f83c0aa6fd54a982bd374ef0fd25113f2",
    "zh:ed9eb0992e901c7b832ae50dc267d12febf53f89ee6b1a02320b7a145830c6ce",
    "zh:f569b65999264a9416862bca5cd2a6177d94ccb0424f3a4ef424428912b9cb3c",
  ]
}

provider "registry.terraform.io/hashicorp/google-beta" {
  version = "6.41.0"
  hashes = [
    "h1:i39xMI2rPpD7mVuJBLqFMPUq85AtlMQw6eVSX8nzvYA=",
    "zh:29cc3b6a1f77778b0fe42b75bb395c1962f7f4bb005c924ebc4de05c437e8b33",
    "zh:35f94214bf5c2b35e3d57752a43bcea045878b43446f26e16ef96fc8b5637268",
    "zh:377b706926908fce2d5038acc6a6e315c4b55294c97a37be73cd9a2511dd85cb",
    "zh:4f0b3a9ff61d787de83fa350ce80d0c197102318352c0fd4efef6c2c84372315",
    "zh:545df307a7fdea44a3b6d9391de97523a2725d996cceaf8a52be5d7769e71ad0",
    "zh:99e190dad60433d4cf2603bb6a65d53eee20e93aa1fd3878fb390adf9a6e11c3",
    "zh:a93f2e391c343f7f4325ac38d534a6f11f0213661290c53e11090d4e4097f89b",
    "zh:bffab16ca0102cdc0c8b21c3d255c7ddd2306e0ecc39c1b55439d2118ccf4845",
    "zh:cfb5cd4038b6966307960317c86c6cdb5b2c22f4338506edd53123879fea42f5",
    "zh:e007a72a34ba9f2768938e668b578e5d4dd5e625c17bdb4feae9aead03fec17c",
    "zh:e21a35539fcc34b5b22a61cd4869fb26f034796d9947a6ab35413105c8bac22c",
    "zh:f569b65999264a9416862bca5cd2a6177d94ccb0424f3a4ef424428912b9cb3c",
  ]
}

==== terraform/data.tf ====
data "google_project" "project" {
  # This data source uses the project ID configured in the Google provider
  # to fetch details about the project, such as its number.
  # The provider is configured via the `project_id` in your .tfvars file.
}
==== terraform/main.tf ====
# Configure the Google Cloud provider
terraform {
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = ">= 4.50.0"
    }
  }
}

provider "google" {
  project = var.project_id
  region  = var.region
}

# Provider alias for resources that may have features only available in beta
provider "google-beta" {
  project = var.project_id
  region  = var.region
}

# --- NEW: MANAGE PROJECT APIS DECLARATIVELY ---
# This section enables all necessary APIs for the project.
# It replaces the need for the manual `enable_apis.sh` script.

locals {
  apis_to_enable = [
    "run.googleapis.com",                 # Cloud Run Jobs
    "cloudbuild.googleapis.com",          # Cloud Build (for deploying from source)
    "artifactregistry.googleapis.com",    # Artifact Registry (to store images)
    "aiplatform.googleapis.com",          # Vertex AI (for embeddings and Vector Search)
    "storage.googleapis.com",             # Cloud Storage
    "cloudresourcemanager.googleapis.com", # Required by many services
    "compute.googleapis.com",            # Required for creating VPC Networks
    "servicenetworking.googleapis.com",  # Required for creating VPC Networks
    "iam.googleapis.com"                 # Required for creating Service Accounts and IAM bindings
  ]
}

resource "google_project_service" "project_apis" {
  for_each = toset(local.apis_to_enable)

  project                    = var.project_id
  service                    = each.key
  disable_on_destroy         = false # Keep APIs enabled even after destroy
}

# --- CHANGE: ADDED BUCKET CREATION ---
# This resource now creates the GCS bucket for our project automatically.
# It depends on the APIs being enabled first.
resource "google_storage_bucket" "bsi_audit_bucket" {
  name                        = "${var.project_id}-audit-data"
  location                    = var.region # Ensures bucket is in the same region as Vertex AI
  force_destroy               = true       # Allows 'terraform destroy' to delete the bucket even if it has files
  uniform_bucket_level_access = true
  depends_on = [google_project_service.project_apis]
}

# --- NEW: ARTIFACT REGISTRY REPOSITORY ---
# Create the repository to store our job's Docker images.
resource "google_artifact_registry_repository" "bsi_repo" {
  provider      = google-beta # The repository resource often has features in beta
  location      = var.region
  repository_id = "bsi-audit-repo"
  description   = "Docker repository for BSI Audit Automator jobs"
  format        = "DOCKER"
  depends_on = [google_project_service.project_apis]
}

# --- NEW: DEDICATED SERVICE ACCOUNT ---
# Create a custom Service Account for our Cloud Run Job to use.
resource "google_service_account" "bsi_job_sa" {
  account_id   = var.service_account_id
  display_name = "Service Account for BSI Audit Automator Job"
  depends_on = [google_project_service.project_apis]
}

# --- NEW: PLACEHOLDER FILE FOR INDEX CREATION ---
# Create an empty, validly named JSON file in the vector index directory.
# This is required to satisfy the API's validation check during 'terraform apply'.
resource "google_storage_bucket_object" "json_placeholder" {
  name         = "vector_index_data/placeholder.json"
  bucket       = google_storage_bucket.bsi_audit_bucket.name
  content_type = "application/json"
  # One Dummy 
  content      =<<EOT
{"id": "DUMMY", "sparse_embedding": {"values": [0.1, 0.2], "dimensions": [1, 4]}}
  EOT
}

# 1. NETWORKING: A VPC is required for the Vertex AI Index Endpoint.
resource "google_compute_network" "bsi_vpc" {
  name                    = var.vpc_network_name
  auto_create_subnetworks = false
  depends_on = [google_project_service.project_apis]
}

# Add this new resource block to terraform/main.tf

resource "google_compute_subnetwork" "bsi_audit_subnet" {
  name                     = "bsi-audit-subnet"
  ip_cidr_range            = "10.10.1.0/24" # A standard private IP range for the subnet
  region                   = var.region     # Must be in the same region as the Cloud Run job
  network                  = google_compute_network.bsi_vpc.id # Links it to our VPC
  private_ip_google_access = true         # Allows the job to reach Google APIs privately
  
  # Ensure the VPC network exists before creating the subnet
  depends_on = [google_compute_network.bsi_vpc]
}

resource "google_compute_global_address" "peering_range" {
  name          = "vertex-ai-peering-range"
  purpose       = "VPC_PEERING"
  address_type  = "INTERNAL"
  prefix_length = 16
  network       = google_compute_network.bsi_vpc.id
}

resource "google_service_networking_connection" "vertex_vpc_connection" {
  network                 = google_compute_network.bsi_vpc.id
  service                 = "servicenetworking.googleapis.com"
  reserved_peering_ranges = [google_compute_global_address.peering_range.name]
}


# 2. VECTOR DATABASE: The Vertex AI Index and its Endpoint.
# -----------------------------------------------------------------

locals {
  # --- CHANGE: DYNAMICALLY USE THE CREATED BUCKET ---
  # This path now refers to the bucket created above, not a variable.
  index_contents_path = "gs://${google_storage_bucket.bsi_audit_bucket.name}/vector_index_data/"
}

resource "google_vertex_ai_index" "bsi_audit_index" {
  display_name = "bsi-audit-index"
  description  = "Vector search index for BSI audit documents for project ${var.project_id}."
  region       = var.region

  metadata {
    contents_delta_uri = local.index_contents_path
    config {
      dimensions                  = 3072
      approximate_neighbors_count = 150
      algorithm_config {
        tree_ah_config {
          leaf_node_embedding_count = 500
        }
      }
    }
  }
}

resource "google_vertex_ai_index_endpoint" "bsi_audit_endpoint" {
  display_name = "bsi-audit-endpoint"
  description  = "Endpoint for querying the BSI audit index for project ${var.project_id}."
  region       = var.region
  # --- FIX FOR PROJECT NUMBER ERROR ---
  # To make the endpoint accessible from cloud shell, we enable the public endpoint.
  public_endpoint_enabled = true

  # --- FIX: USE A PROVISIONER TO DEPLOY THE INDEX ---
  # This runs the gcloud command on the local machine after the endpoint is created.
  provisioner "local-exec" {
    when = create
    command = "gcloud ai index-endpoints deploy-index ${self.name} --index=${google_vertex_ai_index.bsi_audit_index.name} --deployed-index-id=bsi_deployed_index_kunde_x --display-name='BSI Deployed Index' --project=${var.project_id} --region=${var.region}"
  }

  # --- FIX: ADD A DESTROY PROVISIONER TO UNDEPLOY THE INDEX ---
  # This runs before the resource is destroyed, ensuring the endpoint is empty.
  # It MUST only use 'self' attributes, not 'var' attributes.
  provisioner "local-exec" {
    when = destroy
    command = "gcloud ai index-endpoints undeploy-index ${self.name} --deployed-index-id=bsi_deployed_index_kunde_x --project=${self.project} --region=${self.region} --quiet"
  }

  # The endpoint depends on the index existing first.
  depends_on = [google_vertex_ai_index.bsi_audit_index]
}

# 3. IAM & PERMISSIONS: Applying the Principle of Least Privilege
# -----------------------------------------------------------------

# Grant our new Service Account permission to use Vertex AI.
resource "google_project_iam_member" "sa_vertex_access" {
  project = var.project_id
  role    = "roles/aiplatform.user"
  member  = "serviceAccount:${google_service_account.bsi_job_sa.email}"
}

# Grant our new Service Account permission to read/write to our specific GCS bucket.
resource "google_storage_bucket_iam_member" "sa_gcs_access" {
  bucket = google_storage_bucket.bsi_audit_bucket.name
  role   = "roles/storage.objectAdmin"
  member = "serviceAccount:${google_service_account.bsi_job_sa.email}"
}

# Grant the default Cloud Build Service Account permission to push images
# to our new Artifact Registry repository. This is the permission that was
# previously missing and caused the build to fail.
resource "google_artifact_registry_repository_iam_member" "cloudbuild_ar_writer" {
  location   = google_artifact_registry_repository.bsi_repo.location
  repository = google_artifact_registry_repository.bsi_repo.name
  role       = "roles/artifactregistry.writer"
  member     = "serviceAccount:${var.project_number}@cloudbuild.gserviceaccount.com"
}
==== terraform/outputs.tf ====
output "vertex_ai_index_id" {
  description = "The full resource ID of the created Vertex AI Index."
  value       = google_vertex_ai_index.bsi_audit_index.id
}

output "vertex_ai_index_endpoint_id" {
  description = "The full resource ID of the created Vertex AI Index Endpoint."
  value       = google_vertex_ai_index_endpoint.bsi_audit_endpoint.id
}

output "vertex_ai_index_endpoint_public_domain" {
  description = "The public domain name for querying the index endpoint. Our Python app will use this."
  value       = google_vertex_ai_index_endpoint.bsi_audit_endpoint.public_endpoint_domain_name
}

output "next_step_gcloud_command" {
  description = "Example gcloud command to deploy the index to the endpoint after the index is populated."
  value       = "gcloud ai index-endpoints deploy-index ${google_vertex_ai_index_endpoint.bsi_audit_endpoint.name} --index=${google_vertex_ai_index.bsi_audit_index.name} --deployed-index-id=bsi_deployed_index --display-name=bsi_deployed_index --project=${var.project_id} --region=${var.region}"
}

output "vector_index_data_gcs_path" {
  description = "The GCS path where the Python application must upload the embedding data files (e.g., index_data.jsonl). The Vertex AI Index automatically monitors this path."
  value       = local.index_contents_path
}

output "service_account_email" {
  description = "The email of the custom service account created for the Cloud Run Job."
  value       = google_service_account.bsi_job_sa.email
}

output "artifact_registry_repository_url" {
  description = "The URL of the created Artifact Registry repository."
  value       = "${google_artifact_registry_repository.bsi_repo.location}-docker.pkg.dev/${var.project_id}/${google_artifact_registry_repository.bsi_repo.repository_id}"
}

output "region" {
  description = "The Google Cloud region where resources are deployed."
  value       = var.region
}

output "artifact_registry_repository_name" {
  description = "The name (repository ID) of the created Artifact Registry repository."
  value       = google_artifact_registry_repository.bsi_repo.repository_id
}

output "project_id" {
  description = "The Google Cloud project ID where resources are deployed."
  value       = var.project_id
}

output "project_number" {
  description = "The Google Cloud project number where resources are deployed."
  value       = data.google_project.project.number
}

output "vpc_network_name" {
  description = "The name of the VPC network created for the audit resources."
  value       = google_compute_network.bsi_vpc.name
}

output "subnet_name" {
  description = "The name of the Subnet created for the Cloud Run Job to connect to."
  value       = google_compute_subnetwork.bsi_audit_subnet.name
}
==== terraform/terraform.tfvars ====
# The Google Cloud project ID.
project_id = "bsi-auditor-3"

# The region where all resources will be deployed.
# Must be a region that supports Vertex AI Vector Search.
region = "europe-west4"

# The name for the dedicated VPC network.
vpc_network_name = "bsi-audit-vpc"

# The ID for the custom service account.
service_account_id = "bsi-automator-sa"

# project NUMBER
project_number ="60392965036"

==== terraform/variables.tf ====
variable "project_id" {
  description = "The Google Cloud Project ID where resources will be deployed."
  type        = string
}

variable "project_number" {
  description = "The unique numeric identifier for the Google Cloud project."
  type        = string
}

variable "region" {
  description = "The Google Cloud region for the resources. Must support Vertex AI Vector Search."
  type        = string
  default     = "europe-west4" # A region that supports the service
}

variable "vpc_network_name" {
  description = "The name of the VPC network to create for the Vertex AI endpoint."
  type        = string
  default     = "bsi-audit-vpc"
}

variable "service_account_id" {
  description = "The ID for the custom service account (e.g., 'bsi-automator-sa')."
  type        = string
}
